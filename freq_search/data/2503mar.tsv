Cold start delays are a main pain point for today’s FaaS (Function-as-a-Service) platforms. A widely used mitigation strategy is keeping recently invoked function containers alive in memory to enable warm starts with minimal overhead. This paper identifies new challenges that state-of-the-art FaaS keep-alive policies neglect. These challenges are caused by concurrent function invocations, a common FaaS workload behavior. First, concurrent requests present a trade-off between reusing busy containers (delayed warm starts) versus cold-starting containers. Second, concurrent requests cause imbalanced evictions of containers that will be reused shortly thereafter. To tackle the challenges, we propose a novel serverless function container orchestration algorithm called CIDRE. CIDRE makes informed decisions to speculatively choose between a delayed warm start and a cold start under concurrency-driven function scaling. CIDRE uses both fine-grained container-level and coarse-grained concurrency information to make balanced eviction decisions. We evaluate CIDRE extensively using two production FaaS workloads. Results show that CIDRE reduces the cold start ratio and the average invocation overhead by up to 75.1% and 39.3% compared to state-of-the-art function keep-alive policies.
3D Gaussian Splatting (3DGS) is an emerging approach for training and representing real-world 3D scenes. Due to its photorealistic novel view synthesis and fast rendering speed (e.g., over 100 FPS), it has the potential to transform how scenes that can be explored in 6 degrees-of-freedom (6-DoF) are represented. However, a limiting factor of 3DGS is its large size, which requires high network bandwidth for streaming reconstructed real-world 3D scenes. In this paper, we propose SGSS for optimizing the streaming transmission of 3DGS scenes during 6-DoF navigation. Since not all Gaussians in the full scene are needed for rendering a user's view, SGSS uses view-adaptive streaming, enabled by optimized spatial partitioning of the scene, for achieving network transmission savings. For each spatial partition, SGSS uses an importance-based Gaussians pre-sorting scheme to enhance the initial view quality and reduce the user-perceived scene loading time. We further design a client-side view-adaptive streaming algorithm that features lightweight visibility checking, prioritized streaming, incremental processing, and stream pausing/resuming schemes. We implement SGSS with JavaScript and WebGL2. Evaluation results show that the quality of views rendered with SGSS streaming is consistently higher than or on par with state-of-the-art approaches. Furthermore, the view-adaptive streaming of SGSS can result in high savings in network transmission without impacting the view quality.
Single-Sparse-Matrix Kernels (SSMKs) such as SpMM, SDDMM, SpMV, and SpTS form the backbone of applications such as data analytics, graph processing, finite-element analysis, machine learning (including GNNs and LLMs), etc. This paper introduces Residue-based Acceleration of Single Sparse Matrix Computation via Adaptive Tiling (RASSM), an input-dependent, adaptive 2-dimensional tiling technique for SSMKs. The adaptation leverages the concept of a residue matrix: a data structure that compactly captures the pattern of non-zeros in the sparse matrix. With residues, we show it is possible to make intelligent decisions on adaptive tile sizes, resulting in increased cache occupancy. Residues allow for optimizations across both spatial and temporal locality. RASSM improves data movement and overall performance as compared to prior techniques. For example, using spatial analysis for SpMM on commodity server CPUs, RASSM has 1.30X speedup over MKL, 1.32X over J-Stream, 1.20X over ASpT, 1.11X over CSF-4 uniform-shape, and 1.10X over CSF-4 uniform-occupancy. RASSM with temporalanalysis improves this to 1.36X (vs. MKL), 1.38X (vs. J-Stream), 1.26X (vs. ASpT), 1.17X (vs. CSF-4 uniform-shape), and 1.16X (vs. CSF-4 uniform-occupancy).
We study the problem of agnostic PAC reinforcement learning (RL): given a policy class Pi, how many rounds of interaction with an unknown MDP (with a potentially large state and action space) are required to learn an epsilon-suboptimal policy with respect to Pi? Towards that end, we introduce a new complexity measure, called the spanning capacity, that depends solely on the set Pi and is independent of the MDP dynamics. With a generative model, we show that the spanning capacity characterizes PAC learnability for every policy class Pi. However, for online RL, the situation is more subtle. We show there exists a policy class Pi with a bounded spanning capacity that requires a superpolynomial number of samples to learn. This reveals a surprising separation for agnostic learnability between generative access and online access models (as well as between deterministic/stochastic MDPs under online access). On the positive side, we identify an additional sunflower structure which in conjunction with bounded spanning capacity enables statistically efficient online RL via a new algorithm called POPLER, which takes inspiration from classical importance sampling methods as well as recent developments for reachable-state identification and policy evaluation in reward-free exploration.
Optimizing quantum circuits is critical: the number of quantum operations needs to be minimized for a successful evaluation of a circuit on a quantum processor. In this paper we unify two disparate ideas for optimizing quantum circuits, rewrite rules, which are fast standard optimizer passes, and unitary synthesis, which is slow, requiring a search through the space of circuits. We present a clean, unifying framework for thinking of rewriting and resynthesis as abstract circuit transformations. We then present a radically simple algorithm, guoq, for optimizing quantum circuits that exploits the synergies of rewriting and resynthesis. Our extensive evaluation demonstrates the ability of guoq to strongly outperform existing optimizers on a wide range of benchmarks.
New low-precision accelerators, vector instruction sets, and library functions make maximizing accuracy and performance of numerical code increasingly challenging. Two lines of work---traditional compilers and numerical compilers---attack this problem from opposite directions. Traditional compiler backends optimize for specific target environments but are limited in their ability to balance performance and accuracy. Numerical compilers trade off accuracy and performance, or even improve both, but ignore the target environment. We join aspects of both to produce Chassis, a target-aware numerical compiler. Chassis compiles mathematical expressions to operators from a target description, which lists the real expressions each operator approximates and estimates its cost and accuracy. Chassis then uses an iterative improvement loop to optimize for speed and accuracy. Specifically, a new instruction selection modulo equivalence algorithm efficiently searches for faster target-specific programs, while a new cost-opportunity heuristic supports iterative improvement. We demonstrate Chassis capabilities on 9 different targets, including hardware ISAs, math libraries, and programming languages. Chassis finds better accuracy and performance trade-offs than both Clang (by 3.5×) or Herbie (by up to 2.0×) by leveraging low-precision accelerators, accuracy-optimized numerical helper functions, and library subcomponents.
The protein scaffold filling problem remains a significant challenge in computational proteomics, which is critical for accurate protein function prediction and drug design. Despite recent advancements, current sequencing methods often yield incomplete protein sequences, referred to as scaffolds, which require precise filling for further analysis. This paper presents a web-based application, implemented using the Django framework, adopting our previously developed machine learning and deep learning techniques for protein scaffold filling. The platform allows users to try our pre-trained models or train models on their datasets for new scaffolds. This system provides a versatile tool for researchers in computational proteomics, enhancing the efficiency of protein sequence prediction. The developed web application can be accessed through https://psf.ncat.edu/.
Inverter-based resources (IBRs) exhibit distinct short-circuit characteristics that challenge traditional protective relays designed for systems dominated by synchronous generators. While research often focuses on IBRs’ positive-sequence currents during faults, their zero- and negative-sequence responses under unsymmetrical faults remain underexplored. Factors such as transformer configurations and grounding methods further complicate the design of protection schemes relying on these sequence components. This paper enhances the understanding of IBR short-circuit behavior during both symmetrical and unsymmetrical faults and investigates the impact of various transformer configurations on these behaviors. We highlight the limitations of traditional protective relays in safeguarding IBRs due to their constrained fault current levels, minimal negative-sequence components, and, in many cases, the absence of zero-sequence currents. To address these challenges, a novel incremental focused directional protection scheme is introduced. This approach offers enhanced fault detection capabilities under the complex conditions posed by high renewable energy penetration and diverse transformer configurations. The proposed method provides a robust solution for ensuring reliable protection in modern power systems with high integration of IBRs, contributing to improved grid stability and resilience. 
Federated learning (FL) enables multiple parties to collaboratively train machine learning models while preserving data privacy. However, securing communication within FL frameworks remains a significant challenge due to potential vulnerabilities to data breaches and integrity attacks. This paper proposes a novel approach using Dilithium, a robust digital signature framework, to enhance data security in FL. By integrating Dilithium into FL protocols, this study demonstrates robust communication security, preventing data tampering and unauthorized access, thereby promoting safer and more efficient collaborative model training across distributed networks. Furthermore, our approach incorporates an optimized client selection algorithm and a parallelized GPU-based training process that reduces latency and ensures seamless synchronization among participants. Experimental results demonstrate that our system achieves a total processing time of 6.891 seconds, significantly outperforming the 10.24 seconds of normal FL and 12.32 seconds of FL-Dilithium systems on the same computing platforms. Additionally, the proposed model achieves an accuracy of 94%, surpassing the 93% of the normal FL. 
Economic constraints on recruiting experts hinder efforts to build qualified datasets for utilizing AI in professional domains (e.g., medical diagnosis), which could provide societal benefits. To solve this issue, previous studies introduced crowdsourcing and AI to enable non-experts to perform expert-level data labeling. Yet, they encountered three challenges: 1) the limited applicability of crowdsourcing in less specialized domains (e.g., identifying animal species); 2) the chicken-and-egg problem, a paradox where high-performance AI is required to build a dataset to train such AI; and 3) over-reliance on AI, where non-experts, lacking expertise, may incorrectly label data when guided by sub-optimal AI. To address this, we introduce DANNY (Data ANnotation for Non-experts made easY), an AI-based tool designed to help non-experts label an arthritis dataset, aiming to increase labeling accuracy and mitigate over-reliance on AI. By externalizing a cognitive forcing intervention to foster critical thinking, DANNY provides two visualizations: 1) the Criteria phase, where non-experts define criteria across four arthritis features, and 2) the Correction phase, where they refine these criteria by comparing them to AI suggestions. In a study with 28 participants, DANNY users achieved higher accuracy and a more appropriate reliance on AI dependency than control groups. A follow-up study with 12 participants demonstrates how DANNY can be used to improve AI with an ensemble method. Our findings contribute new insights into using AI to support non-experts in labeling domain-specific data when expert resources are limited.
The rise of end-user applications powered by large language models (LLMs), including both conversational interfaces and add-ons to existing graphical user interfaces (GUIs), introduces new privacy challenges. However, many users remain unaware of the risks. This paper explores methods to increase user awareness of privacy risks associated with LLMs in end-user applications. We conducted five co-design workshops to uncover user privacy concerns and their demand for contextual privacy information within LLMs. Based on these insights, we developed CLEAR (Contextual LLM-Empowered Privacy Policy Analysis and Risk Generation), a just-in-time contextual assistant designed to help users identify sensitive information, summarize relevant privacy policies, and highlight potential risks when sharing information with LLMs. We evaluated the usability and usefulness of CLEAR across two example domains: ChatGPT and the Gemini plugin in Gmail. Our findings demonstrated that CLEAR is easy to use and improves users’ understanding of data practices and privacy risks. We also discussed LLM’s duality in posing and mitigating privacy risks, offering design and policy implications.
Recently, several nanostructures with subwavelength scale dimensions were demonstrated to efficiently enhance light chirality. However, the intrinsic lossy nature of metals and inherent narrowband response of dielectric nanostructures pose severe limitations toward the practical realization of chiroptical systems. In our talk, we demonstrate a new way to tackle these problems by designing and experimentally realizing all-dielectric silicon-based L-shaped optical metamaterials based on tilted nanopillars that exhibit broadband and enhanced chirality in transmission operation. We use an emerging bottom-up fabrication approach, named glancing angle deposition, to assemble these new dielectric metamaterials on a wafer scale. We experimentally measure the strong and tunable chiral responses and theoretically explain our findings. The reported strong chirality can be tailored in terms of both amplitude and operating frequency. It may also be totally switched by simply varying the shape and dimensions of the nanopillars.
Needle-based interventions, which number over 30 million annually in the US, suffer from a high complication rate of up to 33% due to inadequate visual guidance. The development of a novel imaging platform integrating Optical Coherent Tomography (OCT) within a minimally invasive probe aims to enhance safety and efficiency. This system, utilizing advanced OCT and machine-learning algorithms, provides real-time, high-resolution imaging to assist in procedures like biopsies and epidural placements, demonstrating potential to reduce complications and procedure times significantly.
Liver transplantation for severe hepatic diseases faces a critical shortage of donor livers globally. Utilizing marginal donor livers could alleviate this issue, yet current biopsy-based assessments are limited in evaluating their viability comprehensively. We propose employing polarization-sensitive optical coherence tomography (PS-OCT) to noninvasively scan multiple regions of donor livers, providing detailed microstructural and tissue property evaluations. Our approach integrates texture feature extraction and machine learning to correlate PS-OCT findings with pathological assessments, demonstrating its potential to enhance pre-transplantation viability evaluations. PS-OCT offers a promising tool for transplant clinics, offering precise, noninvasive insights into liver tissue quality across entire donor organs.
GaN has recently been shown to host bright, photostable, defect single-photon emitters in the 600–700 nm wavelength range that are promising for quantum applications. Our studies have revealed the optical dipole structure, mechanisms associated with optical dipole dephasing, and the spin structure of these emitters. We have also discovered optically detected magnetic resonance (ODMR) in two distinct species of defects. In one group, we found negative optically detected magnetic resonance of a few percent associated with a metastable electronic state, whereas in the other, we found positive optically detected magnetic resonance of up to 30% associated with the ground and optically excited electronic states. We also established coherent control over a single defect’s ground-state spin. In this talk, we will present our results on the basic physics of these defects and also discuss the spin physics associated with the observed ODMR.
Pre-silicon tools for hardening hardware against side-channel and fault injection attacks have become popular recently. However, the security of the system is still threatened by sophisticated physical attacks, which exploit the physical layer characteristics of the computing system beyond the integrated circuits (ICs) and, therefore, bypass the conventional countermeasures. Further, environmental conditions for the hardware can also impact side-channel leakage and fault vulnerability in unexpected ways that are challenging to model in pre-silicon. Thus, attacks cannot be addressed solely by conventional countermeasures at higher layers of the compute stack due to the lack of awareness about the events occurring at the physical layer during runtime. In this paper, we first discuss why the current pre-silicon security and verification tools might fail to achieve security against physical threats in the post-silicon phase. Afterward, we provide insights from the fields of power/signal integrity (PI/SI), and failure analysis (FA) to understand the fundamental issue with the failed current practices. We argue that hardware-based moving target defenses (MTDs) to randomize the physical fabric’s characteristics of the system can mitigate such unaccounted post-silicon threats. We show the effectiveness of such an approach by presenting the results of two case studies in which we perform powerful attacks, such as impedance analysis and laser voltage probing. Finally, we review the overhead of our proposed approach and show that the imposed overhead by MTD solutions can be addressed by making them active only when a threat is detected.
In contrast to uniform distribution in power wires, actual currents tend to exhibit complicated crowding phenomena at the connections between Through-silicon-via (TSV) and power wires. The current crowding effect degrades power integrity and increases the difficulty of 3D IC power delivery network (PDN) analysis. Therefore, a detailed analysis of current distribution and IR drops in power TSVs within 3D IC PDN is important. This paper will explore the complicated current behavior within TSVs and PDNs of the promising face-to-face 3D IC architecture. Since existing simulation methods are computationally intensive and time-consuming, we propose a graph attention network-based (GAT-based) framework, with novel aggregation methods in the GAT models and informative fine-grained graph generation methods, to achieve efficient analysis of current crowding and IR drops in face-to-face 3D IC TSVs. For current density and voltage predictions, the proposed framework attains R2 scores of 0.9776 and 0.9952 compared to ground truth results, respectively. Our framework also demonstrates over 837× speedup than ANSYS Q3D Extractor. Furthermore, the proposed framework outperforms other machine learning-based (ML-based) methods, including the state-of-the-art method.
In this paper, we consider a setting in which geographically constrained “local” wireless services operate in a shared spectrum band and compete in the same market for customers who fall within their local coverage areas. When their desired coverage areas overlap, there are multiple ways that spectrum usage could be coordinated. We discuss ways in which this coordination could arise. We then characterize the market impacts of different forms of coordination via a framework of Cournot competition with congestion. Our analysis illustrates the economic trade-offs of different coordination mechanisms for local services.
We present a new geometric interpretation of Markov Decision Processes (MDPs) with a natural normalization procedure that allows us to adjust the value function at each state without altering the advantage of any action with respect to any policy. This advantage-preserving transformation of the MDP motivates a class of algorithms which we call Reward Balancing, which solve MDPs by iterating through these transformations, until an approximately optimal policy can be trivially found. We provide a convergence analysis of several algorithms in this class, in particular showing that for MDPs for unknown transition probabilities we can improve upon state-of-the-art sample complexity results.
Computing distances and finding shortest paths in massive real-world networks is a fundamental algorithmic task in network analysis. There are two main approaches to solving this task. On one end are traversal-based algorithms like bidirectional breadth-first search (BiBFS), which have no preprocessing step but are slow on individual distance inquiries. On the other end are indexing-based approaches, which create and maintain a large index. This allows for answering individual inquiries very fast; however, index creation is prohibitively expensive. We seek to bridge these two extremes: quickly answer distance inquiries without the need for costly preprocessing. We propose a new algorithm and data structure, WormHole, for approximate shortest path computations. WormHole leverages structural properties of social networks to build a sublinearly sized index, drawing upon the core-periphery decomposition of Ben-Eliezer et al. [10]. Empirically, WormHole's preprocessing time improves upon index-based solutions by orders of magnitude: indexing billion edges graphs takes only a few minutes. Real time performance is consistently much faster than in BiBFS. The acceleration comes at the cost of a minor accuracy trade-off. We complement these empirical results with provable theoretical guarantees.