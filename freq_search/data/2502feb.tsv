The retrospective is a crucial component of the agile software development process. In previous studies of retrospectives in undergraduate team software development projects, students exhibited limited and shallow reflection. We speculate that this is due to students' limited experience with reflection and the absence of clear guidance for engaging in deep reflection during agile retrospectives. To explore the potential for a pedagogical intervention to foster deeper reflection in retrospectives, we present an empirical comparison of a standard retrospective model against an enhanced retrospective model that scaffolds deeper levels of reflection by prompting students to justify and critique their practices and weigh alternative approaches. Through a systematic classification of the reflection level of statements made during individual brainstorming and team discussion phases of retrospectives, our study found that the enhanced model led to individuals and teams engaging in significantly higher levels of reflection. Our findings contribute to improving software engineering education by demonstrating the efficacy of an enhanced pedagogical model for team retrospectives. 
This paper offers a comparative study of two soils- Glauconite and Ottawa F65- utilizing X-ray micro-computed tomography (µCT) scan. The tendency of glauconite sand to transform from coarse to fine-grained material through particle crushing poses challenges in terms of stability and strength, particularly in foundation engineering and offshore site investigation. This paper investigates the particle size distribution and explores the subtleties of particle characteristics. Non-invasive µCT and 3D image analysis are used to measure and compare particle shape parameters: median aspect ratio (0.56 for Glauconite,0.54 for Ottawa F65), median convexity is 0.86 for both soils, and median sphericity (0.81 for Glauconite, 0.83 for Ottawa F65). By drawing comparisons between the statistical data of particle shape parameters from both soils, insights are gained into their morphological characteristics. Additionally, fitted Johnson distributions are provided for 3D Aspect ratio, sphericity, and convexity which may be useful for discrete element method modeling of these soils. 
Homomorphic encryption enables computations on the ciphertext to preserve data privacy with significant computational overhead compared to plaintext computations. In response to this challenge, we present HEDWIG, a novel end-to-end hardware acceleration system designed to enhance the efficiency of homomorphic encryption, specifically tailored for the latest BFV-HPS scheme. To the best of our knowledge, this is the first hardware accelerator for the BFV-HPS scheme variant. Key contributions include computation workload reduction via approximation, hardware resource optimization on modular computation modules, and computation workflow optimization to improve HE algorithm efficiency. The proposed HEDWIG is employed to perform homomorphic evaluations over machine learning model inference with a high reduction in DSP usage compared to prior BFV RNS variants FPGA accelerator with clock frequency reached 300MHz.
As AI continues to grow, modern applications are becoming more data- and compute-intensive, driving the development of specialized AI chips to meet these demands. One example is AMD's AI Engine (AIE), a dedicated hardware system that includes a 2D array of high-frequency very-long instruction words (VLIW) vector processors to provide high computational throughput and reconfigurability. However, AIE's specialized architecture presents tremendous challenges in programming and compiler optimization. Existing AIE programming frameworks lack a clean abstraction to represent multi-level parallelism in AIE; programmers have to figure out the parallelism within a kernel, manually do the partition, and assign sub-tasks to different AIE cores to exploit parallelism. These significantly lower the programming productivity. Furthermore, some AIE architectures include FPGAs to provide extra flexibility, but there is no unified intermediate representation (IR) that captures these architectural differences. As a result, existing compilers can only optimize the AIE portions of the code, overlooking potential FPGA bottlenecks and leading to suboptimal performance. To address these limitations, we introduce ARIES, an agile multi-level intermediate representation (MLIR) based compilation flow for reconfigurable devices with AIEs. ARIES introduces a novel programming model that allows users to map kernels to separate AIE cores, exploiting task- and tile-level parallelism without restructuring code. It also includes a declarative scheduling interface to explore instruction-level parallelism within each core. At the IR level, we propose a unified MLIR-based representation for AIE architectures, both with or without FPGA, facilitating holistic optimization and better portability across AIE device families. For the General Matrix Multiply (GEMM) benchmark, ARIES achieves 4.92 TFLOPS, 15.86 TOPS, and 45.94 TOPS throughput under FP32, INT16, and, INT8 data types on Versal VCK190 respectively. Compared with the state-of-the-art (SOTA) work CHARM for AIE, ARIES improves the throughput by 1.17x, 1.59x, and 1.47x correspondingly. For ResNet residual layer, ARIES achieves up to 22.58x speedup compared with optimized SOTA work Riallto on Ryzen-AI NPU. ARIES is open-sourced on GitHub: https://github.com/arc-research-lab/Aries. 
Real-time requirements are essential for safety-critical systems such as autonomous driving and robotics. In these systems, schedulability - ensuring every task completes before its deadline - is critical, as missing even a single deadline can lead to catastrophic consequences. Unlike Quality-of-Service (QoS) or service-level agreement (SLA) metrics, where occasional deadline misses are tolerable, time-predictable safety-critical systems demand strict adherence to deadlines. Existing multi-task scheduling frameworks, optimized for QoS/SLA, fail to meet these requirements, even when incorporating real-time techniques like preemption and scheduling. In FPGA-based systems, accelerators are widely used to handle real-time computational tasks. While existing accelerators focus on low latency and high throughput, these optimizations are insufficient when multiple tasks share a single accelerator, as low latency alone does not ensure schedulability. For example in Fig.1(a), a system with tasks of 20 ms and 400 ms periods, requiring 10 ms and 200 ms execution times, will miss deadlines on a 1 TOPS accelerator. Even increasing throughput to 10 TOPS (Fig.1(b)) without proper scheduling fails to resolve the issue. Furthermore, without a theoretical analysis, designers cannot determine whether the system after latency optimization can meet the deadline. As a result, these latency-optimized accelerators can not be safely used in time-predictable, safety-critical real-time systems. To address these challenges, we propose the ART framework: Accelerator Customization for Real-Time Systems. ART enables FPGA accelerators to support Earliest Deadline First (EDF) scheduling and limited preemption mechanisms, specifically targeting real-time safety-critical systems. Alongside hardware support, ART provides a software framework to analyze schedulability and balance between schedulability and performance. As shown in Figure 1(c), a 1 TOPS accelerator with ART can satisfy the schedulability requirement. Whereas the latency-optimized accelerator without ART in Figure 1(b) can not satisfy and thus is not viable in this scenario. Our contributions are summarized as follows: Necessity analysis of scheduling and preemption mechanism: We conduct a detailed case study to understand the necessity of scheduling and preemption. ART hardware framework and accelerator system: We propose the ART hardware framework and accelerator system for EDF scheduling and limited preemption on FPGA. ART can transform a baseline HLS accelerator to support these features. ART software framework: To guarantee the schedulability in real-time scenarios, we propose the ART software framework. We migrate the schedulability analysis and optimal preemption point placements to hardware acceleration, explain their effectiveness and package these algorithms into an automated framework. ART implementations and experiments: We implement and evaluate the ART accelerator system in AMD Zynq ZCU102, and Versal VCK190 platforms. The onboard experiment shows that ART guarantees the schedulability whereas the corresponding baseline accelerators can not. ART is lightweight, utilizing < 0.4% of resources on ZCU102 and < 0.1% on VCK190.
The Central High Plains are experiencing severe droughts due to climate change and diminishing water resources, impacting rural livelihoods. This study seeks to address these challenges by investigating biochar amendments to enhance the water-holding capacity (WHC) of sandy and loamy soils. Laboratory experiments, including gravimetric WHC tests, pressure plate tests, and contact angle measurements, were conducted using biochar from wood-based, oat hull, and sawdust feedstocks. Factors such as feedstock type, application rate, particle size, porosity, and hydrophobicity were analyzed. Results indicate that a 3% application rate of fine biochar particles (<0.6 mm) may significantly enhance WHC, with oat hull biochar showing the highest effectiveness due to its good combination of hydrophilicity, micro-porosity, and particle shape. These findings suggest that biochar properties significantly influence the WHC of soils, offering a sustainable solution for drought mitigation in the Central High Plains. Future studies should explore biochar and soil-wetting bacteria interactions to further enhance WHC, providing a robust strategy for water-scarce regions.
Federated Learning (FL) aims to train a shared model using data and computation power on distributed agents coordinated by a central server. Decentralized FL (DFL) utilizes local model exchange and aggregation between agents to reduce the communication and computation overheads on the central server. However, when agents are mobile, the communication opportunity between agents can be sporadic, largely hindering the convergence and accuracy of DFL. In this paper, we study delay-tolerant model spreading and aggregation enabled by model caching on mobile agents. Each agent stores not only its own model, but also models of agents encountered in the recent past. When two agents meet, they exchange their own models as well as the cached models. Local model aggregation works on all models in the cache. We theoretically analyze the convergence of DFL with cached models, explicitly taking into account the model staleness introduced by caching. We design and compare different model caching algorithms for different DFL and mobility scenarios. We conduct detailed case studies in a vehicular network to systematically investigate the interplay between agent mobility, cache staleness, and model convergence. In our experiments, cached DFL converges quickly, and significantly outperforms DFL without caching. 
Offshore wind turbines have become a feasible solution to meet the growing demands for renewable energy. However, offshore wind turbines with fixed foundations become increasingly economically and technically unfeasible at locations with large water depth. At these locations, floating platforms supported by mooring system and subsea anchors are a more feasible solution. Deeply embedded ring-shaped anchors can have greater efficiency than piles and caissons and greater capacity than drag anchors. In this paper, the result of a series of monotonic centrifuge load tests in clay is presented, with a focus on evaluation of the effect of loading inclination and anchor embedment depth on the tensile capacity and load-displacement response of the ring anchors. Tests were performed at the UC Davis Center for Geotechnical Modeling (CGM) with a scaling of 70g. The ring anchor models were embedded in normally consolidated kaolin clay. The clay shear strength was estimated from T-bar soundings performed in-flight. The anchors were connected to an actuator using taut steel wire ropes, and the line load, displacement, and inclination were measured. The results indicate that the capacity of the ring anchors increases as the loading inclination changes from vertical to horizontal. Increasing the embedment depth also resulted in an increase in capacity; however, the capacity normalized by the clay’s undrained shear strength is independent of depth. The interaction diagram describing the capacity as a function of load inclination and depth indicates that the ring anchor’s horizontal tensile capacity is much greater than its vertical capacity. Furthermore, analyses also show that the system stiffness increases as the load inclination angle increases. Overall, the results of this study show that the ring anchor could be a potential foundation solution with greater material efficiency than other alternatives.
Every year, humanity clears 10 million hectares of forests, which releases more than 5.6 billion tonnes of greenhouse gases. This significant contribution to climate change has led to the passage of global regulations, such as the EUDR, which aims to ensure that products linked to deforestation are excluded from the European market. Satellite-based remote sensing tools are popularly used for global monitoring to enable such compliance. However, they struggle to differentiate vegetation types in farms and orchards from forests (Fig.1.A). To solve this, we develop TerraTrace, a temporal signature mapping tool that combines Spectral Vegetation Indices, Satellite Imagery, and open data like Cropland Data Layer (CDL) to estimate historical land use. The key insight is that satellite-based spectral index data shows temporal variables like agricultural practices and plant growth cycles. Specifically, we demonstrate that yearly patterns of the Normalized Difference Vegetation Index (NDVI), based on plant photosynthesis, have temporal signatures unique to different crops, can distinguish forests from crops, and follow consistent patterns across different locations. Leveraging this we make the following contributions (Fig.2):
Developing a repertoire of notional machines (i.e., pedagogical tools for teaching programming) is essential for new computer science (CS) educators. However, there is a lack of documentation of notional machines and related pedagogical content knowledge (i.e., insights into teaching CS content). Our experience report addresses this lack of documentation and captures insights from our professional learning community. We co-designed an approach to use physical objects to teach inheritance in Java. Unlike a research paper that would rigorously document a few student outcomes with the expectation that these would generalize, our experience report shares our observations from multiple years of teaching with the goal of providing a number of things for educators to consider when teaching inheritance in Java. Drawing from an analysis of our meeting notes, we describe our instructional sequence, our perceptions of its current strengths and weaknesses for supporting students’ learning, insights from our previous failed attempts, and eight pedagogical practices. 
We present IMPS: Immersive Media Player System, a tightly synchronized 360º media player that leverages Android VR headsets to deliver immersive educational experiences. Designed for deployment in classrooms, IMPS allows instructors to manage synchronized playback for up to 50 headsets using a tablet interface. The system’s synchronization algorithm ensures lockstep playback across devices within 10 ms, addressing audio and video desynchronization issues of previous systems. IMPS has been successfully deployed by the Act One non-profit to deliver VR content to Title I schools in Arizona and is also used at Arizona State University for synchronized playback of 360º media in educational settings. 
Volumetric streaming is a powerful medium that transmits volumetric data, which primarily includes color and depth information, over a network in real-time. While color data can be effectively compressed using standard video codecs, compressing depth data poses a significant challenge as we need to change the bitrate for transmission over standard video codecs. Streaming depth data using standard video codecs introduces visual artifacts that degrade the quality of volumetric streaming workflows. To address this issue, we propose a demonstration exploring efficient compression techniques for color and depth data that minimize visual artifacts while preserving visual fidelity. Using a physical tabletop game filmed with volumetric cameras, we compare two voumetric streaming video workflows. The first workflow streams and renders the color and depth data directly from the camera in an uncompressed format, which is used as the ground truth. The second workflow encodes and streams the data in a compressed format. The comparison is conducted on a desktop system to evaluate performance for analyzing factors such as data transmission, compression quality, and overall system responsiveness.
AutoCalNet enables continuous real-time calibration of mobile 3D cameras by decoupling calibration from content streaming. It leverages a scalable device-edge-cloud network to minimize bandwidth, manage latency, and maintain high precision in calibration data, prioritizing trackable regions and feature points that will facilitate spatiotemporal tracking. This approach provides a flexible, efficient solution for networked camera systems without being constrained by content-specific requirements. 
Deep learning algorithms have depicted commendable performance in a variety of computer vision applications. However, training a robust deep neural network necessitates a large amount of labeled training data, which is timeconsuming and labor-intensive to acquire. This problem is even more serious for an application like image segmentation, as the human oracle has to hand-annotate each and every pixel in a given training image, which is extremely laborious. Active learning algorithms automatically identify the salient and exemplar samples from large amounts of unlabeled data, and tremendously reduce human annotation effort in inducing a machine learning model. In this paper, we propose a novel active learning algorithm for image segmentation, with the goal of further reducing the labeling burden on the human oracles. Our framework identifies a batch of informative images, together with a list of semantic classes for each, and the human annotator merely needs to answer whether a given semantic class is present or absent in a given image. To the best of our knowledge, this is the first research effort to develop an active learning framework for image segmentation, which poses only binary (yes/no) queries to the users. We pose the image and class selection as a constrained optimization problem and derive a linear programming relaxation to select a batch of (image-class) pairs, which are maximally informative to the underlying deep neural network. Our extensive empirical studies on three challenging datasets corroborate the potential of our method in substantially reducing human annotation effort for real-world image segmentation applications.
Test Time Adaptation (TTA) has emerged as a practical solution to mitigate the performance degradation of Deep Neural Networks (DNNs) in the presence of corruption/ noise affecting inputs. Existing approaches in TTA continuously adapt the DNN, leading to excessive resource consumption and performance degradation due to accumulation of error stemming from lack of supervision. In this work, we propose Domain-Aware Real- Time Dynamic Adaptation (DARDA) to address such issues. Our key approach is to proactively learn latent representations of some corruption types, each one associated with a sub-network state tailored to correctly classify inputs affected by that corruption. After deployment, DARDA adapts the DNN to previously unseen corruptions in an unsupervised fashion by (i) estimating the latent representation of the ongoing corruption; (ii) selecting the sub-network whose associated corruption is the closest in the latent space to the ongoing corruption; and (iii) adapting DNN state, so that its representation matches the ongoing corruption. This way, DARDA is more resource-efficient and can swiftly adapt to new distributions caused by different corruptions without requiring a large variety of input data. Through experiments with two popular mobile edge devices - Raspberry Pi and NVIDIA Jetson Nano - we show that DARDA reduces energy consumption and average cache memory footprint respectively by 1.74 x and 2.64 x with respect to the state of the art, while increasing the performance by 10.4%, 5.7% and 4.4% on CIFAR-10, CIFAR-100 and TinyImagenet.
The fast-paced development and deployment of private messaging applications demands mechanisms to protect against the concomitant potential for abuse. While widely used end-to-end encrypted (E2EE) messaging systems have deployed mechanisms for users to verifiably report abusive messages without compromising the privacy of unreported messages, abuse reporting schemes for systems that additionally protect message metadata are still in their infancy. Existing solutions either focus on a relatively small portion of the design space or incur much higher communication and computation costs than their E2EE brethren. This paper introduces new abuse reporting mechanisms that work for any private messaging system based on onion encryption. This includes low-latency systems that employ heuristic or opportunistic mixing of user traffic, as well as schemes based on mixnets. Along the way, we show that design decisions and abstractions that are well-suited to the E2EE setting may actually impede security and performance improvements in the metadata-hiding setting. We also explore stronger threat models for abuse reporting and moderation not explored in prior work, showing where prior work falls short and how to strengthen both our scheme and others'—including deployed E2EE messaging platforms—to achieve higher levels of security. We implement a prototype of our scheme and find that it outperforms the best known solutions in this setting by well over an order of magnitude for each step of the message delivery and reporting process, with overheads almost matching those of message franking techniques used by E2EE encrypted messaging apps today. 
A recent work introduces the problem of learning set functions from data generated by a so-called optimal subset oracle. Their approach approximates the underlying utility function with an energy-based model, whose parameters are estimated via mean-field variational inference. This approximation reduces to fixed point iterations; however, as the number of iterations increases, automatic differentiation quickly becomes computationally prohibitive due to the size of the Jacobians that are stacked during backpropagation. We address this challenge with implicit differentiation and examine the convergence conditions for the fixed-point iterations. We empirically demonstrate the efficiency of our method on synthetic and real-world subset selection applications including product recommendation, set anomaly detection and compound selection tasks. 
Continual learning (CL) learns a sequence of tasks incre- mentally. This paper studies the challenging CL setting of class-incremental learning (CIL). CIL has two key chal- lenges: catastrophic forgetting (CF) and inter-task class sep- aration (ICS). Despite numerous proposed methods, these issues remain persistent obstacles. This paper proposes a novel CIL method, called Kernel Linear Discriminant Analy- sis (KLDA), that can effectively avoid CF and ICS problems. It leverages only the powerful features learned in a foundation model (FM). However, directly using these features proves suboptimal. To address this, KLDA incorporates the Radial Basis Function (RBF) kernel and its Random Fourier Fea- tures (RFF) to enhance the feature representations from the FM, leading to improved performance. When a new task ar- rives, KLDA computes only the mean for each class in the task and updates a shared covariance matrix for all learned classes based on the kernelized features. Classification is performed using Linear Discriminant Analysis. Our empir- ical evaluation using text and image classification datasets demonstrates that KLDA significantly outperforms baselines. Remarkably, without relying on replay data, KLDA achieves accuracy comparable to joint training of all classes, which is considered the upper bound for CIL performance. The KLDA code is available at https://github.com/salehmomeni/klda. 
Large language models (LLMs) are expected to follow in- structions from users and engage in conversations. Tech- niques to enhance LLMs’ instruction-following capabilities typically fine-tune them using data structured according to a predefined chat template. Although chat templates are shown to be effective in optimizing LLM performance, their impact on safety alignment of LLMs has been less understood, which is crucial for deploying LLMs safely at scale. In this paper, we investigate how chat templates affect safety alignment of LLMs. We identify a common vulnerability, named ChatBug, that is introduced by chat templates. Our key insight to identify ChatBug is that the chat templates provide a rigid format that need to be followed by LLMs, but not by users. Hence, a malicious user may not necessar- ily follow the chat template when prompting LLMs. Instead, malicious users could leverage their knowledge of the chat template and accordingly craft their prompts to bypass safety alignments of LLMs. We study two attacks to exploit the ChatBug vulnerability. Additionally, we demonstrate that the success of multiple existing attacks can be attributed to the ChatBug vulnerability. We show that a malicious user can exploit the ChatBug vulnerability of eight state-of-the- art (SOTA) LLMs and effectively elicit unintended responses from these models. Moreover, we show that ChatBug can be exploited by existing jailbreak attacks to enhance their at- tack success rates. We investigate potential countermeasures to ChatBug. Our results show that while adversarial train- ing effectively mitigates the ChatBug vulnerability, the vic- tim model incurs significant performance degradation. These results highlight the trade-off between safety alignment and helpfulness. Developing new methods for instruction tuning to balance this trade-off is an open and critical direction for future research. 
Metric magnitude is a measure of the “size” of point clouds with many desirable geometric properties. It has been adapted to various mathematical contexts and recent work suggests that it can enhance machine learning and optimization algorithms. But its usability is limited due to the computational cost when the dataset is large or when the computation must be carried out repeatedly (e.g. in model training). In this paper, we study the magnitude computation problem, and show efficient ways of approximating it. We show that it can be cast as a convex optimization problem, but not as a submodular optimization. The paper describes two new algorithms – an iterative approximation algorithm that converges fast and is accurate, and a subset selection method that makes the computation even faster. It has been previously proposed that magnitude of model sequences generated during stochastic gradient descent is correlated to generalization gap. Extension of this result using our more scalable algorithms shows that longer sequences in fact bear higher correlations. We also describe new applications of magnitude in machine learning – as an effective regularizer for neural network training, and as a novel clustering criterion. 