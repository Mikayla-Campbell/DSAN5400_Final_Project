We introduce the CRONOS algorithm for convex optimization of two-layer neural networks. CRONOS is the first algorithm capable of scaling to high-dimensional datasets such as ImageNet, which are ubiquitous in modern deep learning. This significantly improves upon prior work, which has been restricted to downsampled versions of MNIST and CIFAR-10. Taking CRONOS as a primitive, we then develop a new algorithm called CRONOS-AM, which combines CRONOS with alternating minimization, to obtain an algorithm capable of training multilayer networks with arbitrary architectures. Our theoretical analysis proves that CRONOS converges to the global minimum of the convex reformulation under mild assumptions. In addition, we validate the efficacy of CRONOS and CRONOS-AM through extensive large-scale numerical experiments with GPU acceleration in JAX. Our results show that CRONOS-AM can obtain comparable or better validation accuracy than predominant tuned deep learning optimizers on vision and language tasks with benchmark datasets such as ImageNet and IMDb. To the best of our knowledge, CRONOS is the first algorithm which utilizes the convex reformulation to enhance performance on large-scale learning tasks
We consider the problem of determining a binary ground truth using advice from a group of independent reviewers (experts) who express their guess about a ground truth correctly with some independent probability (competence) pi. In this setting, when all reviewers are competent with p≥0.5, the Condorcet Jury Theorem tells us that adding more reviewers increases the overall accuracy, and if all pi's are known, then there exists an optimal weighting of the reviewers. However, in practical settings, reviewers may be noisy or incompetent, i.e., pi≤0.5, and the number of experts may be small, so the asymptotic Condorcet Jury Theorem is not practically relevant. In such cases we explore appointing one or more chairs (judges) who determine the weight of each reviewer for aggregation, creating multiple levels. However, these chairs may be unable to correctly identify the competence of the reviewers they oversee, and therefore unable to compute the optimal weighting. We give conditions on when a set of chairs is able to weight the reviewers optimally, and depending on the competence distribution of the agents, give results about when it is better to have more chairs or more reviewers. Through simulations we show that in some cases it is better to have more chairs, but in many cases it is better to have more reviewers.
We consider a generalized version of the (weighted) one-center problem on graphs. Given an undirected graph G of n vertices and m edges and a positive integer k≤n, the problem aims to find a point in G so that the maximum (weighted) distance from it to k connected vertices in its shortest path tree(s) is minimized. No previous work has been proposed for this problem except for the case k=n, that is, the classical graph one-center problem. In this paper, an O(mnlognlogmn+m2lognlogmn)-time algorithm is proposed for the weighted case, and an O(mnlogn)-time algorithm is presented for the unweighted case, provided that the distance matrix for G is given. When G is a tree graph, we propose an algorithm that solves the weighted case in O(nlog2nlogk) time with no given distance matrix, and improve it to O(nlog2n) for the unweighted case.
Deep reinforcement learning has demonstrated re- markable achievements across diverse domains such as video games, robotic control, autonomous driving, and drug discovery. Common methodologies in partially observable domains largely lean on end-to-end learning from high-dimensional observations, such as images, without explicitly reasoning about true state. We suggest an alternative direction, introducing the Partially Supervised Reinforcement Learning (PSRL) framework. At the heart of PSRL is the fusion of both supervised and unsupervised learning. The approach leverages a state estimator to distill supervised semantic state information from high-dimensional observations which are often fully observable at training time. This yields more interpretable policies that compose state predictions with control. In parallel, it captures an unsupervised latent representation. These two—the semantic state and the latent state—are then fused and utilized as inputs to a policy network. This juxtaposition offers practitioners a flexible and dynamic spectrum: from emphasizing supervised state information to integrating richer, latent insights. Extensive experimental results indicate that by merging these dual representations, PSRL offers a balance, enhancing interpretability while preserving, and often significantly outperforming, the performance benchmarks set by traditional methods in terms of reward and convergence speed. 
As the scale of production HPC platforms increases, so does the computing and I/O performance gap, exacerbating the storage bottleneck. High-performance storage systems have been developed to alleviate this bottleneck, but many questions remain concerning their architecture, implementation, and configuration. Answering these questions via experimental campaigns proves arduous. First, some answers are required before deploying the system. Second, once a system hits production the experimental scope is limited by the system's specific configuration and by constraints of production use. In this work we identify challenges posed by the design and validation of a storage simulator. We then propose solutions implemented in Fives, a simulator of HPC workloads on platforms that comprise a parallel file system. We show how our simulator can be instantiated and calibrated for the accurate simulation of a production Lustre deployment.
Adversaries minimally perturb deep learning input data to reduce a learning model's ability to produce domain-specific data-driven recommendations to solve specialized tasks. This vulnerability to adversarial perturbations has been argued to stem from a learning model's nonlocal generalization over complex input data. Given the incomplete information in a complex dataset, a learning model captures nonlinear patterns between data points with volatility in the loss surface and exploitable areas of low-confidence knowledge. It is the responsibility of activation functions to capture the nonlinearity in data and, thus, has inspired disjointed research efforts to create robust activation functions. This work unifies the properties of activation functions that contribute to robust generalization with the generalized gamma distribution function. We show that combining the disjointed characteristics presented in the literature with our parametric generalized gamma activation function provides more effective robustness than the individual characteristics alone11The source code for this research effort: https://github.com/sheilaalemany/generalized-gamma-activation.git.
Adversarial perturbations in object recognition and image classification tasks impact a learning model's ability to perform accurately and increase the safety risks of deployed machine learning models. During the adversarial example generation process, adversaries approach areas most prone to model uncertainty. Identifying partially occluded items, especially without understanding general object shapes, contributes to significant model uncertainty since object boundaries are not inherently at the forefront of the feature generalization process in deep learning models. Thus, this work aims to reduce model uncertainty surrounding partially occluded boundaries and increase adversarial robustness by augmenting the training dataset with amodal segmentation boundary masks. By observing performance degradation, robust sensitivity, and loss sensitivity, we show how including these masks during training impacts an adversary's ability to generate effective adversarial examples on the versatile MS COCO dataset. Lastly, we observe how including these masks during training influences the performance of adversarial training.
Parallel applications can spend a significant amount of time performing I/O on large-scale supercomputers. Fast near-compute storage accelerators called burst buffers can reduce the time a processor spends performing I/O and mitigate I/O bottlenecks. However, determining if a given application could be accelerated using burst buffers is not straightforward even for storage experts. The relationship between an application's I/O characteristics (such as I/O volume, processes involved, etc.) and the best storage sub-system for it can be complicated. As a result, adapting parallel applications to use burst buffers efficiently is a trial-and-error process. In this work, we present a Python-based tool called PrismIO that enables programmatic analysis of I/O traces. Using PrismIO, we identify performance bottlenecks when using burst buffers and parallel file systems, and explain why certain I/O patterns perform poorly. Further, we use machine learning to model the relationship between I/O characteristics and file system selections. We use IOR, an I/O benchmark to gather performance data for training the machine learning model. Our model can predict the better performing storage system for unseen IOR scenarios with an accuracy of 94.47% and for four real applications with an accuracy of 95.86%.
The increasing demand for flexible and efficient optical networks has led to the development of Software-Defined Elastic Optical Networks (SD-EONs). These networks leverage the programmability of Software-Defined Networking (SDN) and the adaptability of Elastic Optical Networks (EONs) to optimize network performance under dynamic traffic conditions. However, existing simulation tools often fall short in terms of transparency, flexibility, and advanced functionality, limiting their utility in cutting-edge research. In this paper, we present a Flexible Unified Simulator for Intelligent Optical Networking (FUSION), a fully open-source simulator designed to address these limitations and provide a comprehensive platform for SD-EON research. FUSION integrates traditional routing and spectrum assignment algorithms with advanced machine learning and reinforcement learning techniques, including support for the Stable Baselines 3 library. The simulator also offers robust unit testing, a fully functional Graphical User Interface (GUI), and extensive documentation to ensure usability and reliability. Performance evaluations demonstrate the effectiveness of FUSION in modeling complex network scenarios, showcasing its potential as a powerful tool for advancing SD-EON research. 
Large Language Models (LLMs) are pre-trained on large-scale corpora and excel in numerous general natural language processing (NLP) tasks, such as question answering (QA). Despite their advanced language capabilities, when it comes to domain-specific and knowledge-intensive tasks, LLMs suffer from hallucinations, knowledge cut-offs, and lack of knowledge attributions. Additionally, fine tuning LLMs' intrinsic knowledge to highly specific domains is an expensive and time consuming process. The retrieval-augmented generation (RAG) process has recently emerged as a method capable of optimization of LLM responses, by referencing them to a predetermined ontology. It was shown that using a Knowledge Graph (KG) ontology for RAG improves the QA accuracy, by taking into account relevant sub-graphs that preserve the information in a structured manner. In this paper, we introduce SMART-SLIC, a highly domain-specific LLM framework, that integrates RAG with KG and a vector store (VS) that store factual domain specific information. Importantly, to avoid hallucinations in the KG, we build these highly domain-specific KGs and VSs without the use of LLMs, but via NLP, data mining, and nonnegative tensor factorization with automatic model selection. Pairing our RAG with a domain-specific: (i) KG (containing structured information), and (ii) VS (containing unstructured information) enables the development of domain-specific chat-bots that attribute the source of information, mitigate hallucinations, lessen the need for fine-tuning, and excel in highly domain-specific question answering tasks. We pair SMART-SLIC with chain-of-thought prompting agents. The framework is designed to be generalizable to adapt to any specific or specialized domain. In this paper, we demonstrate the question answering capabilities of our framework on a corpus of scientific publications on malware analysis and anomaly detection.
This paper presents a novel approach for classifying electrocardiogram (ECG) signals in healthcare applications using federated learning and stacked convolutional neural networks (CNNs). Our innovative technique leverages the distributed nature of federated learning to collaboratively train a high-performance model while preserving data privacy on local devices. We propose a stacked CNN architecture tailored for ECG data, effectively extracting discriminative features across different temporal scales. The evaluation confirms the strength of our approach, culminating in a final model accuracy of 98.6% after 100 communication rounds, significantly exceeding baseline performance. This promising result paves the way for accurate and privacy-preserving ECG classification in diverse healthcare settings, potentially leading to improved diagnosis and patient monitoring. 
Microfluidic biochips are widely used in biomedical research, clinical diagnostics, and point-of-care testing. However, their complex supply chains make them vulnerable to counterfeiting, overbuilding, and intellectual property (IP) piracy. We present fluorescent carbon quantum dot (CQD) stickers1 that can be integrated with the polydimethylsiloxane (PDMS) based biochips for authentication. The stickers can be plasma-bonded to biochips made of glass and silicon. A protective spin-coated PDMS layer makes them obscured and tamperproof. However, they are detectable under UV light and can be authenticated via spectral analysis. The scheme exhibits unique excitation-dependent responses associated with the variability of the CQD sizes. This makes it ideal for physical authentication. Reliability studies concerning mechanical, photonic, and thermal degradation have demonstrated highly stable results. The stability of CQDs within the PDMS, their robust excitation-based emission fluorescence response, and the use of waste polypropylene masks make this a sustainable and robust authenticator for biochips.
Variational estimation of a mechanical system is based on the application of variational principles from mechanics to state estimation of the system evolving on its configuration manifold. If the configuration manifold is a Lie group, then the underlying group structure can be used to design nonlinearly stable observers for estimation of configuration and velocity states from measurements. Measured quantities are on a vector space on which the Lie group acts smoothly. We formulate the design of variational observers on a general finite-dimensional Lie group, followed by the design and experimental evaluation of a variational observer for rigid body motions on the Lie group SE(3). 
This paper presents a closed-form notion of controllability and observability for systems with communication delays, actuation delays, and locality constraints. The formulation reduces to classical notions of controllability and observability in the unconstrained setting. As a consequence of our formulation, we show that the addition of locality and communication constraints may not affect the controllability and observability of the system, and we provide an efficient sufficient condition under which this phenomenon occurs. This contrasts with actuation and sensing delays, which cause a gradual loss of controllability and observability as the delays increase. We illustrate our results using linearized swing equations for the power grid, showing how actuation delay and locality constraints affect controllability.
Feedback-evolving games is a framework that models the co-evolution between payoff functions and an environmental state. It serves as a useful tool to analyze many social dilemmas such as natural resource consumption, behaviors in epidemics, and the evolution of biological populations. However, it has primarily focused on the dynamics of a single population of agents. In this paper, we consider the impact of two populations of agents that share a common environmental resource. We focus on a scenario where individuals in one population are governed by an environmentally “responsible” incentive policy, and individuals in the other population are environmentally “irresponsible”. An analysis on the asymptotic stability of the coupled system is provided, and conditions for which the resource collapses are identified. We then derive consumption rates for the irresponsible population that optimally exploit the environmental resource, and analyze how incentives should be allocated to the responsible population that most effectively promote the environment via a sensitivity analysis.
The growing availability of COVID-19 data and advancements in AI offer potential for improved pandemic prediction and prevention. Federated Learning (FL) frameworks support collaborative, privacy-preserving COVID-19 detection, but often neglect the need for simpler models in resource-constrained settings. Knowledge distillation, where a complex "teacher" model transfers insights to a simpler "student" model, struggles to retain detailed information, especially with a single teacher. To address this, we propose a novel FL algorithm, FL-MTKD, which uses multiple teachers to distill knowledge into an efficient 2.5 MB student model. Our results show that while simplified architectures like FL-SimpCNN (2.5 MB) handle non-IID datasets better, larger models like FL-CovidCNN (20.74 MB) and FL-DeepCovid (351.6 MB) perform poorly in such settings. FL-MTKD outperforms other models, achieving 89.74% accuracy and 89.71% F1 score on non-IID datasets, and over 93% accuracy on IID (Independent and Identically Distributed) datasets, offering strong generalization with minimal storage needs. Our developed code can be found from QinggeLab-ACMBCB-24 on github.
Optimal control methods provide solutions to safety-critical problems but easily become intractable. Control Barrier Functions (CBFs) have emerged as a popular technique that facilitates their solution by provably guaranteeing safety, through their forward invariance property, at the expense of some performance loss. This approach involves defining a performance objective alongside CBF-based safety constraints that must always be enforced with both performance and solution feasibility significantly impacted by two key factors: (i) the selection of the cost function and associated parameters, and (ii) the calibration of parameters within the CBF-based constraints, which capture the trade-off between performance and conservativeness. To address these challenges, we propose a Reinforcement Learning (RL)-based Receding Horizon Control (RHC) approach leveraging Model Predictive Control (MPC) with CBFs (MPC-CBF). In particular, we parameterize our controller and use bilevel optimization, where RL is used to learn the optimal parameters while MPC computes the optimal control input. We validate our method by applying it to the challenging automated merging control problem for Connected and Automated Vehicles (CAVs) at conflicting roadways. Results demonstrate improved performance and a significant reduction in the number of infeasible cases compared to traditional heuristic approaches used for tuning CBF-based controllers, showcasing the effectiveness of the proposed method. In order to guarantee reproducibility, our code is provided here: link1.1github.com/EhsanSabouni/CDC2024_RL_adpative_MPC_CBF/tree/main
This paper investigates how to incorporate expert observations (without explicit information on expert actions) into a deep reinforcement learning setting to improve sample efficiency. First, we formulate an augmented policy loss combining a maximum entropy reinforcement learning objective with a behavioral cloning loss that leverages a forward dynamics model. Then, we propose an algorithm that automatically adjusts the weights of each component in the augmented loss function. Experiments on a variety of continuous control tasks demonstrate that the proposed algorithm outperforms various benchmarks by effectively utilizing available expert observations.
The selection of stepsizes has always been an elusive task in distributed optimization and learning. Although some stepsize-automation approaches have been proposed in centralized optimization, these approaches are inapplicable in the distributed setting. This is because in distributed optimization/learning, letting individual agents adapt their own stepsizes unavoidably results in stepsize heterogeneity, which can easily lead to algorithmic divergence. To solve this issue, we propose an approach that enables agents to adapt their individual stepsizes without any manual adjustments or global knowledge of the objective function. To the best of our knowledge, this is the first algorithm to successfully automate stepsize selection in distributed optimization/learning. Its performance is validated using several machine learning applications, including logistic regression, matrix factorization, and image classification.
The rapid expansion of distributed energy resources is heightening uncertainty and variability in distribution system operations, potentially leading to power quality challenges such as voltage magnitude violations and excessive voltage unbalance. Ensuring the dependable and secure operation of distribution grids requires system real-time assessment. However, constraints in sensing, measurement, and communication capabilities within distribution grids result in limited awareness of the system’s state. To achieve better real-time estimates of distribution system security, we propose a real-time security assessment based on data from smart meters, which are already prevalent in most distribution grids. Assuming that it is possible to obtain a limited number of voltage magnitude measurements in real time, we design an iterative algorithm to adaptively identify a subset of smart meters whose real-time measurements allow us to certify that all voltage magnitudes remain within bounds. This algorithm iterates between (i) solving optimization problems to determine the worst possible voltage magnitudes, given a limited set of voltage magnitude measurements, and (ii) leveraging the solutions and sensitivity information from these problems to update the measurement set. Numerical tests on the IEEE 123 bus distribution feeder demonstrate that the proposed algorithm consistently identifies and tracks the nodes with the highest and lowest voltage magnitude, even as the load changes over time. 
We consider estimation of motion on spheres as a variational problem. The concept of variational estimation for mechanical systems is based on application of variational principles from mechanics, to state estimation of mechanical systems evolving on configuration manifolds. If the configuration manifold is a symmetric space, then the overlying connected Lie group of which it is a quotient space, can be used to design nonlinearly stable observers for estimation of configuration and velocity states from measurements. If the configuration manifold is a sphere, then it can be globally represented by an unit vector. We illustrate the design of variational observers for mechanical systems evolving on spheres, through its application to estimation of pointing directions (reduced attitude) on the regular sphere S^2. 
We present a policy iteration algorithm for the infinite-horizon N -player general-sum deterministic linear quadratic dynamic games and compare it to policy gradient methods. We demonstrate that the proposed policy iteration algorithm is distinct from the Gauss-Newton policy gradient method in the N -player game setting, in contrast to the single-player setting where under suitable choice of step size they are equivalent. We illustrate in numerical experiments that the convergence rate of the proposed policy iteration algorithm significantly surpasses that of the Gauss-Newton policy gradient method and other policy gradient variations. Furthermore, our numerical results indicate that, compared to policy gradient methods, the convergence performance of the proposed policy iteration algorithm is less sensitive to the initial policy and changes in the number of players.
Estimation algorithms and nonlinear observers are widespread tools used in a variety of real-world applications, from satellite control to epidemiological studies. Their primary purpose is to provide an estimate of the state computed from available measurements and a model of the dynamics. When state estimates are used to enforce safety properties, it is essential to understand and characterize how accurate these estimates are so that safety is still guaranteed. While several observer design techniques provide bounds for the estimation error, they are either computationally expensive or too conservative and thus difficult to use in practice. Our work tackles these issues by providing error bounds for observers based on Savitzky-Golay filtering which are applicable to nonlinear systems satisfying a suitable observability assumption. Moreover, the error bounds are computed online based on measured data are thus tighter than offline bounds based on worst case assumptions. We generalize prior theoretical results by some of the authors from polynomial approximations to other functions and use this added flexibility to obtain tighter bounds. Finally, we illustrate the results using multiple examples, including an application to in-host models used in epidemiology.
BIKE is a code-based Key Encapsulation Mechanism (KEM) currently under consideration for standardization by the National Institute of Standards and Technology (NIST). BIKE, along with several other candidates, is being evaluated in the fourth round of the NIST Post-Quantum Cryptography (PQC) competition. In comparison to the lattice-based candidates, relatively little effort has been focused on analyzing this algorithm for side-channel vulnerabilities, especially in hardware. There have been several works on side-channel attacks and countermeasures on software implementations of BIKE, but as of yet, there have been no works focused on hardware. This work presents the first side-channel attack on a hardware implementation of BIKE. The attack targets a public implementation of the algorithm and is able to fully recover the long-term secret key with only several dozen traces. This work reveals BIKE’s significant susceptibilities to side-channel attacks when implemented in hardware and the need for investigation of hardware countermeasures. 
Abstract—An optimal guidance method is developed that reduces sensitivity to parametric uncertainties in the dynamic model. The method combines a previously developed method for guidance and control using adaptive Legendre-Gauss-Radau (LGR) collocation and a previously developed approach for desensitized optimal control. Guidance updates are performed such that the desensitized optimal control problem is re-solved on the remaining horizon at the start of each guidance cycle. The effectiveness of the method is demonstrated on a simple example using Monte Carlo simulation. The application of the method results in a smaller final state error distribution when compared to desensitized optimal control without guidance as well as a previously developed method for optimal guidance and control. 
In recent years, Network-on-Chip (NoC) has emerged as a promising solution for addressing a critical performance bottleneck encountered in designing large-scale multi-core systems, i.e., data communication. With advancements in chip manufacturing technologies and the increasing complexity of system designs, the task of designing the communication sub- systems has become increasingly challenging. The emergence of hardware accelerators, such as GPUs, FPGAs and ASICs, together with heterogeneous system integration of the CPUs and the accelerators creates new challenges in NoC design. Conventional NoC architectures developed for CPU-based multi- core systems are not able to satisfy the traffic demands of heterogeneous systems. In recent years, numerous research efforts have been dedicated to exploring the various aspects of NoC design in hardware accelerators and heterogeneous systems. However, there is a need for a comprehensive understanding of the current state-of-the-art research in this emerging research area. This paper aims to provide a summary of research work conducted in heterogeneous NoC design. Through this survey, we aim to present a comprehensive overview of the current related research, highlighting key findings, challenges, and future directions in this field. 
A modified form of Legendre-Gauss orthogonal direct collocation is developed for solving optimal control problems whose solutions are nonsmooth due to control discon- tinuities. This new method adds switch time variables, control variables, and collocation conditions at both endpoints of a mesh interval, whereas these new variables and collocation con- ditions are not included in standard Legendre-Gauss orthogonal collocation. The modified Legendre-Gauss collocation method alters the search space of the resulting nonlinear programming problem and optimizes the switch point of the control solution. The transformed adjoint system of the modified Legendre- Gauss collocation method is then derived and shown to satisfy the necessary conditions for optimality. Finally, an example is provided where the optimal control is bang-bang and contains multiple switches. This method is shown to be capable of solving complex optimal control problems with nonsmooth solutions. 
In this paper, we present an efficient algorithm to solve online Stackelberg games, featuring multiple followers, in a follower-agnostic manner. Unlike previous works, our approach works even when leader has no knowledge about the followers’ utility functions, strategy space or learning algorithm. Our algorithm introduces a unique gradient estimator, leveraging specially designed strategies to probe followers. In a departure from traditional assumptions of optimal play, we model followers’ responses using a convergent adaptation rule, allowing for realistic and dynamic interactions. The leader constructs the gradient estimator solely based on observations of followers’ actions. We provide both non-asymptotic convergence rates to stationary points of the leader’s objective and demonstrate asymptotic convergence to a local Stackelberg equilibrium. To validate the effectiveness of our algorithm, we use this algorithm to solve the problem of incentive design on a largescale transportation network, showcasing its robustness even when the leader lacks access to followers’ demand information.
This paper introduces a method of identifying a maximal set of safe strategies from data for stochastic systems with unknown dynamics using barrier certificates. The first step is learning the dynamics of the system via Gaussian Process (GP) regression and obtaining probabilistic errors for this estimate. Then, we develop an algorithm for constructing piecewise stochastic barrier functions to find a maximal permissible strategy set using the learned GP model, which is based on sequentially pruning the worst controls until a maximal set is identified. The permissible strategies are guaranteed to maintain probabilistic safety for the true system. This is especially important for learned systems, because a rich strategy space enables additional data collection and complex behaviors while remaining safe. Case studies on linear and nonlinear systems demonstrate that increasing the size of the dataset for learning grows the permissible strategy set. 
When learning in strategic environments, a key question is whether agents can overcome uncertainty about their preferences to achieve outcomes they could have achieved absent any uncertainty. Can they do this solely through interactions with each other? We focus this question on the ability of agents to attain the value of their Stackelberg optimal strategy and study the impact of information asymmetry. We study repeated interactions in fully strategic environments where players' actions are decided based on learning algorithms that take into account their observed histories and knowledge of the game. We study the pure Nash equilibria (PNE) of a meta-game where players choose these algorithms as their actions. We demonstrate that if one player has perfect knowledge about the game, then any initial informational gap persists. That is, while there is always a PNE in which the informed agent achieves her Stackelberg value, there is a game where no PNE of the meta-game allows the partially informed player to achieve her Stackelberg value. On the other hand, if both players start with some uncertainty about the game, the quality of information alone does not determine which agent can achieve her Stackelberg value. In this case, the concept of information asymmetry becomes nuanced and depends on the game's structure. Overall, our findings suggest that repeated strategic interactions alone cannot facilitate learning effectively enough to earn an uninformed player her Stackelberg value. 
As the Next-Generation Sequencing (NGS) techniques need to process enormous amounts of data, cost-efficientfand high-throughput computational analysis is essential in genomicsfstudy. Conventional computing platforms face great challenges to meet these demands due to their limited processing speed and scalability. Hardware accelerators, such as Graphics Processing Units (GPUs), Field-Programmable Gate Arrays (FPGAs), and Application-Specific Integrated Circuits (ASICs), offer transformative solutions to these computational challenges. This paper provides a state-of-the-art review of the roles of hardware accelerators in genomic analysis.We performed a comprehensive and in-depth analysis of cutting-edge genomics hardware accelerators, such as GPUs, FPGAs, and ASICs, in the context of the specific algorithms they aim to enhance. Besides reviewing opportunities in hardware genome acceleration, we also provide insights into the challenges regarding processing speed, cost efficiency, and scalability. 
Machine learning tools often rely on embedding text as vectors of real numbers.In this paper, we study how the semantic structure of language is encoded in the algebraic structure of such embeddings.Specifically, we look at a notion of "semantic independence" capturing the idea that, e.g., "eggplant" and "tomato" are independent given "vegetable". Although such examples are intuitive, it is difficult to formalize such a notion of semantic independence. The key observation here is that any sensible formalization should obey a set of so-called independence axioms, and thus any algebraic encoding of this structure should also obey these axioms. This leads us naturally to use partial orthogonality as the relevant algebraic structure. We develop theory and methods that allow us to demonstrate that partial orthogonality does indeed capture semantic independence.Complementary to this, we also introduce the concept of independence preserving embeddings where embeddings preserve the conditional independence structures of a distribution, and we prove the existence of such embeddings and approximations to them. 
We study calibration measures in a sequential prediction setup. In addition to rewarding accurate predictions (completeness) and penalizing incorrect ones (soundness), an important desideratum of calibration measures is truthfulness, a minimal condition for the forecaster not to be incentivized to exploit the system. Formally, a calibration measure is truthful if the forecaster (approximately) minimizes the expected penalty by predicting the conditional expectation of the next outcome, given the prior distribution of outcomes. We conduct a taxonomy of existing calibration measures. Perhaps surprisingly, all of them are far from being truthful. We introduce a new calibration measure termed the Subsampled Smooth Calibration Error (SSCE), which is complete and sound, and under which truthful prediction is optimal up to a constant multiplicative factor. In contrast, under existing calibration measures, there are simple distributions on which a polylogarithmic (or even zero) penalty is achievable, while truthful prediction leads to a polynomial penalty. 
In recent years the market for small satellites have exponentially grown, which has resulted in rapid advancements in CubeSat compatible technologies. Deployable antennas offer a solution to achieve large antenna apertures, while allowing to stow the aperture during the launch process. This paper presents a novel deployment mechanism based on embedded dielectric thread, and studies the effects of this thread in the performance of the metasurface elements of the deployable antenna. The proposed hinge mechanism minimizes gaps created by traditional hinges, ensuring structural integrity and RF performance. Direct light processing is used to manufacture the channelized metasurface elements which are metalized using silver conductive pastes. Experimental results show that the channelized structure introduces minimal phase variation of 1.15% (1.92°) and 0.139 dB of added loss at 30 GHz, making it suitable for use with deployable reflectarrays in space applications. This work validates the usage of embedded channels in the dielectric of metasurface antennas, and presents results that can be used to adjust the antenna geometry to account for the effects of such channels.
Over the last decade, spacecraft are increasingly integrating both microwave and optical communication systems, driving the need for electrically conductive and optically transparent materials. This study presents a comprehensive examination of advanced electromagnetic characterization techniques, focusing on Indium Tin Oxide (ITO) films on alkaline earth boro-aluminosilicate glass for transparent radio-frequency (RF) applications. Utilizing 4-point probes and coplanar waveguide (CPW) ground signal ground (GSG) probes, we investigated the electrical properties of ITO, including conductivity, permittivity, and loss tangent. The ITO films achieved an average permittivity of 7.16 in the range of 5-10 GHz, a DC conductivity of 6.86 MS/m, and a CPW loss tangent of 0.0054 at 10 GHz. These results underscore the suitability of ITO for hybrid optical and microwave hardware. Our findings offer critical insights for designing next-generation electronic and photonic devices, demonstrating the pivotal role of precise material characterization.
Faced with the complexities of managing natural gas-dependent power system amid the surge of renewable integration and load unpredictability, this study explores strategies for navigating emergency transitions to costlier secondary fuels. Our aim is to develop decision-support tools for operators during such exigencies. We approach the problem through a Markov Decision Process (MDP) framework, accounting for multiple uncertainties. These include the potential for dual-fuel generator failures and operator response during high-pressure situations. Additionally, we consider the finite reserves of primary fuel, governed by gas-flow partial differential equations (PDEs) and constrained by nodal pressure. Other factors include the variability in power forecasts due to renewable generation and the economic impact of compulsory load shedding. For tractability, we address the MDP in a simplified context, replacing it by Markov Processes evaluated against a selection of policies and scenarios for comparison. Our study considers two models for the natural gas system: an oversimplified model tracking linepack and a more nuanced model that accounts for gas flow network heterogeneity. The efficacy of our methods is demonstrated using a realistic model replicating Israel’s power-gas infrastructure.
In many power systems, particularly those isolated from larger intercontinental grids, reliance on natural gas is crucial. This dependence becomes particularly critical during periods of volatility or scarcity in renewable energy sources, further complicated by unpredictable consumption trends. To ensure the uninterrupted operation of these isolated gas-grid systems, innovative and efficient management strategies are essential. This paper investigates the complexities of achieving synchronized, dynamic, and stochastic optimization for autonomous transmission-level gas-grid infrastructures. We introduce a novel methodology grounded in differentiable programming, which synergizes symbolic programming, a conservative numerical method for solving gas-flow partial differential equations, and automated sensitivity analysis powered by SciML/Julia. Our methodology refines the co-optimization landscape for gas-grid systems by grounding gas dynamics in physics-adherent simulation. We demonstrate efficiency and precision of the methodology by solving a stochastic optimal gas flow problem, phrased on an open source model of Israel’s gas grid model.
The relationships between persistence of excitation (PE) conditions and asymptotically stable convergence of parameter estimates are well-known for adaptive systems where stability and convergence are derived with respect to the origin of a combined state error and parameter estimation error system. We address a class of second-order mechanical systems in which the true parameters are, rather than one single point in parameter space, members of a nullspace defined by feasible evolutions of a regressor matrix. Differences within the set of true parameters are unobservable, requiring a new characterization of PE and parameter convergence. We report an adaptive identification (AID) approach for this class of systems and show local stability and parameter estimate convergence to the true parameter set under a subspace PE condition. This approach is applicable to many second-order mechanical systems, including robot arms and undersea, land, aerial, and space vehicles, and enables a more complete parameterization of uncertainty in the dynamics, e.g. enabling simultaneous AID of plant and actuator parameters for mechanical systems.
The qualitative opacity of a secret is a security property, which means that a system trajectory satisfying the secret is observation-equivalent to a trajectory violating the secret. In this paper, we study how to synthesize a control policy that maximizes the probability of a secret being made opaque against an eavesdropping attacker/observer, while subject to other task performance constraints. In contrast to the existing belief-based approach for opacity-enforcement, we develop an approach that uses the observation function, the secret, and the model of the dynamical systems to construct a so-called opaqueobservations automaton that accepts the exact set of observations that enforce opacity. Leveraging this opaque-observations automaton, we can reduce the optimal planning in Markov decision processes(MDPs) for maximizing probabilistic opacity or its dual notion, transparency, subject to task constraints into a constrained planning problem over an augmented-state MDP. Finally, we illustrate the effectiveness of the developed methods in robot motion planning problems with opacity or transparency requirements.
We consider a large population of learning agents noncooperatively selecting strategies from a common set, influencing the dynamics of an exogenous system (ES) we seek to stabilize at a desired equilibrium. Our approach is to design a dynamic payoff mechanism capable of shaping the population’s strategy profile, thus affecting the ES’s state, by offering incentives for specific strategies within budget limits. Employing system-theoretic passivity concepts, we establish conditions under which a payoff mechanism can be systematically constructed to ensure the global asymptotic stability of the ES’s equilibrium. In comparison to previous approaches originally studied in the context of the so-called epidemic population games, the method proposed here allows for more realistic epidemic models and other types of ESs, such as predator-prey dynamics. The stability of the equilibrium is established with the support of a Lyapunov function, which provides useful bounds on the transient states.
With the development of space-air-ground integrated networks, Low Earth Orbit (LEO) satellite networks are envisioned to play a crucial role in providing data transmission services in the 6G era. However, the increasing number of connected devices leads to a surge in data volume and bursty traffic patterns. Ensuring the communication stability of LEO networks has thus become essential. While Lyapunov optimization has been applied to network optimization for decades and can guarantee stability when traffic rates remain within the capacity region, its applicability in LEO satellite networks is limited due to the bursty and dynamic nature of LEO network traffic. To address this issue, we propose a robust Lyapunov optimization method to ensure stability in LEO satellite networks. We theoretically show that for a stabilizable network system, traffic rates do not have to always stay within the capacity region at every time slot. Instead, the network can accommodate temporary capacity region violations, while ensuring the long-term network stability. Extensive simulations under various traffic conditions validate the effectiveness of the robust Lyapunov optimization method, demonstrating that LEO satellite networks can maintain stability under finite violations of the capacity region. 
This paper investigates the robot state estimation problem within a non-inertial environment. The proposed state estimation approach overcomes the common assumption of static ground in the system modeling. The process and measurement models explicitly treat the movement of the non-inertial environments without requiring knowledge of its motion in the inertial frame or relying on GPS or sensing environmental landmarks. Further, the proposed state estimator is formulated as an invariant extended Kalman filter (InEKF) with the deterministic part of its process model obeying the group-affine property, leading to log-linear error dynamics. The observability analysis of the filter confirms that the robot’s pose (i.e., position and orientation) and velocity relative to the non-inertial environment are observable. Hardware experiments on a humanoid robot moving on a rotating and translating treadmill demonstrate the high convergence rate and accuracy of the proposed InEKF even under significant treadmill pitch sway, as well as large estimation errors.
In stochastic and dynamic environments, the ability to infer an accurate model of the underlying dynamical system is crucial for ensuring objectives such as responsiveness, performance, or reliability. We present a novel approach to update predictive models of discrete-time, stochastic, dynamical systems in an online fashion. Our approach is based in physics-informed conditional distribution embeddings, a nonparametric machine learning technique that approximates an integral operator to assess the most likely distribution. We propose an efficient numerical method to update the predictive model as new data is gathered, employing low-rank updates. We validate our approach on examples of varying complexity, including an F-16 ground collision avoidance scenario.
Locality sensitive hashing (LSH) is a widely used technique for approximate nearest neighbor search (ANNS). In an LSH-based solution for ANNS, the computation of query-to-data (Q2D) distances accounts for a considerable fraction of the query time, but such distance information is thrown away after nearest neighbors are identified. In this paper, we propose CanDE (Candidate-based Distribution Estimation), a lightweight add-on to LSH that reuses such information for a wide range of analytics tasks including Q2D distance distribution estimation (QDDE), kernel density estimation (KDE), and query-time recall estimation (QTRE). This allows for significant savings in indexing costs and query time for multiple tasks associated with the original query.The main technical hurdle that CanDE addresses is the accurate estimation of some important statistics of the dataset via importance sampling. We discover that the existing estimators of these statistics are not accurate, because they approximate the actual number of collisions (called collision rate) in the LSH index using the theoretical collision probability (of the LSH function family), and this approximation is crude. To address this issue, we propose more accurate estimators based on a novel scheme called inferred collision rate (ICR), which gives a much better approximation to the actual collision rate. Furthermore, we propose an efficient algorithm for computing ICR from the nearest neighbor candidates returned by ANNS. Our evaluation shows that CanDE outperforms existing solutions on multiple analytics tasks while adding only about 8% to 19% query time overhead to ANNS.
This work mines big data in Sentinel-1 satellite images to unveil geographical patterns in offshore wind energy. We leverage unsupervised machine learning to extract insights from a 44GB open access dataset for decision support in wind farm orientations to guide stakeholders. It has broader impacts of overcoming climate change by enhancing renewable energy.
As high-performance computing resources have become increasingly available, new modes of applying and experimenting with simulation and other computational tools have become possible. This tutorial presents recent advancements to the Extreme-scale Model Exploration with Swift (EMEWS) framework. EMEWS is a high-performance computing (HPC) model exploration (ME) framework, developed for large-scale analyses (e.g., calibration, optimization) of computational models. We focus on three new use-inspired EMEWS capabilities, improved accessibility through binary installation, a new decoupled architecture (EMEWS DB) and task API for distributing workflows on heterogeneous compute resources, and improved EMEWS project creation capabilities. We present a complete worked example where EMEWS DB is used to connect a Python Bayesian optimization algorithm to worker pools running both locally and on remote compute resources. The example, including an R version, and additional details on EMEWS are made available on a public website.
In recent years, the rapid pace of urbanization has posed profound challenges globally, exacerbating environmental concerns and escalating traffic congestion in metropolitan areas. To mitigate these issues, Advanced Air Mobility (AAM) has emerged as a promising transportation alternative. However, the effective implementation of AAM requires robust demand modeling. This study delves into the demand dynamics of AAM by analyzing employment based trip data across Tennessee’s census tracts, employing statistical techniques and machine learning models to enhance accuracy in demand forecasting. Drawing on datasets from the Bureau of Transportation Statistics (BTS), the Internal Revenue Service (IRS), the Federal Aviation Administration (FAA), and additional sources, we perform cost, time, and risk assessments to compute the Generalized Cost of Trip (GCT). Our findings indicate that trips are more likely to be viable for AAM if air transportation accounts for over 70% of the GCT and the journey spans more than 250 miles. The study not only refines the understanding of AAM demand but also guides strategic planning and policy formulation for sustainable urban mobility solutions. The data and code can be accessed on GitHub. 1
Cardiovascular diseases, such as heart attack and congestive heart failure, are the leading cause of death in the United States and worldwide. The current medical practice for diagnosing cardiovascular diseases is not suitable for long-term, out-of-hospital use. A key to long-term, at-home cardiac care is the ability to provide continuous monitoring, and detect abnormal cardiac rhythms, i.e., arrhythmia, in real-time. Various big data and deep learning based approaches have been developed to analyze electrocardiogram data to identify arrhythmia conditions. However, most existing studies only focus on the accuracy of arrhythmia classification, instead of runtime performance of the workflow, which is critical for real-time detection. In this paper, we propose progressive resolution shrinking, a new method for supporting efficient execution of deep learning models for arrhythmia detection, without compromising the detection accuracy. Specifically, we explored multidimensional methods in reducing the amount of information needed for the learning task, and developed a new training method to leverage the advantage of reduced resolution. We have evaluated this approach using real electrocardiogram data, and the experimental results show that it effectively improves the efficiency of arrhythmia detection while preserving high accuracy. We expect this approach will pave the way for real-time arrhythmia detection on resource-constrained wearable devices.
Graph learning plays a pivotal role in many high-impact application domains. Despite significant advances in this field, there is currently no single solution that effectively handles (1) data heterogeneity, (2) long-range dependencies, (3) graph heterophily, and (4) scalability to large graphs, all at the same time. Classical graph neural networks (GNNs) and graph transformers (GTs) address some but not all of these issues, often suffering from limitations such as quadratic computation complexity and/or suboptimal generalization performance in realistic applications. This paper introduces UnifiedGT, a novel framework that systematically addresses all of these challenges by automatically providing a graph transformer architecture with multiple components via neural architecture search. UnifiedGT consists of five major components: (1) graph sampling, (2) structural prior injection, (3) graph attention, (4) local/global information mixing, and (5) type-specific feedforward networks (FFNs). This modular approach enables the efficient processing of large-scale graphs and effective management of heterogeneity and heterophily while capturing long-range dependencies. We demonstrate the versatility of UnifiedGT through comprehensive experiments on several benchmark datasets, revealing insights such as the efficacy of graph sampling for GTs, the importance of explicit graph structure injection via attention masking, and the synergistic effect of local/global information mixing via a combination of global attention with local message passing. Furthermore, we formulate these design choices into a search space, where an optimal combination can be discovered for a particular dataset via neural architecture search. Notably, UnifiedGT improves generalization performance on various graph datasets, outperforming state-of-the-art GT models by a margin of about 3.7% on average. The framework is available on Github (https://github.com/junhongmit/H2GB) and PyPI (https://pypi.org/project/H2GB/), and documentation can be found at https://junhongmit.github.io/H2GB/.
Identifying, localizing, and resolving bugs in software engineering is challenging and costly. Approaches to resolve software bugs range from Large Language Model (LLM) code analysis and repair, and automated code repair technology that aims to alleviate the technical burden of difficult to solve bugs. We propose RAGFix, which enhances LLM’s capabilities for bug localization and code repair using Retrieval Augmented Generation (RAG) based on dynamically collected Stack Overflow posts. These posts are searchable via a Question and Answer Knowledge Graph (KGQA). We evaluate our method on the HumanEvalFix benchmark for Python using relevant closed and open-source models. Our approach facilitates error resolution in Python coding problems by creating a searchable, embedded knowledge graph representation of bug and solution information from Stack Overflow, interlinking bugs, and solutions through semi-supervised graph construction methods. We use cosine similarity on embeddings based on LLM-synthesized summaries and algorithmic features describing the coding problem and potential solution to find relevant results that improve LLM in-context performance. Our results indicate that our system enhances small open-source models’ ability to effectively repair code, particularly where these models have less parametric knowledge about relevant coding problems and can leverage nonparametric knowledge to provide accurate, actionable fixes. 