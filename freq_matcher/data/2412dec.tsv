We introduce the CRONOS algorithm for convex optimization of two-layer neural networks. CRONOS is the first algorithm capable of scaling to high-dimensional datasets such as ImageNet, which are ubiquitous in modern deep learning. This significantly improves upon prior work, which has been restricted to downsampled versions of MNIST and CIFAR-10. Taking CRONOS as a primitive, we then develop a new algorithm called CRONOS-AM, which combines CRONOS with alternating minimization, to obtain an algorithm capable of training multilayer networks with arbitrary architectures. Our theoretical analysis proves that CRONOS converges to the global minimum of the convex reformulation under mild assumptions. In addition, we validate the efficacy of CRONOS and CRONOS-AM through extensive large-scale numerical experiments with GPU acceleration in JAX. Our results show that CRONOS-AM can obtain comparable or better validation accuracy than predominant tuned deep learning optimizers on vision and language tasks with benchmark datasets such as ImageNet and IMDb. To the best of our knowledge, CRONOS is the first algorithm which utilizes the convex reformulation to enhance performance on large-scale learning tasks
We consider the problem of determining a binary ground truth using advice from a group of independent reviewers (experts) who express their guess about a ground truth correctly with some independent probability (competence) pi. In this setting, when all reviewers are competent with p≥0.5, the Condorcet Jury Theorem tells us that adding more reviewers increases the overall accuracy, and if all pi's are known, then there exists an optimal weighting of the reviewers. However, in practical settings, reviewers may be noisy or incompetent, i.e., pi≤0.5, and the number of experts may be small, so the asymptotic Condorcet Jury Theorem is not practically relevant. In such cases we explore appointing one or more chairs (judges) who determine the weight of each reviewer for aggregation, creating multiple levels. However, these chairs may be unable to correctly identify the competence of the reviewers they oversee, and therefore unable to compute the optimal weighting. We give conditions on when a set of chairs is able to weight the reviewers optimally, and depending on the competence distribution of the agents, give results about when it is better to have more chairs or more reviewers. Through simulations we show that in some cases it is better to have more chairs, but in many cases it is better to have more reviewers.
We consider a generalized version of the (weighted) one-center problem on graphs. Given an undirected graph G of n vertices and m edges and a positive integer k≤n, the problem aims to find a point in G so that the maximum (weighted) distance from it to k connected vertices in its shortest path tree(s) is minimized. No previous work has been proposed for this problem except for the case k=n, that is, the classical graph one-center problem. In this paper, an O(mnlognlogmn+m2lognlogmn)-time algorithm is proposed for the weighted case, and an O(mnlogn)-time algorithm is presented for the unweighted case, provided that the distance matrix for G is given. When G is a tree graph, we propose an algorithm that solves the weighted case in O(nlog2nlogk) time with no given distance matrix, and improve it to O(nlog2n) for the unweighted case.
Deep reinforcement learning has demonstrated re- markable achievements across diverse domains such as video games, robotic control, autonomous driving, and drug discovery. Common methodologies in partially observable domains largely lean on end-to-end learning from high-dimensional observations, such as images, without explicitly reasoning about true state. We suggest an alternative direction, introducing the Partially Supervised Reinforcement Learning (PSRL) framework. At the heart of PSRL is the fusion of both supervised and unsupervised learning. The approach leverages a state estimator to distill supervised semantic state information from high-dimensional observations which are often fully observable at training time. This yields more interpretable policies that compose state predictions with control. In parallel, it captures an unsupervised latent representation. These two—the semantic state and the latent state—are then fused and utilized as inputs to a policy network. This juxtaposition offers practitioners a flexible and dynamic spectrum: from emphasizing supervised state information to integrating richer, latent insights. Extensive experimental results indicate that by merging these dual representations, PSRL offers a balance, enhancing interpretability while preserving, and often significantly outperforming, the performance benchmarks set by traditional methods in terms of reward and convergence speed. 
As the scale of production HPC platforms increases, so does the computing and I/O performance gap, exacerbating the storage bottleneck. High-performance storage systems have been developed to alleviate this bottleneck, but many questions remain concerning their architecture, implementation, and configuration. Answering these questions via experimental campaigns proves arduous. First, some answers are required before deploying the system. Second, once a system hits production the experimental scope is limited by the system's specific configuration and by constraints of production use. In this work we identify challenges posed by the design and validation of a storage simulator. We then propose solutions implemented in Fives, a simulator of HPC workloads on platforms that comprise a parallel file system. We show how our simulator can be instantiated and calibrated for the accurate simulation of a production Lustre deployment.
Adversaries minimally perturb deep learning input data to reduce a learning model's ability to produce domain-specific data-driven recommendations to solve specialized tasks. This vulnerability to adversarial perturbations has been argued to stem from a learning model's nonlocal generalization over complex input data. Given the incomplete information in a complex dataset, a learning model captures nonlinear patterns between data points with volatility in the loss surface and exploitable areas of low-confidence knowledge. It is the responsibility of activation functions to capture the nonlinearity in data and, thus, has inspired disjointed research efforts to create robust activation functions. This work unifies the properties of activation functions that contribute to robust generalization with the generalized gamma distribution function. We show that combining the disjointed characteristics presented in the literature with our parametric generalized gamma activation function provides more effective robustness than the individual characteristics alone11The source code for this research effort: https://github.com/sheilaalemany/generalized-gamma-activation.git.
Adversarial perturbations in object recognition and image classification tasks impact a learning model's ability to perform accurately and increase the safety risks of deployed machine learning models. During the adversarial example generation process, adversaries approach areas most prone to model uncertainty. Identifying partially occluded items, especially without understanding general object shapes, contributes to significant model uncertainty since object boundaries are not inherently at the forefront of the feature generalization process in deep learning models. Thus, this work aims to reduce model uncertainty surrounding partially occluded boundaries and increase adversarial robustness by augmenting the training dataset with amodal segmentation boundary masks. By observing performance degradation, robust sensitivity, and loss sensitivity, we show how including these masks during training impacts an adversary's ability to generate effective adversarial examples on the versatile MS COCO dataset. Lastly, we observe how including these masks during training influences the performance of adversarial training.
Parallel applications can spend a significant amount of time performing I/O on large-scale supercomputers. Fast near-compute storage accelerators called burst buffers can reduce the time a processor spends performing I/O and mitigate I/O bottlenecks. However, determining if a given application could be accelerated using burst buffers is not straightforward even for storage experts. The relationship between an application's I/O characteristics (such as I/O volume, processes involved, etc.) and the best storage sub-system for it can be complicated. As a result, adapting parallel applications to use burst buffers efficiently is a trial-and-error process. In this work, we present a Python-based tool called PrismIO that enables programmatic analysis of I/O traces. Using PrismIO, we identify performance bottlenecks when using burst buffers and parallel file systems, and explain why certain I/O patterns perform poorly. Further, we use machine learning to model the relationship between I/O characteristics and file system selections. We use IOR, an I/O benchmark to gather performance data for training the machine learning model. Our model can predict the better performing storage system for unseen IOR scenarios with an accuracy of 94.47% and for four real applications with an accuracy of 95.86%.
The increasing demand for flexible and efficient optical networks has led to the development of Software-Defined Elastic Optical Networks (SD-EONs). These networks leverage the programmability of Software-Defined Networking (SDN) and the adaptability of Elastic Optical Networks (EONs) to optimize network performance under dynamic traffic conditions. However, existing simulation tools often fall short in terms of transparency, flexibility, and advanced functionality, limiting their utility in cutting-edge research. In this paper, we present a Flexible Unified Simulator for Intelligent Optical Networking (FUSION), a fully open-source simulator designed to address these limitations and provide a comprehensive platform for SD-EON research. FUSION integrates traditional routing and spectrum assignment algorithms with advanced machine learning and reinforcement learning techniques, including support for the Stable Baselines 3 library. The simulator also offers robust unit testing, a fully functional Graphical User Interface (GUI), and extensive documentation to ensure usability and reliability. Performance evaluations demonstrate the effectiveness of FUSION in modeling complex network scenarios, showcasing its potential as a powerful tool for advancing SD-EON research. 
Large Language Models (LLMs) are pre-trained on large-scale corpora and excel in numerous general natural language processing (NLP) tasks, such as question answering (QA). Despite their advanced language capabilities, when it comes to domain-specific and knowledge-intensive tasks, LLMs suffer from hallucinations, knowledge cut-offs, and lack of knowledge attributions. Additionally, fine tuning LLMs' intrinsic knowledge to highly specific domains is an expensive and time consuming process. The retrieval-augmented generation (RAG) process has recently emerged as a method capable of optimization of LLM responses, by referencing them to a predetermined ontology. It was shown that using a Knowledge Graph (KG) ontology for RAG improves the QA accuracy, by taking into account relevant sub-graphs that preserve the information in a structured manner. In this paper, we introduce SMART-SLIC, a highly domain-specific LLM framework, that integrates RAG with KG and a vector store (VS) that store factual domain specific information. Importantly, to avoid hallucinations in the KG, we build these highly domain-specific KGs and VSs without the use of LLMs, but via NLP, data mining, and nonnegative tensor factorization with automatic model selection. Pairing our RAG with a domain-specific: (i) KG (containing structured information), and (ii) VS (containing unstructured information) enables the development of domain-specific chat-bots that attribute the source of information, mitigate hallucinations, lessen the need for fine-tuning, and excel in highly domain-specific question answering tasks. We pair SMART-SLIC with chain-of-thought prompting agents. The framework is designed to be generalizable to adapt to any specific or specialized domain. In this paper, we demonstrate the question answering capabilities of our framework on a corpus of scientific publications on malware analysis and anomaly detection.
This paper presents a novel approach for classifying electrocardiogram (ECG) signals in healthcare applications using federated learning and stacked convolutional neural networks (CNNs). Our innovative technique leverages the distributed nature of federated learning to collaboratively train a high-performance model while preserving data privacy on local devices. We propose a stacked CNN architecture tailored for ECG data, effectively extracting discriminative features across different temporal scales. The evaluation confirms the strength of our approach, culminating in a final model accuracy of 98.6% after 100 communication rounds, significantly exceeding baseline performance. This promising result paves the way for accurate and privacy-preserving ECG classification in diverse healthcare settings, potentially leading to improved diagnosis and patient monitoring. 
Microfluidic biochips are widely used in biomedical research, clinical diagnostics, and point-of-care testing. However, their complex supply chains make them vulnerable to counterfeiting, overbuilding, and intellectual property (IP) piracy. We present fluorescent carbon quantum dot (CQD) stickers1 that can be integrated with the polydimethylsiloxane (PDMS) based biochips for authentication. The stickers can be plasma-bonded to biochips made of glass and silicon. A protective spin-coated PDMS layer makes them obscured and tamperproof. However, they are detectable under UV light and can be authenticated via spectral analysis. The scheme exhibits unique excitation-dependent responses associated with the variability of the CQD sizes. This makes it ideal for physical authentication. Reliability studies concerning mechanical, photonic, and thermal degradation have demonstrated highly stable results. The stability of CQDs within the PDMS, their robust excitation-based emission fluorescence response, and the use of waste polypropylene masks make this a sustainable and robust authenticator for biochips.
Variational estimation of a mechanical system is based on the application of variational principles from mechanics to state estimation of the system evolving on its configuration manifold. If the configuration manifold is a Lie group, then the underlying group structure can be used to design nonlinearly stable observers for estimation of configuration and velocity states from measurements. Measured quantities are on a vector space on which the Lie group acts smoothly. We formulate the design of variational observers on a general finite-dimensional Lie group, followed by the design and experimental evaluation of a variational observer for rigid body motions on the Lie group SE(3). 
This paper presents a closed-form notion of controllability and observability for systems with communication delays, actuation delays, and locality constraints. The formulation reduces to classical notions of controllability and observability in the unconstrained setting. As a consequence of our formulation, we show that the addition of locality and communication constraints may not affect the controllability and observability of the system, and we provide an efficient sufficient condition under which this phenomenon occurs. This contrasts with actuation and sensing delays, which cause a gradual loss of controllability and observability as the delays increase. We illustrate our results using linearized swing equations for the power grid, showing how actuation delay and locality constraints affect controllability.
Feedback-evolving games is a framework that models the co-evolution between payoff functions and an environmental state. It serves as a useful tool to analyze many social dilemmas such as natural resource consumption, behaviors in epidemics, and the evolution of biological populations. However, it has primarily focused on the dynamics of a single population of agents. In this paper, we consider the impact of two populations of agents that share a common environmental resource. We focus on a scenario where individuals in one population are governed by an environmentally “responsible” incentive policy, and individuals in the other population are environmentally “irresponsible”. An analysis on the asymptotic stability of the coupled system is provided, and conditions for which the resource collapses are identified. We then derive consumption rates for the irresponsible population that optimally exploit the environmental resource, and analyze how incentives should be allocated to the responsible population that most effectively promote the environment via a sensitivity analysis.
The growing availability of COVID-19 data and advancements in AI offer potential for improved pandemic prediction and prevention. Federated Learning (FL) frameworks support collaborative, privacy-preserving COVID-19 detection, but often neglect the need for simpler models in resource-constrained settings. Knowledge distillation, where a complex "teacher" model transfers insights to a simpler "student" model, struggles to retain detailed information, especially with a single teacher. To address this, we propose a novel FL algorithm, FL-MTKD, which uses multiple teachers to distill knowledge into an efficient 2.5 MB student model. Our results show that while simplified architectures like FL-SimpCNN (2.5 MB) handle non-IID datasets better, larger models like FL-CovidCNN (20.74 MB) and FL-DeepCovid (351.6 MB) perform poorly in such settings. FL-MTKD outperforms other models, achieving 89.74% accuracy and 89.71% F1 score on non-IID datasets, and over 93% accuracy on IID (Independent and Identically Distributed) datasets, offering strong generalization with minimal storage needs. Our developed code can be found from QinggeLab-ACMBCB-24 on github.
Optimal control methods provide solutions to safety-critical problems but easily become intractable. Control Barrier Functions (CBFs) have emerged as a popular technique that facilitates their solution by provably guaranteeing safety, through their forward invariance property, at the expense of some performance loss. This approach involves defining a performance objective alongside CBF-based safety constraints that must always be enforced with both performance and solution feasibility significantly impacted by two key factors: (i) the selection of the cost function and associated parameters, and (ii) the calibration of parameters within the CBF-based constraints, which capture the trade-off between performance and conservativeness. To address these challenges, we propose a Reinforcement Learning (RL)-based Receding Horizon Control (RHC) approach leveraging Model Predictive Control (MPC) with CBFs (MPC-CBF). In particular, we parameterize our controller and use bilevel optimization, where RL is used to learn the optimal parameters while MPC computes the optimal control input. We validate our method by applying it to the challenging automated merging control problem for Connected and Automated Vehicles (CAVs) at conflicting roadways. Results demonstrate improved performance and a significant reduction in the number of infeasible cases compared to traditional heuristic approaches used for tuning CBF-based controllers, showcasing the effectiveness of the proposed method. In order to guarantee reproducibility, our code is provided here: link1.1github.com/EhsanSabouni/CDC2024_RL_adpative_MPC_CBF/tree/main
This paper investigates how to incorporate expert observations (without explicit information on expert actions) into a deep reinforcement learning setting to improve sample efficiency. First, we formulate an augmented policy loss combining a maximum entropy reinforcement learning objective with a behavioral cloning loss that leverages a forward dynamics model. Then, we propose an algorithm that automatically adjusts the weights of each component in the augmented loss function. Experiments on a variety of continuous control tasks demonstrate that the proposed algorithm outperforms various benchmarks by effectively utilizing available expert observations.
The selection of stepsizes has always been an elusive task in distributed optimization and learning. Although some stepsize-automation approaches have been proposed in centralized optimization, these approaches are inapplicable in the distributed setting. This is because in distributed optimization/learning, letting individual agents adapt their own stepsizes unavoidably results in stepsize heterogeneity, which can easily lead to algorithmic divergence. To solve this issue, we propose an approach that enables agents to adapt their individual stepsizes without any manual adjustments or global knowledge of the objective function. To the best of our knowledge, this is the first algorithm to successfully automate stepsize selection in distributed optimization/learning. Its performance is validated using several machine learning applications, including logistic regression, matrix factorization, and image classification.
The rapid expansion of distributed energy resources is heightening uncertainty and variability in distribution system operations, potentially leading to power quality challenges such as voltage magnitude violations and excessive voltage unbalance. Ensuring the dependable and secure operation of distribution grids requires system real-time assessment. However, constraints in sensing, measurement, and communication capabilities within distribution grids result in limited awareness of the system’s state. To achieve better real-time estimates of distribution system security, we propose a real-time security assessment based on data from smart meters, which are already prevalent in most distribution grids. Assuming that it is possible to obtain a limited number of voltage magnitude measurements in real time, we design an iterative algorithm to adaptively identify a subset of smart meters whose real-time measurements allow us to certify that all voltage magnitudes remain within bounds. This algorithm iterates between (i) solving optimization problems to determine the worst possible voltage magnitudes, given a limited set of voltage magnitude measurements, and (ii) leveraging the solutions and sensitivity information from these problems to update the measurement set. Numerical tests on the IEEE 123 bus distribution feeder demonstrate that the proposed algorithm consistently identifies and tracks the nodes with the highest and lowest voltage magnitude, even as the load changes over time. 