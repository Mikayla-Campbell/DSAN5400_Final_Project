When training deep neural networks, a model's generalization error is often observed to follow a power scaling law dependent both on the model size and the data size. Perhaps the best known example of such scaling laws are for transformer-based large language models (**LLMs**), where networks with billions of parameters are trained on trillions of tokens of text. Yet, despite sustained widespread interest, a rigorous understanding of why transformer scaling laws exist is still missing. To answer this question, we establish novel statistical estimation and mathematical approximation theories for transformers when the input data are concentrated on a low-dimensional manifold. Our theory predicts a power law between the generalization error and both the training data size and the network size for transformers, where the power depends on the intrinsic dimension d of the training data. Notably, the constructed model architecture is shallow, requiring only logarithmic depth in d. By leveraging low-dimensional data structures under a manifold hypothesis, we are able to explain transformer scaling laws in a way which respects the data geometry. Moreover, we test our theory with empirical observation by training LLMs on natural language datasets. We find the observed empirical scaling laws closely agree with our theoretical predictions. Taken together, these results rigorously show the intrinsic dimension of data to be a crucial quantity affecting transformer scaling laws in both theory and practice.
To operate at a building scale, service robots must perform very long-horizon mobile manipulation tasks by navigating to different rooms, accessing different floors, and interacting with a wide and unseen range of everyday objects. We refer to these tasks as Building-wide Mobile Manipulation. To tackle these inherently long-horizon tasks, we propose BUMBLE, a unified VLM-based framework integrating open-world RGBD perception, a wide spectrum of gross-to-fine motor skills, and dual-layered memory. Our extensive evaluation (90+ hours) indicates that BUMBLE outperforms multiple baselines in long-horizon building-wide tasks that require sequencing up to 12 ground truth skills spanning 15 minutes per trial. BUMBLE achieves 47.1% success rate averaged over 70 trials in different buildings, tasks, and scene layouts from different starting rooms and floors. Our user study demonstrates 22% higher satisfaction with our method than state-of-the-art mobile manipulation methods. Finally, we demonstrate the potential of using increasingly capable foundation models to push performance further.
Transformer-based human skeleton action recognition has been developed for years. However, the complexity and high parameter count demands of these models hinder their practical applications, especially in resource-constrained environments. In this work, we propose FreqMixForemrV2, which was built upon the Frequency-aware Mixed Transformer (FreqMixFormer) for identifying subtle and discriminative actions with pioneered frequency-domain analysis. We design a lightweight architecture that maintains robust performance while significantly reducing the model complexity. This is achieved through a redesigned frequency operator that optimizes high-frequency and low-frequency parameter adjustments, and a simplified frequency-aware attention module. These improvements result in a substantial reduction in model parameters, enabling efficient deployment with only a minimal sacrifice in accuracy. Comprehensive evaluations of standard datasets (NTU RGB+D, NTU RGB+D 120, and NW-UCLA datasets) demonstrate that the proposed model achieves a superior balance between efficiency and accuracy, outperforming state-of-the-art methods with only 60% of the parameters.
Entry guidance has played a vital role in planetary landing missions from the Apollo era to now. Modern techniques that directly utilize highly nonlinear entry dynamics, unlike the Apollo reentry guidance, are gaining attention. This study demonstrates that numerical predictor-corrector guidance can generate a trajectory that closely resembles the historic Apollo final reentry path. Secondly, it compares a conventional bank reversal algorithm with a prediction-based lateral guidance algorithm. Finally, the study proposes a predictive no-fly zone avoidance guidance algorithm that can effectively address geographic constraints for hypersonic gliding vehicles.
Schistosomiasis is a parasitic disease with significant global health and socio-economic implications. Drug discovery for schistosomiasis typically involves high-content whole-organism screening. In this approach, parasites are ex-posed to various chemical compounds and their systemic, whole-organism-level responses are captured via microscopy and analyzed to obtain a quanti-tative assessment of chemical effect. These effects are multidimensional and time-varying, impacting shape, appearance, and behavior. Accurate identifi-cation of object boundaries is essential for preparing images for subsequent analysis in high-content studies. Object segmentation is one of the most deeply studied problems in computer vision where recent efforts have incor-porated deep learning. Emerging results indicate that acquiring robust fea-tures in spectral domain using Fast Fourier Transform (FFT) within Deep Neural Networks (DNNs) can enhance segmentation accuracy. In this paper, we explore this direction further and propose a latent space Phase-Gating (PG) method that builds upon FFT and leverages phase information to effi-ciently identify globally significant features. While the importance of phase in analyzing signals has long been known, technical difficulties in calculat-ing phase in manners that are invariant to imaging parameters has limited its use. A key result of this paper is to show how phase information can be in-corporated in neural architectures that are compact. Experiments conducted on complex HCS datasets demonstrate how this idea leads to improved seg-mentation accuracy, while maintaining robustness against commonly en-countered noise (blurring) in HCS. The compactness of the proposed method also makes it well-suited for application specific architectures (ASIC) de-signed for high-content screening.
Low-latency and low-power edge AI is crucial for Virtual Reality and Augmented Reality applications. Recent advances demonstrate that hybrid models, combining convolution layers (CNN) and transformers (ViT), often achieve a superior accuracy/performance tradeoff on various computer vision and machine learning (ML) tasks. However, hybrid ML models can present system challenges for latency and energy efficiency due to their diverse nature in dataflow and memory access patterns. In this work, we leverage architecture heterogeneity from Neural Processing Units (NPU) and Compute-In-Memory (CIM) and explore diverse execution schemas to efficiently execute these hybrid models. We introduce H4H-NAS, a two-stage Neural Architecture Search (NAS) framework to automate the design of efficient hybrid CNN/ViT models for heterogeneous edge systems featuring both NPU and CIM. We propose a two-phase incremental supernet training in our NAS framework to resolve gradient conflicts between sampled subnets caused by different types of blocks in a hybrid model search space. Our H4H-NAS approach is also powered by a performance estimator built with NPU performance results measured on real silicon, and CIM performance based on industry IPs. H4H-NAS searches hybrid CNN-ViT models with fine granularity and achieves significant (up to 1.34%) top-1 accuracy improvement on ImageNet. Moreover, results from our algorithm/hardware co-design reveal up to 56.08% overall latency and 41.72% energy improvements by introducing heterogeneous computing over baseline solutions. Overall, our framework guides the design of hybrid network architectures and system architectures for NPU+CIM heterogeneous systems.
Large Language Models (LLMs) have been applied to various hardware design tasks, including Verilog code generation, EDA tool scripting, and RTL bug fixing. Despite this extensive exploration, LLMs are yet to be used for the task of post-synthesis metric reasoning and estimation of HDL designs. In this paper, we assess the ability of LLMs to reason about post-synthesis metrics of Verilog designs. We introduce MetRex, a large-scale dataset comprising 25,868 Verilog HDL designs and their corresponding post-synthesis metrics, namely area, delay, and static power. MetRex incorporates a Chain of Thought (CoT) template to enhance LLMs' reasoning about these metrics. Extensive experiments show that Supervised Fine-Tuning (SFT) boosts the LLM's reasoning capabilities on average by 37.0%, 25.3%, and 25.7% on the area, delay, and static power, respectively. While SFT improves performance on our benchmark, it remains far from achieving optimal results, especially on complex problems. Comparing to state-of-the-art regression models, our approach delivers accurate post-synthesis predictions for 17.4% more designs (within a 5% error margin), in addition to offering a 1.7x speedup by eliminating the need for pre-processing. This work lays the groundwork for advancing LLM-based Verilog code metric reasoning.
In this paper, a dual-band 3×3 Nolen matrix network is investigated. The proposed dual-band Nolen matrix achieves different progressive phases at two desired frequencies such that it can generated radiation beams in the different angles at two frequencies. The proposed circuit is composed of two types of dual-band 180° couplers with different coupling ratios and three dual-band phase shifters. The closed-from equations are derived to find its characteristics at two frequencies. To verify the design theory, a prototype has been designed, and the simulation measurement align well with each other to prove the design concept. Compared to other designs, this work provides additional approach for dual-band phased array design with high flexibility, low cost, and simplified circuit topology.
We introduce CLOB, a novel continual learning (CL) paradigm wherein a large language model (LLM) is regarded as a black box. Learning is done incrementally via only verbal prompting. CLOB does not fine-tune any part of the LLM or add any trainable parameters to it. It is particularly suitable for LLMs that are accessible via APIs. We also propose a new CL technique, called CIS, based on incremental summarization that also overcomes the LLM’s input length limit. Experiments show CIS outperforms baselines by a very large margin.
This manuscript presents high-throughput sorting of cellular-sized microparticles within a three-dimensional microfluidic channel by focused bulk acoustic wave (BAW) produced by a Self-Focusing Acoustic Transducer (SFAT). The focused ultrasound induces a substantially higher acoustic radiation force within the focal region, enabling sorting based on particle size and density. Unlike surface-acoustic-wave-based setups, the BAW-based technique uses a three-dimensional microfluidic channel through which a mixture of particles is transported, while SFAT(s) may be placed at multiple points along the channel for multi-stage sorting. The technique has been successfully used in sorting 50 μm microparticles, which are analogous to cancerous or differentiated Mesenchymal Stem Cells (MSC), from 30 μm microparticles, which are analogous to healthy MSC. The sorting results in 97.5% purity at the smaller microparticle outlet and a 97.2% recovery rate for the smaller particles. The technique allows sorting 650,000 smaller and 142,000 larger microparticles within a mere 10 minutes.
Existing continual learning (CL) methods mainly rely on fine-tuning or adapting large language mod- els (LLMs). They still suffer from catastrophic for- getting (CF). Little work has been done to exploit in-context learning (ICL) to leverage the extensive knowledge within LLMs for CL without updating any parameters. However, incrementally learning each new task in ICL necessitates adding training examples from each class of the task to the prompt, which hampers scalability as the prompt length in- creases. This issue not only leads to excessively long prompts that exceed the input token limit of the underlying LLM but also degrades the model’s performance due to the overextended context. To address this, we introduce InCA, a novel approach that integrates an external continual learner (ECL) with ICL to enable scalable CL without CF. The ECL is built incrementally to pre-select a small subset of likely classes for each test instance. By restricting the ICL prompt to only these selected classes, InCA prevents prompt lengths from becom- ing excessively long, while maintaining high per- formance. Experimental results demonstrate that InCA significantly outperforms existing CL base- lines, achieving substantial performance gains.
The Respool Fiber Research (RFR) model provides a comprehensive protocol for evaluating mechanically recycled textiles for "second life" product applications, addressing critical gaps in the circular economy. With textile waste predominantly landfilled, mechanical recycling often results in short, weak fibers unsuitable for high-value products. The RFR model leverages sensory evaluations and laboratory testing to guide the transformation of recycled fibers into yarn or nonwoven prototypes. Demonstrating its applicability with 100% recycled cotton denim and wool, the model prioritizes material consistency, compatibility, and upcycling potential. Key steps include assessing fiber properties, blending for consistency, and determining appropriate prototype pathways through tensile and elongation testing. The RFR model advances beyond existing frameworks, enabling educators, researchers, and developers to optimize recycled materials for multi-use cycles. Future research will scale the model for broader industry application, fostering sustainability and innovation in textile recycling.
Understanding the factors that lead to teacher success and persistence in high-need school districts is imperative for the success of the students in those districts. Teacher success means many things to different stakeholders in high-needs communities: families, colleagues, and administrators are all positioned to benefit from increased teacher retention, leadership, and/or test scores. However, preparing and supporting teachers in their work towards these successes may be more challenging. In this research study, we worked with six administrators and ten teachers representing four high-need districts in the New York metro area to better illustrate their perspectives on what teachers need to be successful in these contexts. Interpreting qualitative data through feminist, identity, and professional learning continuum framing, we asked: How do administrators and teachers perceive the qualities of teachers who persist in high-need schools? Preliminary findings illustrate that although teachers and administrators are in agreement on the qualities required of teachers, the reality is that teachers embodying these qualities are frequently not those who end up being hired. Thus, there is tension on the school culture and goals for student learning, especially for schools in which teacher attrition is greatest.
Cancer is one of the leading causes of death world- wide. Pathogenic viruses are estimated to be responsible for 15% of all human cancers globally and pose significant threats to pub- lic health. Viruses integrate their genetic material into the host genome, increasing the risk of cancer promoting changes in it. To understand the molecular mechanisms of virus-mediated cancers, it is crucial to identify viral insertion sites in cancer genomes. However, this effort is hindered by the rapidly increasing volume of tumor sequencing data, along with the challenges of accurate data analysis caused by high viral mutation rates and the difficulty of aligning short reads to the reference genome. Thus it is crucial to develop an efficient method for virus integration site detection in tumor genomes. This paper proposes a novel pipeline to identify viral integration sites leveraging deep Convolutional Neural Networks (CNN). Our contributions are twofold: (i) We propose and integrate three novel matrix generation methods into the pipeline, developed after aligning the host and viral genomes with their respective reference genomes.; (ii) We employ one-hot encoded images with reduced computational complexity to represent viral integration sites and harness the capabilities of Deep CNN networks for detection. The paper illustrates our proposed approach and presents experiments conducted using both synthetic and real sequencing data. Our preliminary experimental results are promising, showcasing the effectiveness of the proposed methods in detecting viral integration sites.
In the United States, heart disease is the leading cause of death, killing about 695,000 people each year. Myocardial infarction (MI) is a cardiac complication which occurs when blood flow to a portion of the heart decreases or halts, leading to damage in the heart muscle. Heart failure and Atrial fibrillation (AF) are closely associated with MI. Heart failure is a common complication of MI and a risk factor for AF. Machine learning (ML) and deep learning techniques have shown potential in predicting cardiovascular conditions. However, developing a sim- plified predictive model, along with a thorough feature analysis, is challenging due to various factors, including lifestyle, age, family history, medical conditions, and clinical variables for cardiac complications prediction. This paper aims to develop simplified models with comprehensive feature analysis and data preprocessing for predicting cardiac complications, such as heart failure and atrial fibrillation linked with MI, using a publicly available dataset of myocardial infarction patients. This will help the students and health care professionals understand various factors responsible for cardiac complications through a simplified workflow. By prioritizing interpretability, this paper illustrates how simpler models, like decision trees and logistic regression, can provide transparent decision-making processes while still maintaining a balance with accuracy. Additionally, this paper examines how age-specific factors affect heart failure and atrial fibrillation conditions. Overall this research focuses on making machine learning accessible and interpretable. Its goal is to equip students and non-experts with practical tools to understand how ML can be applied in healthcare, particularly for the cardiac complications prediction for patients having MI.
The global COVID-19 pandemic has strained healthcare systems and highlighted the need for accessible and efficient diagnostic methods. Traditional diagnostic tools, such as nasal swabs and biosensors, while accurate, pose significant logistical challenges and high costs, limiting their scalability. This paper explores an alternative, non-invasive approach to COVID-19 detection using machine learning algorithms to analyze vocal patterns, particularly cough and breathing sounds. Leveraging a publicly available dataset, we developed machine learning models capable of classifying audio samples as COVID-19 positive or negative. Our models achieve an AUC of up to 85% and an F1- score of 81%, demonstrating the potential of machine learning in enabling rapid, cost-effective COVID-19 diagnosis. These findings suggest that audio-based diagnostics could be a practical and scalable solution, particularly in resource-limited settings where traditional methods are less feasible.
Whiteface Mountain (WFM) in northern NY State is the site of a historic mountaintop atmospheric observatory with an ongoing cloud water chemistry monitoring program that has been operating every summer (June through September) since 1994. Though long-term chemical analysis has been conducted, no analysis on the microbiome has been completed at WFM. Over the years, a new chemical regime has been reported in the cloudwater with missing analytes. Knowing how microbes can interact with chemicals, we hypothesize microbes are partially responsible for this shift and are crucial in understanding the chemical background of clouds. To start this study, cloudwater filters have been analyzed both chemically and microbially. Chemically, weighted averages have been calculated for each cloudwater filter based on the chemical composition of the clouds. Microbially, we have begun DNA extractions and subsequent metagenomic analysis using the Oxford Nanopore MinION using a select number of cloud water filters from 2024. Overall, this study aims to build upon microbial work accomplished by the Puy de Dôme groups and discuss the collection, storage, and analysis of cloudwater filters to connect the chemical to the microbial at WFM.
Collaboration is crucial for reaching collective goals. However, its effectiveness is often undermined by the strategic behavior of individual agents -- a fact that is captured by a high Price of Stability (PoS) in recent literature [Blum et al., 2021]. Implicit in the traditional PoS analysis is the assumption that agents have full knowledge of how their tasks relate to one another. We offer a new perspective on bringing about efficient collaboration among strategic agents using information design. Inspired by the growing importance of collaboration in machine learning (such as platforms for collaborative federated learning and data cooperatives), we propose a framework where the platform has more information about how the agents' tasks relate to each other than the agents themselves. We characterize how and to what degree such platforms can leverage their information advantage to steer strategic agents toward efficient collaboration. Concretely, we consider collaboration networks where each node is a task type held by one agent, and each task benefits from contributions made in their inclusive neighborhood of tasks. This network structure is known to the agents and the platform, but only the platform knows each agent's real location -- from the agents' perspective, their location is determined by a random permutation. We employ private Bayesian persuasion and design two families of persuasive signaling schemes that the platform can use to ensure a small total workload when agents follow the signal. The first family aims to achieve the minmax optimal approximation ratio compared to the optimal collaboration, which is shown to be Θ(n‾√) for unit-weight graphs, Θ(n2/3) for graphs with constant minimum edge weights, and O(n3/4) for general weighted graphs. The second family ensures per-instance strict improvement compared to full information disclosure.
We study local filters for the Lipschitz property of real-valued functions f : V → [0,r], where the Lipschitz property is defined with respect to an arbitrary undirected graph G = (V, E ). We give nearly optimal local Lipschitz filters both with respect to ℓ1-distance and ℓ0-distance. Previous work only considered unbounded- range functions over [n]d. Jha and Raskhodnikova (SICOMP ‘13) gave an algorithm for such functions with lookup complexity exponential in d, which Awasthi et al. (ACM Trans. Comput. Theory) showed was necessary in this setting. We demonstrate that important applications of local Lipschitz filters can be accomplished with filters for functions whose range is bounded in [0,r]. For functions f : [n]d → [0,r], we achieve running time (dr log n )O (log r ) for the ℓ1-respecting filter and dO(r) polylog n for the ℓ0-respecting filter, thus circumventing the lower bound. Our local filters provide a novel Lipschitz extension that can be implemented locally. Furthermore, we show that our algorithms are nearly optimal in terms of the dependence on r for the domain {0,1}d, an important special case of the domain [n]d. In addition, our lower bound resolves an open question of Awasthi et al., removing one of the conditions necessary for their lower bound for general range. We prove our lower bound via a reduction from distribution-free Lipschitz testing and a new technique for proving hardness for adaptive algorithms. Finally, we provide two applications of our local filters to real-valued functions, with no restrictions on the range. In the first application, we use them in conjunction with the Laplace mechanism for differential privacy and noisy binary search to provide mechanisms for privately releasing outputs of black-box functions, even in the presence of malicious clients. In particular, our differentially private mechanism for arbitrary real-valued functions runs in time 2polylog min(r,nd ) and, for honest clients, has accuracy comparable to the Laplace mechanism for Lipschitz functions, up to a factor of O (log min(r,nd )). In the second application, we use our local filters to obtain the first nontrivial tolerant tester for the Lipschitz property. Our tester works for functions of the form f : {0,1}d → ℝ, makes queries, and has tolerance ratio 2.01. Our applications demonstrate that local filters for bounded-range functions can be applied to construct efficient algorithms for arbitrary real-valued functions.
We explore the use of local algorithms in the design of streaming algorithms for the Maximum Directed Cut problem. Specifically, building on the local algorithm of (Buchbinder, Feldman, Seffi, and Schwartz [14] and Censor-Hillel, Levy, and Shachnai [16]), we develop streaming algorithms for both adversarially and randomly ordered streams that approximate the value of maximum directed cut in bounded-degree graphs. In n-vertex graphs, for adversarially ordered streams, our algorithm uses O (n1-Ω(1)) (sub-linear) space and for randomly ordered streams, our algorithm uses logarithmic space. Moreover, both algorithms require only one pass over the input stream. With a constant number of passes, we give a logarithmic-space algorithm which works even on graphs with unbounded degree on adversarially ordered streams. Our algorithms achieve any fixed constant approximation factor less than 1/2. In the single-pass setting, this is tight: known lower bounds show that obtaining any constant approximation factor greater than 1/2 is impossible without using linear space in adversarially ordered streams (Kapralov and Krachun [37]) and space in randomly ordered streams, even on bounded degree graphs (Kapralov, Khanna, and Sudan [35]). In terms of techniques, our algorithms partition the vertices into a small number of different types based on the structure of their local neighborhood, ensuring that each type carries enough information about the structure to approximately simulate the local algorithm on a vertex with that type. We then develop tools to accurately estimate the frequency of each type. This allows us to simulate an execution of the local algorithm on all vertices, and thereby approximate the value of the maximum directed cut.