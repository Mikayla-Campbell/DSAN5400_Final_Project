When training deep neural networks, a model's generalization error is often observed to follow a power scaling law dependent both on the model size and the data size. Perhaps the best known example of such scaling laws are for transformer-based large language models (**LLMs**), where networks with billions of parameters are trained on trillions of tokens of text. Yet, despite sustained widespread interest, a rigorous understanding of why transformer scaling laws exist is still missing. To answer this question, we establish novel statistical estimation and mathematical approximation theories for transformers when the input data are concentrated on a low-dimensional manifold. Our theory predicts a power law between the generalization error and both the training data size and the network size for transformers, where the power depends on the intrinsic dimension d of the training data. Notably, the constructed model architecture is shallow, requiring only logarithmic depth in d. By leveraging low-dimensional data structures under a manifold hypothesis, we are able to explain transformer scaling laws in a way which respects the data geometry. Moreover, we test our theory with empirical observation by training LLMs on natural language datasets. We find the observed empirical scaling laws closely agree with our theoretical predictions. Taken together, these results rigorously show the intrinsic dimension of data to be a crucial quantity affecting transformer scaling laws in both theory and practice.
To operate at a building scale, service robots must perform very long-horizon mobile manipulation tasks by navigating to different rooms, accessing different floors, and interacting with a wide and unseen range of everyday objects. We refer to these tasks as Building-wide Mobile Manipulation. To tackle these inherently long-horizon tasks, we propose BUMBLE, a unified VLM-based framework integrating open-world RGBD perception, a wide spectrum of gross-to-fine motor skills, and dual-layered memory. Our extensive evaluation (90+ hours) indicates that BUMBLE outperforms multiple baselines in long-horizon building-wide tasks that require sequencing up to 12 ground truth skills spanning 15 minutes per trial. BUMBLE achieves 47.1% success rate averaged over 70 trials in different buildings, tasks, and scene layouts from different starting rooms and floors. Our user study demonstrates 22% higher satisfaction with our method than state-of-the-art mobile manipulation methods. Finally, we demonstrate the potential of using increasingly capable foundation models to push performance further.
Transformer-based human skeleton action recognition has been developed for years. However, the complexity and high parameter count demands of these models hinder their practical applications, especially in resource-constrained environments. In this work, we propose FreqMixForemrV2, which was built upon the Frequency-aware Mixed Transformer (FreqMixFormer) for identifying subtle and discriminative actions with pioneered frequency-domain analysis. We design a lightweight architecture that maintains robust performance while significantly reducing the model complexity. This is achieved through a redesigned frequency operator that optimizes high-frequency and low-frequency parameter adjustments, and a simplified frequency-aware attention module. These improvements result in a substantial reduction in model parameters, enabling efficient deployment with only a minimal sacrifice in accuracy. Comprehensive evaluations of standard datasets (NTU RGB+D, NTU RGB+D 120, and NW-UCLA datasets) demonstrate that the proposed model achieves a superior balance between efficiency and accuracy, outperforming state-of-the-art methods with only 60% of the parameters.
Entry guidance has played a vital role in planetary landing missions from the Apollo era to now. Modern techniques that directly utilize highly nonlinear entry dynamics, unlike the Apollo reentry guidance, are gaining attention. This study demonstrates that numerical predictor-corrector guidance can generate a trajectory that closely resembles the historic Apollo final reentry path. Secondly, it compares a conventional bank reversal algorithm with a prediction-based lateral guidance algorithm. Finally, the study proposes a predictive no-fly zone avoidance guidance algorithm that can effectively address geographic constraints for hypersonic gliding vehicles.
Schistosomiasis is a parasitic disease with significant global health and socio-economic implications. Drug discovery for schistosomiasis typically involves high-content whole-organism screening. In this approach, parasites are ex-posed to various chemical compounds and their systemic, whole-organism-level responses are captured via microscopy and analyzed to obtain a quanti-tative assessment of chemical effect. These effects are multidimensional and time-varying, impacting shape, appearance, and behavior. Accurate identifi-cation of object boundaries is essential for preparing images for subsequent analysis in high-content studies. Object segmentation is one of the most deeply studied problems in computer vision where recent efforts have incor-porated deep learning. Emerging results indicate that acquiring robust fea-tures in spectral domain using Fast Fourier Transform (FFT) within Deep Neural Networks (DNNs) can enhance segmentation accuracy. In this paper, we explore this direction further and propose a latent space Phase-Gating (PG) method that builds upon FFT and leverages phase information to effi-ciently identify globally significant features. While the importance of phase in analyzing signals has long been known, technical difficulties in calculat-ing phase in manners that are invariant to imaging parameters has limited its use. A key result of this paper is to show how phase information can be in-corporated in neural architectures that are compact. Experiments conducted on complex HCS datasets demonstrate how this idea leads to improved seg-mentation accuracy, while maintaining robustness against commonly en-countered noise (blurring) in HCS. The compactness of the proposed method also makes it well-suited for application specific architectures (ASIC) de-signed for high-content screening.
Low-latency and low-power edge AI is crucial for Virtual Reality and Augmented Reality applications. Recent advances demonstrate that hybrid models, combining convolution layers (CNN) and transformers (ViT), often achieve a superior accuracy/performance tradeoff on various computer vision and machine learning (ML) tasks. However, hybrid ML models can present system challenges for latency and energy efficiency due to their diverse nature in dataflow and memory access patterns. In this work, we leverage architecture heterogeneity from Neural Processing Units (NPU) and Compute-In-Memory (CIM) and explore diverse execution schemas to efficiently execute these hybrid models. We introduce H4H-NAS, a two-stage Neural Architecture Search (NAS) framework to automate the design of efficient hybrid CNN/ViT models for heterogeneous edge systems featuring both NPU and CIM. We propose a two-phase incremental supernet training in our NAS framework to resolve gradient conflicts between sampled subnets caused by different types of blocks in a hybrid model search space. Our H4H-NAS approach is also powered by a performance estimator built with NPU performance results measured on real silicon, and CIM performance based on industry IPs. H4H-NAS searches hybrid CNN-ViT models with fine granularity and achieves significant (up to 1.34%) top-1 accuracy improvement on ImageNet. Moreover, results from our algorithm/hardware co-design reveal up to 56.08% overall latency and 41.72% energy improvements by introducing heterogeneous computing over baseline solutions. Overall, our framework guides the design of hybrid network architectures and system architectures for NPU+CIM heterogeneous systems.
Large Language Models (LLMs) have been applied to various hardware design tasks, including Verilog code generation, EDA tool scripting, and RTL bug fixing. Despite this extensive exploration, LLMs are yet to be used for the task of post-synthesis metric reasoning and estimation of HDL designs. In this paper, we assess the ability of LLMs to reason about post-synthesis metrics of Verilog designs. We introduce MetRex, a large-scale dataset comprising 25,868 Verilog HDL designs and their corresponding post-synthesis metrics, namely area, delay, and static power. MetRex incorporates a Chain of Thought (CoT) template to enhance LLMs' reasoning about these metrics. Extensive experiments show that Supervised Fine-Tuning (SFT) boosts the LLM's reasoning capabilities on average by 37.0%, 25.3%, and 25.7% on the area, delay, and static power, respectively. While SFT improves performance on our benchmark, it remains far from achieving optimal results, especially on complex problems. Comparing to state-of-the-art regression models, our approach delivers accurate post-synthesis predictions for 17.4% more designs (within a 5% error margin), in addition to offering a 1.7x speedup by eliminating the need for pre-processing. This work lays the groundwork for advancing LLM-based Verilog code metric reasoning.
In this paper, a dual-band 3×3 Nolen matrix network is investigated. The proposed dual-band Nolen matrix achieves different progressive phases at two desired frequencies such that it can generated radiation beams in the different angles at two frequencies. The proposed circuit is composed of two types of dual-band 180° couplers with different coupling ratios and three dual-band phase shifters. The closed-from equations are derived to find its characteristics at two frequencies. To verify the design theory, a prototype has been designed, and the simulation measurement align well with each other to prove the design concept. Compared to other designs, this work provides additional approach for dual-band phased array design with high flexibility, low cost, and simplified circuit topology.
We introduce CLOB, a novel continual learning (CL) paradigm wherein a large language model (LLM) is regarded as a black box. Learning is done incrementally via only verbal prompting. CLOB does not fine-tune any part of the LLM or add any trainable parameters to it. It is particularly suitable for LLMs that are accessible via APIs. We also propose a new CL technique, called CIS, based on incremental summarization that also overcomes the LLM’s input length limit. Experiments show CIS outperforms baselines by a very large margin.
This manuscript presents high-throughput sorting of cellular-sized microparticles within a three-dimensional microfluidic channel by focused bulk acoustic wave (BAW) produced by a Self-Focusing Acoustic Transducer (SFAT). The focused ultrasound induces a substantially higher acoustic radiation force within the focal region, enabling sorting based on particle size and density. Unlike surface-acoustic-wave-based setups, the BAW-based technique uses a three-dimensional microfluidic channel through which a mixture of particles is transported, while SFAT(s) may be placed at multiple points along the channel for multi-stage sorting. The technique has been successfully used in sorting 50 μm microparticles, which are analogous to cancerous or differentiated Mesenchymal Stem Cells (MSC), from 30 μm microparticles, which are analogous to healthy MSC. The sorting results in 97.5% purity at the smaller microparticle outlet and a 97.2% recovery rate for the smaller particles. The technique allows sorting 650,000 smaller and 142,000 larger microparticles within a mere 10 minutes.
Existing continual learning (CL) methods mainly rely on fine-tuning or adapting large language mod- els (LLMs). They still suffer from catastrophic for- getting (CF). Little work has been done to exploit in-context learning (ICL) to leverage the extensive knowledge within LLMs for CL without updating any parameters. However, incrementally learning each new task in ICL necessitates adding training examples from each class of the task to the prompt, which hampers scalability as the prompt length in- creases. This issue not only leads to excessively long prompts that exceed the input token limit of the underlying LLM but also degrades the model’s performance due to the overextended context. To address this, we introduce InCA, a novel approach that integrates an external continual learner (ECL) with ICL to enable scalable CL without CF. The ECL is built incrementally to pre-select a small subset of likely classes for each test instance. By restricting the ICL prompt to only these selected classes, InCA prevents prompt lengths from becom- ing excessively long, while maintaining high per- formance. Experimental results demonstrate that InCA significantly outperforms existing CL base- lines, achieving substantial performance gains.
The Respool Fiber Research (RFR) model provides a comprehensive protocol for evaluating mechanically recycled textiles for "second life" product applications, addressing critical gaps in the circular economy. With textile waste predominantly landfilled, mechanical recycling often results in short, weak fibers unsuitable for high-value products. The RFR model leverages sensory evaluations and laboratory testing to guide the transformation of recycled fibers into yarn or nonwoven prototypes. Demonstrating its applicability with 100% recycled cotton denim and wool, the model prioritizes material consistency, compatibility, and upcycling potential. Key steps include assessing fiber properties, blending for consistency, and determining appropriate prototype pathways through tensile and elongation testing. The RFR model advances beyond existing frameworks, enabling educators, researchers, and developers to optimize recycled materials for multi-use cycles. Future research will scale the model for broader industry application, fostering sustainability and innovation in textile recycling.
Understanding the factors that lead to teacher success and persistence in high-need school districts is imperative for the success of the students in those districts. Teacher success means many things to different stakeholders in high-needs communities: families, colleagues, and administrators are all positioned to benefit from increased teacher retention, leadership, and/or test scores. However, preparing and supporting teachers in their work towards these successes may be more challenging. In this research study, we worked with six administrators and ten teachers representing four high-need districts in the New York metro area to better illustrate their perspectives on what teachers need to be successful in these contexts. Interpreting qualitative data through feminist, identity, and professional learning continuum framing, we asked: How do administrators and teachers perceive the qualities of teachers who persist in high-need schools? Preliminary findings illustrate that although teachers and administrators are in agreement on the qualities required of teachers, the reality is that teachers embodying these qualities are frequently not those who end up being hired. Thus, there is tension on the school culture and goals for student learning, especially for schools in which teacher attrition is greatest.
Cancer is one of the leading causes of death world- wide. Pathogenic viruses are estimated to be responsible for 15% of all human cancers globally and pose significant threats to pub- lic health. Viruses integrate their genetic material into the host genome, increasing the risk of cancer promoting changes in it. To understand the molecular mechanisms of virus-mediated cancers, it is crucial to identify viral insertion sites in cancer genomes. However, this effort is hindered by the rapidly increasing volume of tumor sequencing data, along with the challenges of accurate data analysis caused by high viral mutation rates and the difficulty of aligning short reads to the reference genome. Thus it is crucial to develop an efficient method for virus integration site detection in tumor genomes. This paper proposes a novel pipeline to identify viral integration sites leveraging deep Convolutional Neural Networks (CNN). Our contributions are twofold: (i) We propose and integrate three novel matrix generation methods into the pipeline, developed after aligning the host and viral genomes with their respective reference genomes.; (ii) We employ one-hot encoded images with reduced computational complexity to represent viral integration sites and harness the capabilities of Deep CNN networks for detection. The paper illustrates our proposed approach and presents experiments conducted using both synthetic and real sequencing data. Our preliminary experimental results are promising, showcasing the effectiveness of the proposed methods in detecting viral integration sites.
In the United States, heart disease is the leading cause of death, killing about 695,000 people each year. Myocardial infarction (MI) is a cardiac complication which occurs when blood flow to a portion of the heart decreases or halts, leading to damage in the heart muscle. Heart failure and Atrial fibrillation (AF) are closely associated with MI. Heart failure is a common complication of MI and a risk factor for AF. Machine learning (ML) and deep learning techniques have shown potential in predicting cardiovascular conditions. However, developing a sim- plified predictive model, along with a thorough feature analysis, is challenging due to various factors, including lifestyle, age, family history, medical conditions, and clinical variables for cardiac complications prediction. This paper aims to develop simplified models with comprehensive feature analysis and data preprocessing for predicting cardiac complications, such as heart failure and atrial fibrillation linked with MI, using a publicly available dataset of myocardial infarction patients. This will help the students and health care professionals understand various factors responsible for cardiac complications through a simplified workflow. By prioritizing interpretability, this paper illustrates how simpler models, like decision trees and logistic regression, can provide transparent decision-making processes while still maintaining a balance with accuracy. Additionally, this paper examines how age-specific factors affect heart failure and atrial fibrillation conditions. Overall this research focuses on making machine learning accessible and interpretable. Its goal is to equip students and non-experts with practical tools to understand how ML can be applied in healthcare, particularly for the cardiac complications prediction for patients having MI.
The global COVID-19 pandemic has strained healthcare systems and highlighted the need for accessible and efficient diagnostic methods. Traditional diagnostic tools, such as nasal swabs and biosensors, while accurate, pose significant logistical challenges and high costs, limiting their scalability. This paper explores an alternative, non-invasive approach to COVID-19 detection using machine learning algorithms to analyze vocal patterns, particularly cough and breathing sounds. Leveraging a publicly available dataset, we developed machine learning models capable of classifying audio samples as COVID-19 positive or negative. Our models achieve an AUC of up to 85% and an F1- score of 81%, demonstrating the potential of machine learning in enabling rapid, cost-effective COVID-19 diagnosis. These findings suggest that audio-based diagnostics could be a practical and scalable solution, particularly in resource-limited settings where traditional methods are less feasible.
Whiteface Mountain (WFM) in northern NY State is the site of a historic mountaintop atmospheric observatory with an ongoing cloud water chemistry monitoring program that has been operating every summer (June through September) since 1994. Though long-term chemical analysis has been conducted, no analysis on the microbiome has been completed at WFM. Over the years, a new chemical regime has been reported in the cloudwater with missing analytes. Knowing how microbes can interact with chemicals, we hypothesize microbes are partially responsible for this shift and are crucial in understanding the chemical background of clouds. To start this study, cloudwater filters have been analyzed both chemically and microbially. Chemically, weighted averages have been calculated for each cloudwater filter based on the chemical composition of the clouds. Microbially, we have begun DNA extractions and subsequent metagenomic analysis using the Oxford Nanopore MinION using a select number of cloud water filters from 2024. Overall, this study aims to build upon microbial work accomplished by the Puy de Dôme groups and discuss the collection, storage, and analysis of cloudwater filters to connect the chemical to the microbial at WFM.
Collaboration is crucial for reaching collective goals. However, its effectiveness is often undermined by the strategic behavior of individual agents -- a fact that is captured by a high Price of Stability (PoS) in recent literature [Blum et al., 2021]. Implicit in the traditional PoS analysis is the assumption that agents have full knowledge of how their tasks relate to one another. We offer a new perspective on bringing about efficient collaboration among strategic agents using information design. Inspired by the growing importance of collaboration in machine learning (such as platforms for collaborative federated learning and data cooperatives), we propose a framework where the platform has more information about how the agents' tasks relate to each other than the agents themselves. We characterize how and to what degree such platforms can leverage their information advantage to steer strategic agents toward efficient collaboration. Concretely, we consider collaboration networks where each node is a task type held by one agent, and each task benefits from contributions made in their inclusive neighborhood of tasks. This network structure is known to the agents and the platform, but only the platform knows each agent's real location -- from the agents' perspective, their location is determined by a random permutation. We employ private Bayesian persuasion and design two families of persuasive signaling schemes that the platform can use to ensure a small total workload when agents follow the signal. The first family aims to achieve the minmax optimal approximation ratio compared to the optimal collaboration, which is shown to be Θ(n‾√) for unit-weight graphs, Θ(n2/3) for graphs with constant minimum edge weights, and O(n3/4) for general weighted graphs. The second family ensures per-instance strict improvement compared to full information disclosure.
We study local filters for the Lipschitz property of real-valued functions f : V → [0,r], where the Lipschitz property is defined with respect to an arbitrary undirected graph G = (V, E ). We give nearly optimal local Lipschitz filters both with respect to ℓ1-distance and ℓ0-distance. Previous work only considered unbounded- range functions over [n]d. Jha and Raskhodnikova (SICOMP ‘13) gave an algorithm for such functions with lookup complexity exponential in d, which Awasthi et al. (ACM Trans. Comput. Theory) showed was necessary in this setting. We demonstrate that important applications of local Lipschitz filters can be accomplished with filters for functions whose range is bounded in [0,r]. For functions f : [n]d → [0,r], we achieve running time (dr log n )O (log r ) for the ℓ1-respecting filter and dO(r) polylog n for the ℓ0-respecting filter, thus circumventing the lower bound. Our local filters provide a novel Lipschitz extension that can be implemented locally. Furthermore, we show that our algorithms are nearly optimal in terms of the dependence on r for the domain {0,1}d, an important special case of the domain [n]d. In addition, our lower bound resolves an open question of Awasthi et al., removing one of the conditions necessary for their lower bound for general range. We prove our lower bound via a reduction from distribution-free Lipschitz testing and a new technique for proving hardness for adaptive algorithms. Finally, we provide two applications of our local filters to real-valued functions, with no restrictions on the range. In the first application, we use them in conjunction with the Laplace mechanism for differential privacy and noisy binary search to provide mechanisms for privately releasing outputs of black-box functions, even in the presence of malicious clients. In particular, our differentially private mechanism for arbitrary real-valued functions runs in time 2polylog min(r,nd ) and, for honest clients, has accuracy comparable to the Laplace mechanism for Lipschitz functions, up to a factor of O (log min(r,nd )). In the second application, we use our local filters to obtain the first nontrivial tolerant tester for the Lipschitz property. Our tester works for functions of the form f : {0,1}d → ℝ, makes queries, and has tolerance ratio 2.01. Our applications demonstrate that local filters for bounded-range functions can be applied to construct efficient algorithms for arbitrary real-valued functions.
We explore the use of local algorithms in the design of streaming algorithms for the Maximum Directed Cut problem. Specifically, building on the local algorithm of (Buchbinder, Feldman, Seffi, and Schwartz [14] and Censor-Hillel, Levy, and Shachnai [16]), we develop streaming algorithms for both adversarially and randomly ordered streams that approximate the value of maximum directed cut in bounded-degree graphs. In n-vertex graphs, for adversarially ordered streams, our algorithm uses O (n1-Ω(1)) (sub-linear) space and for randomly ordered streams, our algorithm uses logarithmic space. Moreover, both algorithms require only one pass over the input stream. With a constant number of passes, we give a logarithmic-space algorithm which works even on graphs with unbounded degree on adversarially ordered streams. Our algorithms achieve any fixed constant approximation factor less than 1/2. In the single-pass setting, this is tight: known lower bounds show that obtaining any constant approximation factor greater than 1/2 is impossible without using linear space in adversarially ordered streams (Kapralov and Krachun [37]) and space in randomly ordered streams, even on bounded degree graphs (Kapralov, Khanna, and Sudan [35]). In terms of techniques, our algorithms partition the vertices into a small number of different types based on the structure of their local neighborhood, ensuring that each type carries enough information about the structure to approximately simulate the local algorithm on a vertex with that type. We then develop tools to accurately estimate the frequency of each type. This allows us to simulate an execution of the local algorithm on all vertices, and thereby approximate the value of the maximum directed cut.
In this work, we show that the class of multivariate degree-d polynomials mapping {0,1}n to any Abelian group G is locally correctable with Õd((log n )d) queries for up to a fraction of errors approaching half the minimum distance of the underlying code. In particular, this result holds even for polynomials over the reals or the rationals, special cases that were previously not known. Further, we show that they are locally list correctable up to a fraction of errors approaching the minimum distance of the code. These results build on and extend the prior work of Amireddy, Behera, Paraashar, Srinivasan, and Sudan [1] (STOC 2024) who considered the case of linear polynomials (d = 1) and gave analogous results. Low-degree polynomials over the Boolean cube {0,1}n arise naturally in Boolean circuit complexity and learning theory, and our work furthers the study of their coding-theoretic properties. Extending the results of [1] from linear polynomials to higher-degree polynomials involves several new challenges and handling them gives us further insights into properties of low-degree polynomials over the Boolean cube. For local correction, we construct a set of points in the Boolean cube that lie between two exponentially close parallel hyperplanes and is moreover an interpolating set for degree-d polynomials. To show that the class of degree-d polynomials is list decodable up to the minimum distance, we stitch together results on anti-concentration of low-degree polynomials, the Sunflower lemma, and the Footprint bound for counting common zeroes of polynomials. Analyzing the local list corrector of [1] for higher degree polynomials involves understanding random restrictions of non-zero degree-d polynomials on a Hamming slice. In particular, we show that a simple random restriction process for reducing the dimension of the Boolean cube is a suitably good sampler for Hamming slices. Thus our exploration unearths several new techniques that are useful in understanding the combinatorial structure of low-degree polynomials over {0, 1}n. 
We study algorithms for approximating the spectral density (i.e., the eigenvalue distribution) of a symmetric matrix A ∈ ℝn×n that is accessed through matrix-vector product queries. Recent work has analyzed popular Krylov subspace methods for this problem, showing that they output an ∈ · || A||2 error approximation to the spectral density in the Wasserstein-1 metric using O (1/∈ ) matrix-vector products. By combining a previously studied Chebyshev polynomial moment matching method with a deflation step that approximately projects off the largest magnitude eigendirections of A before estimating the spectral density, we give an improved error bound of ∈ · σℓ (A) using O (ℓ log n + 1/∈ ) matrix-vector products, where σℓ (A) is the ℓth largest singular value of A. In the common case when A exhibits fast singular value decay and so σℓ (A) « ||A||2, our bound can be much stronger than prior work. We also show that it is nearly tight: any algorithm giving error ∈ · σℓ (A) must use Ω(ℓ + 1/∈ ) matrix-vector products. We further show that the popular Stochastic Lanczos Quadrature (SLQ) method essentially matches the above bound for any choice of parameter ℓ, even though SLQ itself is parameter-free and performs no explicit deflation. Our bound helps to explain the strong practical performance and observed ‘spectrum adaptive’ nature of SLQ, and motivates a simple variant of the method that achieves an even tighter error bound. Technically, our results require a careful analysis of how eigenvalues and eigenvectors are approximated by (block) Krylov subspace methods, which may be of independent interest. Our error bound for SLQ leverages an analysis of the method that views it as an implicit polynomial moment matching method, along with recent results on low-rank approximation with single-vector Krylov methods. We use these results to show that the method can perform ‘implicit deflation’ as part of moment matching. 
We present a new class of preconditioned iterative methods for solving linear systems of the form Ax = b. Our methods are based on constructing a low-rank Nyström approximation to A using sparse random matrix sketching. This approximation is used to construct a preconditioner, which itself is inverted quickly using additional levels of random sketching and preconditioning. We prove that the convergence of our methods depends on a natural average condition number of A, which improves as the rank of the Nyström approximation increases. Concretely, this allows us to obtain faster runtimes for a number of fundamental linear algebraic problems: 1. We show how to solve any n × n linear system that is well-conditioned except for k outlying large singular values in Õ (n2.065 + kω) time, improving on a recent result of [Derezmski, Yang, STOC 2024] for all k ≳ n0.78. 2. We give the first Õ (n2 + dλω) time algorithm for solving a regularized linear system (A+λΙ)x = b, where A is positive semidefinite with effective dimension dλ = tr(A(A + λΙ)-1). This problem arises in applications like Gaussian process regression. 3. We give faster algorithms for approximating Schatten p-norms and other matrix norms. For example, for the Schatten 1-norm (nuclear norm), we give an algorithm that runs in Õ (n2.11) time, improving on an Õ (n2.18) method of [Musco et al., ITCS 2018]. All results are proven in the real RAM model of computation. Interestingly, previous state-of-the-art algorithms for most of the problems above relied on stochastic iterative methods, like stochastic coordinate and gradient descent. Our work takes a completely different approach, instead leveraging tools from matrix sketching. 
We describe a randomized algorithm for producing a near-optimal hierarchical off-diagonal low-rank (HODLR) approximation to an n × n matrix A, accessible only though matrix-vector products with A and AT. We prove that, for the rank-k HODLR approximation problem, our method achieves a (1 + β )log(n )-optimal approximation in expected Frobenius norm using O (k log(n )/β3) matrix-vector products. In particular, the algorithm obtains a (1 + ∈ )-optimal approximation with O (k log4(n )/∈3) matrix-vector products, and for any constant c, an nc-optimal approximation with O (k log(n )) matrix-vector products. Apart from matrix-vector products, the additional computational cost of our method is just O (n poly(log(n ), k, β )). We complement the upper bound with a lower bound, which shows that any matrix-vector query algorithm requires at least Ω(k log(n ) + k/ε ) queries to obtain a (1 + ε )-optimal approximation. Our algorithm can be viewed as a robust version of widely used “peeling” methods for recovering HODLR matrices and is, to the best of our knowledge, the first matrix-vector query algorithm to enjoy theoretical worst- case guarantees for approximation by any hierarchical matrix class. To control the propagation of error between levels of hierarchical approximation, we introduce a new perturbation bound for low-rank approximation, which shows that the widely used Generalized Nyström method enjoys inherent stability when implemented with noisy matrix-vector products. We also introduce a novel randomly perforated matrix sketching method to further control the error in the peeling algorithm. 
Interactive content in online spaces is often meant to inform a broad audience regarding issues of societal interest. Recently, combating misinformation has emerged as another task requiring interventions to reach broad audiences. We are engaged in two efforts to investigate how lay users leverage social media to inform and engage a broader audience and the characteristics of the resulting user-generated discourse. We found that tailoring content for broader audiences can be laborious and the impact of the efforts is neither guaranteed nor immediately apparent. Our future work will focus on approaches to help lay users mitigate these problems.
The COVID-19 pandemic has underscored the need for effective diagnostic tools, particularly in resource-limited settings. While RT-PCR and CT scans are standard, their limitations drive the need for advanced techniques. This study leverages Convolutional Neural Networks, Knowledge Distillation, Ensemble Learning, and Federated Learning to develop robust, privacy-preserving models for COVID-19 detection from CT scans. We propose two federated learning strategies to simplify deep learning models for use in clinical environments with limited computational resources. The first strategy uses knowledge distillation from a complex model to a simplified model shared across a federated network. The second allows each hospital to distill knowledge to its simplified model, later combined into a global model via ensemble learning. Our methods, AFKD and IKDEFL, outperform traditional federated learning approaches such as FedAvg and FedAdam. AFKD, paired with the COVID-CNN model, achieves 91%-95% accuracy on IID (Independent and Identically Distributed) datasets and 70%-89% on non-IID datasets. IKDEFL further improves performance, with 92%-95% accuracy on IID datasets and 76%-88% on non-IID datasets. These approaches provide promising solutions for enhancing COVID-19 detection in federated learning.
The study of space-bounded computation has drawn extensively from ideas and results in the field of communication complexity. Catalytic Computation (Buhrman, Cleve, Koucký, Loff and Speelman, STOC 2013) studies the power of bounded space augmented with a pre-filled hard drive that can be used non-destructively during the computation. Presently, many structural questions in this model remain open. Towards a better understanding of catalytic space, we define a model of catalytic communication complexity and prove new upper and lower bounds. In our model, Alice and Bob share a blackboard with a tiny number of free bits, and a larger section with an arbitrary initial configuration. They must jointly compute a function of their inputs, communicating only via the blackboard, and must always reset the blackboard to its initial configuration. We prove several upper and lower bounds: 1) We characterize the simplest nontrivial model, that of one bit of free space and three rounds, in terms of 𝔽₂ rank. In particular, we give natural problems that are solvable with a minimal-sized blackboard that require near-maximal (randomized) communication complexity, and vice versa. 2) We show that allowing constantly many free bits, as opposed to one, allows an exponential improvement on the size of the blackboard for natural problems. To do so, we connect the problem to existence questions in extremal graph theory. 3) We give tight connections between our model and standard notions of non-uniform catalytic computation. Using this connection, we show that with an arbitrary constant number of rounds and bits of free space, one can compute all functions in TC⁰. We view this model as a step toward understanding the value of filled space in computation. 
For graphs of average degree d, positive integer weights bounded by W, and accuracy parameterϵ>0, [Chazelle, Rubinfeld, Trevisan; SICOMP'05] have shown that the weight of the minimum spanning tree can be (1+ϵ)-approximated in ~O(Wd/ϵ2) expected time. This algorithm is frequently taught in courses on sublinear time algorithms. However, the ~O(Wd/ϵ2)-time variant requires an involved analysis, leading to simpler but much slower variations being taught instead. Here we present an alternative that is not only simpler to analyze, but also improves the number of queries, getting closer to the nearly-matching information theoretic lower bound. In addition to estimating the weight of the MST, our algorithm is also a perfect sampler for sampling uniformly at random an edge of the MST. At the core of our result is the insight that halting Prim's algorithm after an expected~O(d) number of steps, then returning the highest weighted edge of the tree, results in sampling an edge of the MST uniformly at random. Via repeated trials and averaging the results, this immediately implies an algorithm for estimating the weight of the MST. Since our algorithm is based on Prim's, it naturally works for non-integer weighted graphs as well. 
Modern cities have hundreds to thousands of traffic cameras distributed across them, many of them with the capa- bility to pan and tilt, but very often these pan and tilt cameras either do not have angle sensors or do not provide camera orientation feedback. This makes it difficult to robustly track traffic using these cameras. Several methods to automatically detect the camera pose have been proposed in literature, with the most popular and robust being deep learning-based approaches. However, they are compute intensive, require large amounts of training data, and generally cannot be run on embedded devices. In this paper, we propose TIPAngle – a Siamese neural network, lightweight training, and a highly optimized inference mechanism and toolset to estimate camera pose and thereby improve traffic tracking even when operators change the pose of the traffic cameras. TIPAngle is 18.45x times faster and 3x more accurate in determining the angle of a camera frame than a ResNet-18 based approach. We deploy TIPAngle to a Raspberry Pi CPU and show that processing an image takes an average of .057s, equating to a frequency of about 17Hz on an embedded device. 
The proliferation of malware in today’s society continues to impact industry, government, and academic organizations. The Dark Web provides cyber criminals with a venue to exchange and store malicious code and malware. Hence, this research develops a crawler to harvest source code, scripts, and executable files that are freely available on the Dark Web to investigate the proliferation of malware. Harvested executable files are analyzed with publicly accessible malware analysis tool services, including VirusTotal, Hybrid Analysis, and MetaDefender Cloud. The crawler crawls over 15 million web pages and collects over 20 thousand files consisting of code, scripts, and executable files. Analysis of the data examines the distribution of files collected from the Dark Web, the differences in the results between the analysis services, and the malicious classification of files. The results reveal that about 30% of the harvested executable files are considered malicious by the malware analysis tools. 
Faults on power lines and other electric equipment are known to cause wildfire ignitions. To mitigate the threat of wildfire ignitions from electric power infrastructure, many utilities preemptively de-energize power lines, which may result in power shutoffs. Data regarding wildfire ignition risks are key inputs for effective planning of power line de-energizations. However, there are multiple ways to formulate risk metrics that spatially aggregate wildfire risk map data, and there are different ways of leveraging this data to make decisions. The key contribution of this paper is to define and compare the results of employing six metrics for quantifying the wildfire ignition risks of power lines from risk maps, considering both threshold- and optimization-based methods for planning power line de-energizations. The numeric results use the California Test System (CATS), a large-scale synthetic grid model with power line corridors accurately representing California infrastructure, in combination with real Wildland Fire Potential Index data for a full year. This is the first application of optimal power shutoff planning on such a large and realistic test case. Our results show that the choice of risk metric significantly impacts the lines that are de-energized and the resulting load shed. We find that the optimization-based method results in significantly less load shed than the threshold-based method while achieving the same risk reduction. 
The surge of covid-19-positive cases and mortality among different communities in the state of Louisiana are concerning. It has affected us in different ways: psychologically, physically (mobility restriction), socially, and economically. It is a global catastrophe and all of us are dealing with multiple challenges due to this. As of 9th April 2023, there are almost 1.6 million covid-19 cases and 18,984 people lost their lives in the state of Louisiana. This pandemic created tremendous pressure in healthcare with an unexpected surge in the demand (more than existing production capability). According to our data, there were 3,022 covid patients hospitalized on 08/17/2021, and there were 571 covid-positive patients on the ventilator on 04/04/2020 on a single day. Louisiana has about 33% black population which is about half of white population of 63.0%. However, the covid infection rate was almost 20.0% higher in the black population compared to the white population. Here, we present a demographic chart, the infection rate, and death by region and race in different communities in Louisiana. 
Reconfigurable Intelligent Surfaces (RIS) have emerged as one of the key enabling technologies for beyond 5G and 6G networks due to their capability to create virtual line-of-sight (LOS) paths between transmitters and receivers in non-line-of-sight (NLOS) environments. However, to achieve narrow beams and significantly enhanced received power, RIS must incorporate a large number of elements. This, in turn, presents a challenge due to the substantial beam training overhead required for optimal beam selection, particularly when dealing with larger RIS arrays. In this work, we propose a novel approach that leverages multi-modal sensing data to guide the beam selection process in RIS, especially in mobile environments where frequent beam alignment is necessary. This approach is motivated by promising results from recent research (G. Charan, T. Osman, A. Hredzak, N. Thawdar and A. Alkhateeb, “Vision-Position Multi-Modal Beam Prediction Using Real Millimeter Wave Datasets,” 2022 IEEE Wireless Communications and Networking Conference (WCNC), 2022, pp. 2727-2731; U. Demirhan and A. Alkhateeb, “Radar Aided 6G Beam Prediction: Deep Learning Algorithms and Real-World Demonstration,” 2022 IEEE Wireless Communications and Networking Conference (WCNC), 2022, pp. 2655–2660), which demonstrated that multi-modal sensing, combined with machine learning, can effectively extract wireless channel information and map it to optimal mmWave beams in vehicle-to-infrastructure (V2I) communication scenarios. These findings suggest that sensing data from co-located sensors and mmWave systems can significantly reduce the beam training overhead in mmWave communication, with minimal impact on system performance.
I/O constitutes a significant portion of most of the application run-time. Spawning many such applications concurrently on an HPC system leads to severe I/O contention. Thus, understanding and subsequently reducing I/O contention induced by such multi-tenancy is critical for the efficient and reliable performance of the HPC system. In this study, we demonstrate that an application’s performance is influenced by the command line arguments passed to the job submission. We model an application’s I/O behavior based on two factors: past I/O behavior within a time window and user-configured I/O settings via command-line arguments. We conclude that I/O patterns for well-known HPC applications like E3SM and LAMMP are predictable, with an average uncertainty below 0.25 (A probability of 80%) and near zero (A probability of 100%) within a day. However, I/O pattern variance increases as the study time window lengthens. Additionally, we show that for 38 users and at least 50 applications constituting approximately 93000 job submissions, there is a high correlation between a submitted command line and the past command lines made within 1 to 10 days submitted by the user. We claim the length of this time window is unique per user.
With the recent rise in the installation of wind turbines, extreme weather conditions such as heavy rainfall and icing weather have been found to affect the performance of wind turbines seriously. Offshore wind turbines are especially affected because of the addition of sea spray icing. The added water will increase the overall Liquid Water Content (LWC) as the turbine blade rotates towards the ocean, resulting in two distinct icing regimes: in-cloud icing at the top of the rotation circle, and sea-spray icing at the bottom. In the present study, experiments are conducted to quantify the effects of high liquid water content (LWC) conditions at the tip region of an offshore wind turbine blade using advanced flow diagnostic techniques. The aerodynamic performance degradation of the turbine blade is first studied through dynamic measurements of the lift and drag force while high-speed imaging captures the dynamic ice accretion process. Consequently, Particle Image Velocimetry is then used to capture the change in the flow field around the airfoil model as the ice builds up. Different LWC values and temperature values are tested to cover the entire spectrum of offshore wind turbine icing conditions, including glaze ice, mixed ice, and rime ice.
Inconel 718 (IN718) is a nickel-based superalloy commonly used in aerospace applications due to its exceptional performance in high-temperature environments. The use of metal additive manufacturing (AM) technologies (i.e., laser powder bed fusion (LPBF), binder jetting (BJ), and direct energy deposition (DED)) to produce these components have become more prevalent given their exhibited comparable mechanical performance to wrought counterparts. Nevertheless, the considerable expense in acquisition and maintenance of these AM technologies limits their potential to provide a cost-efficient approach from part design to implementation. This study focuses on assessing the viability of the low-cost material extrusion additive manufacturing (MEAM) technology, on manufacturing Inconel 718 samples for fatigue applications. The MEAM process which is dictated by its manufacturing of a component from metal powder bounded within a polymeric filament through the extrusion process, is used to print varying IN718 specimen geometries (i.e., cube, tensile, torsion, and rotating bending fatigue test specimens) in the as-built (green) state. The role of processing parameters (i.e., increased flow rate, hot end temperature etc.) on part quality and defect presence was assessed, and monotonic tensile and torsion tests were performed to assess the structural integrity and mechanical properties of MEAM IN718 in the as-built (green state). This study is novel in that it sets the framework for optimization of 3D print processing parameters on as-built (green) fatigue specimen design prior to future work, which will assess impacts of the debinding and sintering process.
Reynolds-averaged Navier-Stokes (RANS) models are widely used in practical aerospace engineering designs because of their low computational costs. However, RANS models' assumptions and simplifications may not provide accurate predictions for complex flows with separation and strong adverse pressure gradients. This study uses field inversion machine learning (FIML) to improve RANS turbulence models' accuracy in predicting separated flows over airfoils. Most existing FIML studies focused on steady-state or time-averaged unsteady flows, and this paper is a step forward to improve prediction accuracy for time-resolved unsteady flows. We augment the Spalart-Allmaras (SA) turbulence model's production term with a spatial scalar field. We then compute the above scalar field using a solver-imbedded neural network (NN) model, which takes the local flow features as the inputs at each time step. We optimize the weights and biases of the built-in NN model to minimize the regulated temporal prediction error between the augmented flow solver and reference data. We consider the unsteady separated flow over a NACA0012 airfoil at a large angle of attack. We use only the time series of drag coefficient as the training data, and the trained model can accurately predict the spatial-temporal evolutions of other surface variables, such as lift, pitching moment, and pressure distribution around the airfoil, as well as field variables, such as velocity and pressure. The FIML-trained model has the potential to be generalized to accurately predict airfoil aerodynamics for different shapes and flow conditions.
Many aerospace engineering design problems require efficient solutions of unsteady flow and adjoint equations. Implicit Runge--Kutta (IRK) is an efficient temporal discretization scheme for unsteady flow solutions because it allows high order accuracy and relaxed Courant numbers. However, existing IRK studies mostly focused on fully coupled solvers for compressible flow. In this paper, we develop an IRK-PIMPLE method using the two-stage Gauss scheme for segregated flow solvers, along with the Gauss IRK-adjoint formulation for efficient gradient computations. We modify the standard iterative PIMPLE method with block Gauss-Seidel sweeps to work with the two-stage Gauss scheme, and we also simplify the Gauss IRK-adjoint formulation to an easy-to-evaluate form. We validate the proposed IRK-PIMPLE method by simulating unsteady flow over a ramp geometry and comparing the resulting flow field against the second-order backward scheme reference. We implement the Gauss IRK-adjoint for the scalar transport problem, and the adjoint derivative agrees reasonably well with the finite difference reference. The proposed IRK-PIMPLE and its corresponding adjoint solver have the potential to significantly reduce the computation cost for time-resolved unsteady optimization using segregated flow solvers.
Variational system identification is a new formulation of maximum likelihood for estimation of parameters of dynamical systems subject to process and measurement noise, such as aircraft flying in turbulence. This formulation is an alternative to the filter-error method that circumvents the solution of a Riccati equation and does not have problems with unstable predictors. In this paper, variational system identification is demonstrated for estimating aircraft parameters from real flight-test data. The results show that, in real applications of practical interest, it has better convergence properties than the filter-error method, reaching the optimum even when null initial guesses are used for all parameters and decision variables. This paper also presents the theory behind the method and practical recommendations for its use. 
This work presents a numerical model to evaluate the energy deposition pathways activated during plasma-assisted ignition of a stoichiometric ammonia/air mixture at atmospheric conditions. To that end, zero-dimensional simulations are conducted of ignition by nanosecond repetitively pulsed discharges (NRPD), for which a detailed energy tracking is performed from the electrical input to the mechanisms that directly influence ignition. The analysis of the plasma energy deposition pathways is relevant to gain insight into complex plasma kinetic mechanisms and help optimize actuation strategies. Results show that the main pathways activated are: (i) slow-gas heating, (ii) fast-gas heating, (iii) dissociation of NH3 into NH2, (iv) dissociation of O2, and (v) dissociation of NH3 into NH. The exact proportion of energy deposited in each pathway is highly dependent on the reduced electric field profile, in particular the "down-slope" dominates the energy breakdown. The influence of the reduced electric field magnitude is studied for square pulses. The correlation between the various plasma energy deposition pathways and NOx emissions is quantified. 
We propose VecKM, a local point cloud geometry encoder that is descriptive and efficient to compute. VecKM leverages a unique approach by vectorizing a kernel mixture to represent the local point cloud. Such representation's descriptiveness is supported by two theorems that validate its ability to reconstruct and preserve the similarity of the local shape. Unlike existing encoders down-sampling the local point cloud, VecKM constructs the local geometry encoding using all neighboring points, producing a more descriptive encoding. Moreover, VecKM is efficient to compute and scalable to large point cloud inputs: VecKM reduces the memory cost from (n2 + nKd) to (nd + np); and reduces the major runtime cost from computing nK MLPs to n MLPs, where n is the size of the point cloud, K is the neighborhood size, d is the encoding dimension, and p is a marginal factor. The efficiency is due to VecKM's unique factorizable property that eliminates the need of explicitly grouping points into neighbors. In the normal estimation task, VecKM demonstrates not only 100× faster inference speed but also highest accuracy and strongest robustness. In classification and segmentation tasks, integrating VecKM as a preprocessing module achieves consistently better performance than the PointNet, PointNet++, and point transformer baselines, and runs consistently faster by up to 10 times. 
This paper leverages the framework of algorithms-with-predictions to design data structures for two fundamental dynamic graph problems: incremental topological ordering and cycle detection. In these problems, the input is a directed graph on n nodes, and the m edges arrive one by one. The data structure must maintain a topological ordering of the vertices at all times and detect if the newly inserted edge creates a cycle. The theoretically best worst-case algorithms for these problems have high update cost (polynomial in n and m). In practice, greedy heuristics (that recompute the solution from scratch each time) perform well but can have high update cost in the worst case. In this paper, we bridge this gap by leveraging predictions to design a learned new data structure for the problems. Our data structure guarantees consistency, robustness, and smoothness with respect to predictions--that is, it has the best possible running time under perfect predictions, never performs worse than the best-known worst-case methods, and its running time degrades smoothly with the prediction error. Moreover, we demonstrate empirically that predictions, learned from a very small training dataset, are sufficient to provide significant speed-ups on real datasets. 
Sustainable aviation fuel (SAF) is a promising solution to mitigate the environmental impact of air travel. Because commercial jets contribute significantly to global greenhouse emissions, the accurate characterization of renewable and petroleum-based fuel mixtures is essential for ensuring regulatory compliance. Additionally, the implementation of in-situ fuel sensors allows for the continuous monitoring of fuel mixtures, enabling real-time quality control and enhanced engine performance. This paper explores the use of time-domain nuclear magnetic resonance (TD-NMR) to differentiate between mixtures of Jet A and Hydro-processed Renewable Jet (HRJ) Camelina. By analyzing 1H TD-NMR relaxation data, we demonstrate a linear correlation between the decay rate and concentration of Jet A and HRJ Camelina mixtures. The fuel mixtures were tested using an open-source 1H TD-NMR system developed by the authors that employs a 0.65~Tesla permanent magnet and operates at a Larmor frequency of 27.68~MHz. Results show that higher concentrations of Jet A yield slower relaxation rates. Furthermore, T_2 decay rate is shown to vary linearly with fuel composition, with correlation between the measured and synthesized relaxation rates of 11 mixtures of Jet A and HRJ Camelina achieving an R^2 value of 0.9845. The TD-NMR approach presented in this work provides a simple and efficient technique for on-site SAF characterization, an integral step towards facilitating the greater adoption of SAF and compliance with future regulations in the aviation industry.
We extend the recently proposed structured input-output analysis framework to identify quasi-secondary instabilities in compressible flows. A key component of any structured input-output analysis is the uncertainty modeling, which arises due to a pseudo-linearization of the quadratic nonlinearity in the governing flow equations. The uncertainty element can be interpreted as a realization of a flow-field ‘frozen’ in time and the homogeneous spatial coordinates, which modifies elements of the base flow with which linear instabilities interact (i.e., quasi-secondary instabilities). We improve upon recently introduced methods for computing structured uncertainties that arise in the context of incompressible flows, focusing on straightforwardness and computational tractability. Results from our proposed analysis method qualitatively match prior results in the literature on incompressible Couette and channel flows. We then extend these methods for the analysis of compressible flows. Analysis of a compressible laminar Couette flow at subsonic, transonic, and supersonic conditions reveals that the spatial auto-correlation structures capture the Mach-number-dependence of the momentum and thermodynamic properties of a flow instability. For example, the specific volume auto-correlations amplify with an increase in the Mach number and thereby introduce a larger modification to the thermodynamic properties of the base flow. These findings are further corroborated by the structured forcing modes associated with the instability.
This paper studies the instability of deployable structures consisting of transversely non-uniform curved, open-section thin-shells subjected to a pure bending moment. Typically, cylindrical shells have been made uniformly curved and symmetrical for their simplicity, such as tubular extendable booms and carpenter-tape springs. However, recent consideration has been given to general curved thin-shell structures for aviation, possessing a more complex geometry to meet aerodynamic requirements. This paper investigate the buckling instability of non-uniformly curved thin-shell. Two samples are investigated: a deployable airfoil-shape thin shell with continuous varying transverse curvature and a deployable boom including a constant curvature flange with a flat web. An analytical study is conducted to predict the buckled cross-section shape based on variational method. Then the normal reaction force, bending moment and in-plane moment are calculated with this analytical model. All results are validated through finite element analysis.
Stability analysis of Mach 5.35 boundary layer flow over a flat plate with two- and three- dimensional sinusoidal roughness is performed and presented. The impact of the distributed roughness amplitude is shown for the second Mack mode, which is the dominant instability for this flow condition. It is found for all roughness heights examined that the second Mack mode is appreciably destabilized relative to the smooth wall flat plate. An energy budget analysis was performed for a selected roughness case and compared with the smooth wall. It was shown that the primary mechanism for destabilization of the second mode in the roughness case is heightened Reynolds stress energy production owing to the compression-expansion shock wave system formed by the roughness elements peaks. Decreased (relative to the smooth wall) disturbance energy dissipation via heat conduction caused by the separated flow over the roughness trough combined to contributed to the destabilization as well. Finally, the same 2-D roughness surface when extended into 3-D showed even larger destabilization of 2-D waves.
The management of intense thermal loads encountered by hypersonic vehicles operating in the atmosphere can be effectively alleviated by utilizing transpiration cooling, which involves the injection of cold fluids at the vehicle's surface. However, this injection process can substantially impact flow stability and potentially induce a premature transition to turbulence. The primary objective of this study is to comprehensively understand the mechanisms responsible for destabilizing flow during transpiration cooling application. To assess the influence of injection gas properties on the stability of high-enthalpy boundary layer flows, air and CO$_2$ injections are being considered. Both fluids are introduced at the same mass flux, resulting in a comparable reduction in heat flux. A reduction in the boundary layer thickness was noted due to the decrease in thermal-diffusivity with CO$_2$ injection compared to air. The injection of air caused the neutral curve to shift further upstream compared to the no-blowing case. Additionally, the frequency leading to the maximum amplification shifted to lower values due to increased Reynolds number based on the boundary layer thickness. The increased destabilization observed with CO$_2$ injection can be attributed to reduced dissipation compared to air injection.
Unmanned aerial systems (UAS) operating in dynamic environments must ensure safety for the duration of their flight. This paper presents an optimization method for planning safe, kinematically constrained, and time-constrained UAS trajectories in the presence of static and dynamic obstacles. Vehicle constraints are applied to a novel 4D 5$^\text{th}$-order polynomial spline formulation and the results are presented to show meaningful relationships between constraints, optimality, and feasibility. This 4D 5$^\text{th}$-order polynomial spline is shown to be a special subset of the B\'ezier spline, which ensures that our approach inherits the advantages and convex hull properties of B\'ezier splines. The connection with B\'ezier splines will allow algorithms designed for B\'ezier splines to be used with 4D-polynomial splines in future work. Numerical experiments show the success of this approach in providing optimal trajectories for UAS navigating dynamic environments. The experiments further illustrate when gradient descent optimization will result in feasible trajectories, sub-optimal solutions, or distributions of optimal trajectories.
Differential privacy and sublinear algorithms are both rapidly emerging algorithmic themes in times of big data analysis. Although recent works have shown the existence of differentially private sublinear algorithms for many problems including graph parameter estimation and clustering, little is known regarding hardness results on these algorithms. In this paper, we initiate the study of lower bounds for problems that aim for both differentially-private and sublinear-time algorithms. Our main result is the incompatibility of both the desiderata in the general case. In particular, we prove that a simple problem based on one-way marginals yields both a differentially-private algorithm, as well as a sublinear-time algorithm, but does not admit a "strictly" sublinear-time algorithm that is also differentially private. 
With the development of AI technologies, especially generative AI (GAI) like ChatGPT, GAI is increasingly assisting people in various tasks. However, people may have different requirements for GAI when using it for different kinds of tasks. For instance, when brainstorming new ideas, people may want GAI to propose different ideas that supplement theirs with different problem-solving perspectives, but for decision-making tasks, they may prefer GAI adopt a similar problem-solving process with people to make a similar or even the same decision as they would. We conducted an online experiment examining how perceived similarities between GAI and human task-solving influence people’s intention to use GAI, mediated by trust, for four task types (creativity, planning, intellective, and decision-making tasks). We demonstrate that the effect of similarity on trust (and so intent to use AI) depends on the type of task. This paper contributes to understanding the impact of task types on the relationship between perceived similarity and GAI adoption, with implications for future use of GAI in various task contexts.