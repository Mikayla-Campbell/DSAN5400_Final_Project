The retrospective is a crucial component of the agile software development process. In previous studies of retrospectives in undergraduate team software development projects, students exhibited limited and shallow reflection. We speculate that this is due to students' limited experience with reflection and the absence of clear guidance for engaging in deep reflection during agile retrospectives. To explore the potential for a pedagogical intervention to foster deeper reflection in retrospectives, we present an empirical comparison of a standard retrospective model against an enhanced retrospective model that scaffolds deeper levels of reflection by prompting students to justify and critique their practices and weigh alternative approaches. Through a systematic classification of the reflection level of statements made during individual brainstorming and team discussion phases of retrospectives, our study found that the enhanced model led to individuals and teams engaging in significantly higher levels of reflection. Our findings contribute to improving software engineering education by demonstrating the efficacy of an enhanced pedagogical model for team retrospectives. 
This paper offers a comparative study of two soils- Glauconite and Ottawa F65- utilizing X-ray micro-computed tomography (µCT) scan. The tendency of glauconite sand to transform from coarse to fine-grained material through particle crushing poses challenges in terms of stability and strength, particularly in foundation engineering and offshore site investigation. This paper investigates the particle size distribution and explores the subtleties of particle characteristics. Non-invasive µCT and 3D image analysis are used to measure and compare particle shape parameters: median aspect ratio (0.56 for Glauconite,0.54 for Ottawa F65), median convexity is 0.86 for both soils, and median sphericity (0.81 for Glauconite, 0.83 for Ottawa F65). By drawing comparisons between the statistical data of particle shape parameters from both soils, insights are gained into their morphological characteristics. Additionally, fitted Johnson distributions are provided for 3D Aspect ratio, sphericity, and convexity which may be useful for discrete element method modeling of these soils. 
Homomorphic encryption enables computations on the ciphertext to preserve data privacy with significant computational overhead compared to plaintext computations. In response to this challenge, we present HEDWIG, a novel end-to-end hardware acceleration system designed to enhance the efficiency of homomorphic encryption, specifically tailored for the latest BFV-HPS scheme. To the best of our knowledge, this is the first hardware accelerator for the BFV-HPS scheme variant. Key contributions include computation workload reduction via approximation, hardware resource optimization on modular computation modules, and computation workflow optimization to improve HE algorithm efficiency. The proposed HEDWIG is employed to perform homomorphic evaluations over machine learning model inference with a high reduction in DSP usage compared to prior BFV RNS variants FPGA accelerator with clock frequency reached 300MHz.
As AI continues to grow, modern applications are becoming more data- and compute-intensive, driving the development of specialized AI chips to meet these demands. One example is AMD's AI Engine (AIE), a dedicated hardware system that includes a 2D array of high-frequency very-long instruction words (VLIW) vector processors to provide high computational throughput and reconfigurability. However, AIE's specialized architecture presents tremendous challenges in programming and compiler optimization. Existing AIE programming frameworks lack a clean abstraction to represent multi-level parallelism in AIE; programmers have to figure out the parallelism within a kernel, manually do the partition, and assign sub-tasks to different AIE cores to exploit parallelism. These significantly lower the programming productivity. Furthermore, some AIE architectures include FPGAs to provide extra flexibility, but there is no unified intermediate representation (IR) that captures these architectural differences. As a result, existing compilers can only optimize the AIE portions of the code, overlooking potential FPGA bottlenecks and leading to suboptimal performance. To address these limitations, we introduce ARIES, an agile multi-level intermediate representation (MLIR) based compilation flow for reconfigurable devices with AIEs. ARIES introduces a novel programming model that allows users to map kernels to separate AIE cores, exploiting task- and tile-level parallelism without restructuring code. It also includes a declarative scheduling interface to explore instruction-level parallelism within each core. At the IR level, we propose a unified MLIR-based representation for AIE architectures, both with or without FPGA, facilitating holistic optimization and better portability across AIE device families. For the General Matrix Multiply (GEMM) benchmark, ARIES achieves 4.92 TFLOPS, 15.86 TOPS, and 45.94 TOPS throughput under FP32, INT16, and, INT8 data types on Versal VCK190 respectively. Compared with the state-of-the-art (SOTA) work CHARM for AIE, ARIES improves the throughput by 1.17x, 1.59x, and 1.47x correspondingly. For ResNet residual layer, ARIES achieves up to 22.58x speedup compared with optimized SOTA work Riallto on Ryzen-AI NPU. ARIES is open-sourced on GitHub: https://github.com/arc-research-lab/Aries. 
Real-time requirements are essential for safety-critical systems such as autonomous driving and robotics. In these systems, schedulability - ensuring every task completes before its deadline - is critical, as missing even a single deadline can lead to catastrophic consequences. Unlike Quality-of-Service (QoS) or service-level agreement (SLA) metrics, where occasional deadline misses are tolerable, time-predictable safety-critical systems demand strict adherence to deadlines. Existing multi-task scheduling frameworks, optimized for QoS/SLA, fail to meet these requirements, even when incorporating real-time techniques like preemption and scheduling. In FPGA-based systems, accelerators are widely used to handle real-time computational tasks. While existing accelerators focus on low latency and high throughput, these optimizations are insufficient when multiple tasks share a single accelerator, as low latency alone does not ensure schedulability. For example in Fig.1(a), a system with tasks of 20 ms and 400 ms periods, requiring 10 ms and 200 ms execution times, will miss deadlines on a 1 TOPS accelerator. Even increasing throughput to 10 TOPS (Fig.1(b)) without proper scheduling fails to resolve the issue. Furthermore, without a theoretical analysis, designers cannot determine whether the system after latency optimization can meet the deadline. As a result, these latency-optimized accelerators can not be safely used in time-predictable, safety-critical real-time systems. To address these challenges, we propose the ART framework: Accelerator Customization for Real-Time Systems. ART enables FPGA accelerators to support Earliest Deadline First (EDF) scheduling and limited preemption mechanisms, specifically targeting real-time safety-critical systems. Alongside hardware support, ART provides a software framework to analyze schedulability and balance between schedulability and performance. As shown in Figure 1(c), a 1 TOPS accelerator with ART can satisfy the schedulability requirement. Whereas the latency-optimized accelerator without ART in Figure 1(b) can not satisfy and thus is not viable in this scenario. Our contributions are summarized as follows: Necessity analysis of scheduling and preemption mechanism: We conduct a detailed case study to understand the necessity of scheduling and preemption. ART hardware framework and accelerator system: We propose the ART hardware framework and accelerator system for EDF scheduling and limited preemption on FPGA. ART can transform a baseline HLS accelerator to support these features. ART software framework: To guarantee the schedulability in real-time scenarios, we propose the ART software framework. We migrate the schedulability analysis and optimal preemption point placements to hardware acceleration, explain their effectiveness and package these algorithms into an automated framework. ART implementations and experiments: We implement and evaluate the ART accelerator system in AMD Zynq ZCU102, and Versal VCK190 platforms. The onboard experiment shows that ART guarantees the schedulability whereas the corresponding baseline accelerators can not. ART is lightweight, utilizing < 0.4% of resources on ZCU102 and < 0.1% on VCK190.
The Central High Plains are experiencing severe droughts due to climate change and diminishing water resources, impacting rural livelihoods. This study seeks to address these challenges by investigating biochar amendments to enhance the water-holding capacity (WHC) of sandy and loamy soils. Laboratory experiments, including gravimetric WHC tests, pressure plate tests, and contact angle measurements, were conducted using biochar from wood-based, oat hull, and sawdust feedstocks. Factors such as feedstock type, application rate, particle size, porosity, and hydrophobicity were analyzed. Results indicate that a 3% application rate of fine biochar particles (<0.6 mm) may significantly enhance WHC, with oat hull biochar showing the highest effectiveness due to its good combination of hydrophilicity, micro-porosity, and particle shape. These findings suggest that biochar properties significantly influence the WHC of soils, offering a sustainable solution for drought mitigation in the Central High Plains. Future studies should explore biochar and soil-wetting bacteria interactions to further enhance WHC, providing a robust strategy for water-scarce regions.
Federated Learning (FL) aims to train a shared model using data and computation power on distributed agents coordinated by a central server. Decentralized FL (DFL) utilizes local model exchange and aggregation between agents to reduce the communication and computation overheads on the central server. However, when agents are mobile, the communication opportunity between agents can be sporadic, largely hindering the convergence and accuracy of DFL. In this paper, we study delay-tolerant model spreading and aggregation enabled by model caching on mobile agents. Each agent stores not only its own model, but also models of agents encountered in the recent past. When two agents meet, they exchange their own models as well as the cached models. Local model aggregation works on all models in the cache. We theoretically analyze the convergence of DFL with cached models, explicitly taking into account the model staleness introduced by caching. We design and compare different model caching algorithms for different DFL and mobility scenarios. We conduct detailed case studies in a vehicular network to systematically investigate the interplay between agent mobility, cache staleness, and model convergence. In our experiments, cached DFL converges quickly, and significantly outperforms DFL without caching. 
Offshore wind turbines have become a feasible solution to meet the growing demands for renewable energy. However, offshore wind turbines with fixed foundations become increasingly economically and technically unfeasible at locations with large water depth. At these locations, floating platforms supported by mooring system and subsea anchors are a more feasible solution. Deeply embedded ring-shaped anchors can have greater efficiency than piles and caissons and greater capacity than drag anchors. In this paper, the result of a series of monotonic centrifuge load tests in clay is presented, with a focus on evaluation of the effect of loading inclination and anchor embedment depth on the tensile capacity and load-displacement response of the ring anchors. Tests were performed at the UC Davis Center for Geotechnical Modeling (CGM) with a scaling of 70g. The ring anchor models were embedded in normally consolidated kaolin clay. The clay shear strength was estimated from T-bar soundings performed in-flight. The anchors were connected to an actuator using taut steel wire ropes, and the line load, displacement, and inclination were measured. The results indicate that the capacity of the ring anchors increases as the loading inclination changes from vertical to horizontal. Increasing the embedment depth also resulted in an increase in capacity; however, the capacity normalized by the clay’s undrained shear strength is independent of depth. The interaction diagram describing the capacity as a function of load inclination and depth indicates that the ring anchor’s horizontal tensile capacity is much greater than its vertical capacity. Furthermore, analyses also show that the system stiffness increases as the load inclination angle increases. Overall, the results of this study show that the ring anchor could be a potential foundation solution with greater material efficiency than other alternatives.
Every year, humanity clears 10 million hectares of forests, which releases more than 5.6 billion tonnes of greenhouse gases. This significant contribution to climate change has led to the passage of global regulations, such as the EUDR, which aims to ensure that products linked to deforestation are excluded from the European market. Satellite-based remote sensing tools are popularly used for global monitoring to enable such compliance. However, they struggle to differentiate vegetation types in farms and orchards from forests (Fig.1.A). To solve this, we develop TerraTrace, a temporal signature mapping tool that combines Spectral Vegetation Indices, Satellite Imagery, and open data like Cropland Data Layer (CDL) to estimate historical land use. The key insight is that satellite-based spectral index data shows temporal variables like agricultural practices and plant growth cycles. Specifically, we demonstrate that yearly patterns of the Normalized Difference Vegetation Index (NDVI), based on plant photosynthesis, have temporal signatures unique to different crops, can distinguish forests from crops, and follow consistent patterns across different locations. Leveraging this we make the following contributions (Fig.2):
Developing a repertoire of notional machines (i.e., pedagogical tools for teaching programming) is essential for new computer science (CS) educators. However, there is a lack of documentation of notional machines and related pedagogical content knowledge (i.e., insights into teaching CS content). Our experience report addresses this lack of documentation and captures insights from our professional learning community. We co-designed an approach to use physical objects to teach inheritance in Java. Unlike a research paper that would rigorously document a few student outcomes with the expectation that these would generalize, our experience report shares our observations from multiple years of teaching with the goal of providing a number of things for educators to consider when teaching inheritance in Java. Drawing from an analysis of our meeting notes, we describe our instructional sequence, our perceptions of its current strengths and weaknesses for supporting students’ learning, insights from our previous failed attempts, and eight pedagogical practices. 
We present IMPS: Immersive Media Player System, a tightly synchronized 360º media player that leverages Android VR headsets to deliver immersive educational experiences. Designed for deployment in classrooms, IMPS allows instructors to manage synchronized playback for up to 50 headsets using a tablet interface. The system’s synchronization algorithm ensures lockstep playback across devices within 10 ms, addressing audio and video desynchronization issues of previous systems. IMPS has been successfully deployed by the Act One non-profit to deliver VR content to Title I schools in Arizona and is also used at Arizona State University for synchronized playback of 360º media in educational settings. 
Volumetric streaming is a powerful medium that transmits volumetric data, which primarily includes color and depth information, over a network in real-time. While color data can be effectively compressed using standard video codecs, compressing depth data poses a significant challenge as we need to change the bitrate for transmission over standard video codecs. Streaming depth data using standard video codecs introduces visual artifacts that degrade the quality of volumetric streaming workflows. To address this issue, we propose a demonstration exploring efficient compression techniques for color and depth data that minimize visual artifacts while preserving visual fidelity. Using a physical tabletop game filmed with volumetric cameras, we compare two voumetric streaming video workflows. The first workflow streams and renders the color and depth data directly from the camera in an uncompressed format, which is used as the ground truth. The second workflow encodes and streams the data in a compressed format. The comparison is conducted on a desktop system to evaluate performance for analyzing factors such as data transmission, compression quality, and overall system responsiveness.
AutoCalNet enables continuous real-time calibration of mobile 3D cameras by decoupling calibration from content streaming. It leverages a scalable device-edge-cloud network to minimize bandwidth, manage latency, and maintain high precision in calibration data, prioritizing trackable regions and feature points that will facilitate spatiotemporal tracking. This approach provides a flexible, efficient solution for networked camera systems without being constrained by content-specific requirements. 
Deep learning algorithms have depicted commendable performance in a variety of computer vision applications. However, training a robust deep neural network necessitates a large amount of labeled training data, which is timeconsuming and labor-intensive to acquire. This problem is even more serious for an application like image segmentation, as the human oracle has to hand-annotate each and every pixel in a given training image, which is extremely laborious. Active learning algorithms automatically identify the salient and exemplar samples from large amounts of unlabeled data, and tremendously reduce human annotation effort in inducing a machine learning model. In this paper, we propose a novel active learning algorithm for image segmentation, with the goal of further reducing the labeling burden on the human oracles. Our framework identifies a batch of informative images, together with a list of semantic classes for each, and the human annotator merely needs to answer whether a given semantic class is present or absent in a given image. To the best of our knowledge, this is the first research effort to develop an active learning framework for image segmentation, which poses only binary (yes/no) queries to the users. We pose the image and class selection as a constrained optimization problem and derive a linear programming relaxation to select a batch of (image-class) pairs, which are maximally informative to the underlying deep neural network. Our extensive empirical studies on three challenging datasets corroborate the potential of our method in substantially reducing human annotation effort for real-world image segmentation applications.
Test Time Adaptation (TTA) has emerged as a practical solution to mitigate the performance degradation of Deep Neural Networks (DNNs) in the presence of corruption/ noise affecting inputs. Existing approaches in TTA continuously adapt the DNN, leading to excessive resource consumption and performance degradation due to accumulation of error stemming from lack of supervision. In this work, we propose Domain-Aware Real- Time Dynamic Adaptation (DARDA) to address such issues. Our key approach is to proactively learn latent representations of some corruption types, each one associated with a sub-network state tailored to correctly classify inputs affected by that corruption. After deployment, DARDA adapts the DNN to previously unseen corruptions in an unsupervised fashion by (i) estimating the latent representation of the ongoing corruption; (ii) selecting the sub-network whose associated corruption is the closest in the latent space to the ongoing corruption; and (iii) adapting DNN state, so that its representation matches the ongoing corruption. This way, DARDA is more resource-efficient and can swiftly adapt to new distributions caused by different corruptions without requiring a large variety of input data. Through experiments with two popular mobile edge devices - Raspberry Pi and NVIDIA Jetson Nano - we show that DARDA reduces energy consumption and average cache memory footprint respectively by 1.74 x and 2.64 x with respect to the state of the art, while increasing the performance by 10.4%, 5.7% and 4.4% on CIFAR-10, CIFAR-100 and TinyImagenet.
The fast-paced development and deployment of private messaging applications demands mechanisms to protect against the concomitant potential for abuse. While widely used end-to-end encrypted (E2EE) messaging systems have deployed mechanisms for users to verifiably report abusive messages without compromising the privacy of unreported messages, abuse reporting schemes for systems that additionally protect message metadata are still in their infancy. Existing solutions either focus on a relatively small portion of the design space or incur much higher communication and computation costs than their E2EE brethren. This paper introduces new abuse reporting mechanisms that work for any private messaging system based on onion encryption. This includes low-latency systems that employ heuristic or opportunistic mixing of user traffic, as well as schemes based on mixnets. Along the way, we show that design decisions and abstractions that are well-suited to the E2EE setting may actually impede security and performance improvements in the metadata-hiding setting. We also explore stronger threat models for abuse reporting and moderation not explored in prior work, showing where prior work falls short and how to strengthen both our scheme and others'—including deployed E2EE messaging platforms—to achieve higher levels of security. We implement a prototype of our scheme and find that it outperforms the best known solutions in this setting by well over an order of magnitude for each step of the message delivery and reporting process, with overheads almost matching those of message franking techniques used by E2EE encrypted messaging apps today. 
A recent work introduces the problem of learning set functions from data generated by a so-called optimal subset oracle. Their approach approximates the underlying utility function with an energy-based model, whose parameters are estimated via mean-field variational inference. This approximation reduces to fixed point iterations; however, as the number of iterations increases, automatic differentiation quickly becomes computationally prohibitive due to the size of the Jacobians that are stacked during backpropagation. We address this challenge with implicit differentiation and examine the convergence conditions for the fixed-point iterations. We empirically demonstrate the efficiency of our method on synthetic and real-world subset selection applications including product recommendation, set anomaly detection and compound selection tasks. 
Continual learning (CL) learns a sequence of tasks incre- mentally. This paper studies the challenging CL setting of class-incremental learning (CIL). CIL has two key chal- lenges: catastrophic forgetting (CF) and inter-task class sep- aration (ICS). Despite numerous proposed methods, these issues remain persistent obstacles. This paper proposes a novel CIL method, called Kernel Linear Discriminant Analy- sis (KLDA), that can effectively avoid CF and ICS problems. It leverages only the powerful features learned in a foundation model (FM). However, directly using these features proves suboptimal. To address this, KLDA incorporates the Radial Basis Function (RBF) kernel and its Random Fourier Fea- tures (RFF) to enhance the feature representations from the FM, leading to improved performance. When a new task ar- rives, KLDA computes only the mean for each class in the task and updates a shared covariance matrix for all learned classes based on the kernelized features. Classification is performed using Linear Discriminant Analysis. Our empir- ical evaluation using text and image classification datasets demonstrates that KLDA significantly outperforms baselines. Remarkably, without relying on replay data, KLDA achieves accuracy comparable to joint training of all classes, which is considered the upper bound for CIL performance. The KLDA code is available at https://github.com/salehmomeni/klda. 
Large language models (LLMs) are expected to follow in- structions from users and engage in conversations. Tech- niques to enhance LLMs’ instruction-following capabilities typically fine-tune them using data structured according to a predefined chat template. Although chat templates are shown to be effective in optimizing LLM performance, their impact on safety alignment of LLMs has been less understood, which is crucial for deploying LLMs safely at scale. In this paper, we investigate how chat templates affect safety alignment of LLMs. We identify a common vulnerability, named ChatBug, that is introduced by chat templates. Our key insight to identify ChatBug is that the chat templates provide a rigid format that need to be followed by LLMs, but not by users. Hence, a malicious user may not necessar- ily follow the chat template when prompting LLMs. Instead, malicious users could leverage their knowledge of the chat template and accordingly craft their prompts to bypass safety alignments of LLMs. We study two attacks to exploit the ChatBug vulnerability. Additionally, we demonstrate that the success of multiple existing attacks can be attributed to the ChatBug vulnerability. We show that a malicious user can exploit the ChatBug vulnerability of eight state-of-the- art (SOTA) LLMs and effectively elicit unintended responses from these models. Moreover, we show that ChatBug can be exploited by existing jailbreak attacks to enhance their at- tack success rates. We investigate potential countermeasures to ChatBug. Our results show that while adversarial train- ing effectively mitigates the ChatBug vulnerability, the vic- tim model incurs significant performance degradation. These results highlight the trade-off between safety alignment and helpfulness. Developing new methods for instruction tuning to balance this trade-off is an open and critical direction for future research. 
Metric magnitude is a measure of the “size” of point clouds with many desirable geometric properties. It has been adapted to various mathematical contexts and recent work suggests that it can enhance machine learning and optimization algorithms. But its usability is limited due to the computational cost when the dataset is large or when the computation must be carried out repeatedly (e.g. in model training). In this paper, we study the magnitude computation problem, and show efficient ways of approximating it. We show that it can be cast as a convex optimization problem, but not as a submodular optimization. The paper describes two new algorithms – an iterative approximation algorithm that converges fast and is accurate, and a subset selection method that makes the computation even faster. It has been previously proposed that magnitude of model sequences generated during stochastic gradient descent is correlated to generalization gap. Extension of this result using our more scalable algorithms shows that longer sequences in fact bear higher correlations. We also describe new applications of magnitude in machine learning – as an effective regularizer for neural network training, and as a novel clustering criterion. 
We present Cloudscape, a dataset of nearly 400 cloud archi- tectures deployed on AWS. We perform an in-depth analysis of the usage of storage services in cloud systems. Our findings include: S3 is the most prevalent storage service (68%), while file system services are rare (4%); heterogeneity is common in the storage layer; storage services primarily interface with Lambda and EC2, while also serving as the foundation for more specialized ML and analytics services. Our findings provide a concrete understanding of how storage services are deployed in real-world cloud architectures, and our analysis of the popularity of different services grounds existing research. 
Reverse proxy servers play a critical role in optimizing Internet services, offering benefits ranging from load balancing to Denial of Service (DoS) protection. A known shortcoming of such proxies is that the backend server becomes oblivious to the IP address of the client who initiated the connection since all requests are forwarded by the proxy server. For HTTP, this issue is trivially solved by the X-Forwarded-For header, which allows the proxy server to pass to the backend server the IP address of the client that originated the request. Unfortunately, no such equivalent exists for many other protocols. To solve this issue, HAProxy created the PROXY protocol, which communicates client information from a proxy server to a backend server at a lower level in the network stack (Layer 4), making it protocol agnostic. In this work, we are the first to study the use of the PROXY protocol at Internet scale and investigate the security impact of its misconfigurations. We launched a measurement study on the full IPv4 address range and found that, over HTTP, more than 170,000 hosts accept PROXY protocol data from arbitrary sources. We demonstrate how to abuse this protocol to bypass onpath proxies (and their protections) and leak sensitive information from backend infrastructures. We discovered over 10,000 servers that are vulnerable to an access bypass, triggered by injecting a (spoofed) PROXY protocol header. Using this technique, we obtained access to over 500 internal servers providing control over IoT monitoring platforms and smart home automation devices, allowing us to, for example, regulate remote controlled window blinds or control security cameras and alarm systems. Beyond HTTP, we demonstrate how the PROXY protocol can be used to turn over 350 SMTP servers into open relays, enabling an attacker to send arbitrary emails from any email address. In sum, our study exposes how PROXY protocol misconfigurations lead to severe security issues that affect multiple protocols prominently used in the 
In applying deep learning for malware classifica- tion, it is crucial to account for the prevalence of malware evolution, which can cause trained classifiers to fail on drifted malware. Existing solutions to address concept drift use active learning. They select new samples for analysts to label and then retrain the classifier with the new labels. Our key finding is that the current retraining techniques do not achieve optimal results. These techniques overlook that updating the model with scarce drifted samples requires learning features that remain consistent across pre-drift and post-drift data. The model should thus be able to disregard specific features that, while beneficial for the classification of pre-drift data, are absent in post-drift data, thereby preventing prediction degradation. In this paper, we propose a new technique for detecting and classifying drifted malware that learns drift-invariant features in malware control flow graphs by leveraging graph neural networks with adversarial domain adaptation. We compare it with existing model retraining methods in active learning-based malware detection systems and other domain adaptation techniques from the vision domain. Our approach significantly improves drifted malware detection on publicly available benchmarks and real-world malware databases reported daily by security companies in 2024. We also tested our approach in predicting multiple malware families drifted over time. A thorough evaluation shows that our approach outperforms the state-of-the-art approaches. 
The online list update problem is defined as follows: we are given a list of items and the cost to access any particular item is its position from the start of the list. A sequence of item accesses come online, and our goal is to dynamically reorder the list so that the aggregate access cost is small. We study the stochastic version of the problem where the items are accessed i.i.d. from an unknown distribution p. The study of the stochastic version goes back at least 60 years to McCabe. In this paper, we first consider the simple online algorithm which swaps an accessed item with the item right before it, unless it is at the very front. This algorithm is known as the Transposition rule. Wetheoretically analyze the stationary behavior of Transposition and prove that its performance is within 1 + o(1) factor of the optimal offline algorithm for access sequences sampled from heavy-tailed distributions, proving a conjecture of Rivest from 1976. While the stationary behavior of the Transposition rule is theoretically optimal in the aforemen tioned i.i.d setting, it can catastrophically fail under adversarial access sequences where only the last and second to last items are repeatedly accessed. A desirable outcome would be a policy that performs well under both circumstances. To achieve this, we use reinforcement learning to design an adaptive policy that performs well for both the i.i.d. setting and the above-mentioned adversarial access. Unsurprisingly, the learned policy appears to be an interpolation between Move-to-Front and Transposition with its behavior closer to Move-to-Front for adversarial access sequences and closer to Transposition for sequences sampled from heavy tailed distributions suggesting that the policy is adaptive and capable of responding to patterns in the access sequence. 
Abstract—Code injection was a favored technique for attackers to exploit buffer overflow vulnerabilities decades ago. Subse- quently, the widespread adoption of lightweight solutions like write-xor-execute (W⊕X) effectively mitigated most of these attacks by disallowing writable-and-executable memory. However, we observe multiple concerning cases where software developers accidentally disabled W⊕X and reintroduced executable stacks to popular applications. Although each violation has been properly fixed, a lingering question remains: what underlying factors contribute to these recurrent mistakes among developers, even in contemporary software development practices? In this paper, we conduct two investigations to gain a comprehensive understanding of the challenges associated with properly enforcing W⊕X in Linux systems. First, we delve into program-hardening tools to assess whether experienced security developers consistently catch the necessary steps to avoid executable stacks. Second, we analyze the enforcement of W⊕X on Linux by inspecting the source code of the compilation toolchain, the kernel, and the loader. Our investigation reveals that properly enforcing W⊕X on Linux requires close collaboration among multiple components. These tools form a complex chain of trust and dependency to safeguard the program stack. However, developers, including security researchers, may overlook the subtle yet essential .note.GNU-stack section when writing assembly code for various purposes, and inadvertently introduce executable stacks. For example, 11 program-hardening tools implemented as inlined reference monitors (IRM) introduce executable stacks to all “hardened” applications. Based on these findings, we discuss potential exploitation scenarios by attackers and provide suggestions to mitigate this issue.
Federated learning collaboratively trains a neural network on a global server, where each local client receives the current global model weights and sends back parameter updates (gradients) based on its local private data. The process of sending these model updates may leak client’s private data information. Existing gradient inversion attacks can exploit this vulnerability to recover private training instances from a client’s gradient vectors. Recently, researchers have proposed advanced gradient inversion techniques that existing defenses struggle to handle effectively. In this work, we present a novel defense tailored for large neural network models. Our defense capitalizes on the high dimensionality of the model parameters to perturb gradients within a subspace orthogonal to the original gradient. By leveraging cold posteriors over orthogonal subspaces, our defense implements a refined gradient update mechanism. This enables the selection of an optimal gradient that not only safeguards against gradient inversion attacks but also maintains model utility. We conduct comprehensive experiments across three different datasets and evaluate our defense against various state-of-the-art attacks and defenses. Code is available at https://censor-gradient.github.io. 
Abstract—Federated learning is known for its capability to safeguard the participants’ data privacy. However, recently emerged model inversion attacks (MIAs) have shown that a malicious parameter server can reconstruct individual users’ local data samples from model updates. The state-of-the-art attacks either rely on computation-intensive iterative optimization methods to reconstruct each input batch, making scaling difficult, or involve the malicious parameter server adding extra modules before the global model architecture, rendering the attacks too conspicuous and easily detectable. To overcome these limitations, we propose Scale-MIA, a novel MIA capable of efficiently and accurately reconstructing local training samples from the aggregated model updates, even when the system is protected by a robust secure aggregation (SA) protocol. Scale-MIA utilizes the inner architecture of models and identifies the latent space as the critical layer for breaching privacy. Scale-MIA decomposes the complex reconstruction task into an innovative two-step process. The first step is to reconstruct the latent space representations (LSRs) from the aggregated model updates using a closed-form inversion mechanism, leveraging specially crafted linear layers. Then in the second step, the LSRs are fed into a fine-tuned generative decoder to reconstruct the whole input batch. We implemented Scale-MIA on commonly used machine learning models and conducted comprehensive experiments across various settings. The results demonstrate that Scale-MIA achieves excellent performance on different datasets, exhibiting high reconstruction rates, accuracy, and attack efficiency on a larger scale compared to state-of-the-art MIAs. Our code is available at https://github.com/unknown123489/Scale-MIA.
A three-week creative coding module was implemented as part of an introductory high school computer science class using a novel programming platform in virtual reality. This paper examines students' open-ended, independent final projects to assess how they leveraged coding concepts covered during lessons and guided practices. Projects reflected mastery of fundamentals such as input and output, lists, and events, but limited use of more complex structures, such as conditionals or loops. Strategies for improving transfer of learning from structured lessons to more independent, creative work are discussed.
Retrieval Augmented Generation (RAG) has been a recent improvement in providing recent and accurate data to Large Language Models (LLMs). Although RAG has been successful in reducing hallucinations within LLMs, it remains susceptible to inaccurate and maliciously manipulated data. In this paper, we present Distributed-RAG (D-RAG), a novel blockchain-based framework designed to increase the integrity of the RAG system. D-RAG addresses the risks of malicious data by replacing the RAG’s traditionally centralized database with communities, each consisting of a database and a permissioned blockchain. The communities are based on different subjects, each containing experts in the field who verify data through a privacy-preserving consensus protocol before it is added to the database. A Retrieval Blockchain is also designed to communicate between the multiple communities. The miners on this Retrieval Blockchain are responsible for retrieving documents from the database for each query and ranking them using an LLM. These rankings are agreed upon, and the top ranked documents are provided to the LLM with the query to generate a response. We perform experiments on our proposed D-RAG framework, and our results show that our Retrieval Blockchain is scalable and our privacy-preserving consensus protocol maintains efficiency as community members increase. These results demonstrate that in a real-world application setting D-RAG is scalable in maintaining data integrity. 
We present a solution to image-based cell counting with dot annotations for both 2D and 3D cases. Current approaches have two major limitations: 1) inability to provide precise locations when cells overlap; and 2) reliance on costly labeled data. To address these two issues, we first adopt the inverse distance kernel, which yields separable density maps for better localization. Second, we take advantage of unlabeled data by self-supervised learning with focal consistency loss, which we propose for our pixel-wise task. These two contributions complement each other. Together, our framework compares favorably against stateof- the-art methods, including methods using full annotations on 2D and 3D benchmarks, while significantly reducing the amount of labeled data needed for training. In addition, we provide a tool to expedite the labeling process for dot annotations. Finally, we make the source code and labeling tool publicly available. 
This study introduces a compact ultrawideband bowtie antenna architecture with a tapered slot feed for wireless communication and sensing applications. The design features a modified bowtie-dipole antenna configuration with incorporated circular slots for bandwidth enhancement and a tapered slot feed for improved impedance matching. The substrate used is FR-4, with a dielectric constant of 4.3 and a tangent loss of 0.025. A 1 Oz (0.035 mm) copper patch is employed. A peak gain of 7.24 dBi is achieved at a center frequency of 9.24 GHz. The overall dimensions of the proposed design are $1.63\lambda\times 1.73\lambda$. The antenna achieves a fractional bandwidth (FBW) of 136.5%, operating within a frequency range from 2.93 GHz to 15.55 GHz. It is designed for easy integration with most printed circuit board (PCB) designs, making it well-suited for broadband wireless communication and sensing applications.
This poster examines the commitment of the computer science (CS) education community to equity and justice. By analyzing 146 public pledges from 119 organizations using the Kapor Center's Culturally Responsive-Sustaining Computer Science (CR-SCS) Framework, we assessed focus areas such as racism, inclusive classroom cultures, rigorous curriculum, student voice, community engagement, and diverse experts. Findings show efforts in affirming student identities and community involvement but reveal gaps in addressing racism and recruiting diverse speakers. The results highlight the need for a comprehensive approach to ensure equity and inclusions in CS education. This study provides crucial insights into the current state of CS education and emphasizes the importance of holistic commitments towards justice.
In the realm of precision medicine, the prediction of phenotypic traits from genotype data plays a pivotal role in understanding disease susceptibility, drug response, and overall health outcomes. Machine learning models trained on genotype data offer promising avenues for phenotype prediction. However, the black-box nature of these models often hinders interpretabil-ity, raising concerns about their reliability and trustworthiness in clinical decision-making. This paper explores the application of SHapley Additive exPlanations (SHAP) and Local Interpretable Model-agnostic Explanations (LIME) on our existing CNN model to identify important features from genotype that contribute to phenotype prediction in yeast data. Both techniques on machine learning models include improving model transparency and interpretability by providing insights into prediction mechanisms. LIME focuses on local interpretability, explaining individual predictions tailored to specific instances swiftly, while SHAP computes feature importance using Shapley values, offering a comprehensive understanding of feature contributions. Our dataset contains 4,390 samples, each with 28,220 features in genotype which are used in the training, testing and validation of our existing CNN model. In our analysis of 20 phenotypes, we identify the number of relevant important features using SHAP, LIME methods, and the intersection which not only improves the Mean Squared Error (MSE) values compared to using all the features in the genotype data, but also the model training time.
Computer Science (CS) professional development has increased opportunities to broaden K12 teachers' and students' exposure to CS learning. However, many Indigenous-serving teacher professional development (PD) participants could not facilitate in-classroom implementation without significant follow-up and support. This study aims to understand how the factors emerging from our CS PD affected teachers' PD transition to in-classroom implementation. We analyzed multiple data sources collected over three years of our project. We use logistic regression to explore the data from the PD course. Through analysis, our study indicates that providing teachers with targeted mentorship and ensuring the completion of detailed lesson plans are two factors in the transition of teachers' learning from PD to teachers' implementations in the classroom.
Inclusive design appears rarely, if at all, in most undergraduate computer science (CS) curricula. As a result, many CS students graduate without knowing how to apply inclusive design to the software they build, and go on to careers that perpetuate the prolif- eration of software that excludes communities of users. Our panel of CS faculty will explain how we have been working to address this problem. For the past several years, we have been integrating bits of inclusive design in multiple courses in CS undergraduate programs, which has had very positive impacts on students' ratings of their instructors, students' ratings of the education climate, and students' retention. The panel's content will be mostly concrete examples of how we are doing this, so that attendees can leave with an in-the-trenches understanding of what this looks like for CS faculty across specialization areas and classes. We also show how it can be used in a department's BPC Plan and point to resources on the CRA's BPCnet Activity Library and on OERcommons, to enable interested faculty to go forward with this approach in their own classes and departments.
As generative AI products could generate code and assist students with programming learning seamlessly, integrating AI into programming education contexts has driven much attention. However, one emerging concern is that students might get answers without learning from the LLM-generated content. In this work, we deployed the LLM-powered personalized Parsons puzzles as scaffolding to write-code practice in a Python learning classroom (PC condition) and conducted an 80-minute randomized between-subjects study. Both conditions received the same practice problems. The only difference was that when requesting help, the control condition showed students a complete solution (CC condition), simulating the most traditional LLM output. Results indicated that students who received personalized Parsons puzzles as scaffolding engaged in practicing significantly longer than those who received complete solutions when struggling.
There is a lack of access to critical knowledge on machine ethics and the impacts of technology on individuals and communities in everyday life. This project pioneers an inclusive curriculum design process to broaden accessibility to machine ethics education. Our approach uses a ''source'' course to develop materials for seven "target" courses. The source course is a machine ethics curriculum development course in which students and faculty collaboratively build curricular materials for integration into non-computer science courses. Here we describe the development of the ''source'' course using a curriculum co-creation process that leverages student and faculty expertise. The process emphasizes an inclusive design approach, rooted in continuous stakeholder feedback and consistent, transparent communication. The products of this process include course materials that incorporate underrepresented ethical frameworks. Additionally, it features peer-reviewed journal assignments that promote reflective learning and sharing of diverse perspectives, as well as a final module project in which students collaborate with faculty to co-create curricular materials. Our approach aims to broaden a culturally relevant understanding of ethical challenges in technology while ensuring that the curriculum resonates with diverse student backgrounds. Our presentation will describe key insights about the process and products of our curriculum design. 
As AI tools become increasingly integrated into everyday life, there is a need to expand resources for K-12 AI education. In this work, we introduce a game-based learning activity focusing on teaching reinforcement learning concepts to middle school students. We present the iterative design of the activity, discussing its initial implementation and subsequent enhancements to improve student learning outcomes. The game-based learning activity was implemented during two summer camps with middle school students. We provide an overview of the two different versions of the activity and analyze key insights drawn from student feedback, collected through pre and post-activity surveys.
Engaging middle school students in complex computational topics such as AI can pose unique challenges to educators, ranging from simplifying potentially difficult mathematical material to maintaining student interest in the subject. One approach to help address these challenges is to utilize hands-on, real-world examples. We conducted a week-long summer camp for 24 students, centering each day around activities aligned with one of the Five Big Ideas in AI. Students participated in exit ticket reflections following each activity. The pathfinding activities, which occurred on one of the days, incorporated real-world examples and digital simulations of three pathfinding algorithms (breadth-first search, depth-first search, and A*), including an activity modeled after the game Pac-Man. Thematic analysis of exit-ticket responses revealed four major themes regarding students' key takeaways from the activities: (1) pathfinding for character movement, notably in video games like Minecraft; (2) pathfinding as a means of efficient navigation; (3) theoretical reasoning regarding the speed of the A* algorithm compared to others, highlighting its intelligent search mechanism; and (4) empirical reasoning based on personal experience during activities, where some students noted A* consistently performed fastest. These findings indicate that students not only engaged with AI concepts but also demonstrated a nuanced understanding of algorithmic efficiency. We examine the implications of these findings on understanding student engagement with interactive pathfinding activities and highlight the potential for future work in this area.
Identifying Knowledge Components (KCs) in computer science education improves curriculum design and teaching strategies. We introduce a framework using Large Language Models to identify KCs from programming assignments automatically. Our framework helps educators align assignments with course objectives. GPT-4 identifies relevant KCs well, though there's a low match with expert-generated KCs at the course level. At the problem level, performance is lower, but key KCs are reasonably identified.
Recent years have seen developments in AI instructional practices for K-12 students. In literature, students' interest in AI is shown to correlate with gaining AI knowledge; however, little is known about how AI interest manifests in classroom discourses during AI literacy lessons. This study examined students' participation in an integrated AI curriculum delivered to a cognitive science class in a high school in the southern US. Students worked in small groups and built a supervised machine learning model to recognize kids' drawings at different stages of artistic development. Our analysis showed that semantic features extracted from students' small group conversations significantly predicted their interest in learning AI. However, we found no significant relationship between students' social construction of knowledge and their interests. This study sheds light on the relationship between the learning process and interest; when further developed, this analysis may be developed into a classroom activity analytics tool that may provide real-time feedback to teachers engaged in AI literacy education to enhance teaching effectiveness in this nascent content area.
This paper presents an AI-enabled algorithmic flow for architecture discovery, circuit topology and parameter optimization for RFICs, particularly exploring design spaces beyond human intuition. RF and mmWave IC design is a complex iterative design process that involves co-design of circuits and electromagnetics (EM), including matching networks (MN), combiners, splitters, hybrids, baluns, switches, diplexers, beamforming networks, antennas, and the like. Design of such high-frequency circuits and EM structures has historically relied on intuitive and analytical approaches with starting template architectures. However, there is no reason to believe that such pre-selected topologies are close to achieving the optimal performance in the space of all possible circuit and EM topologies. Consider the design of a typical RFIC, such as a mmWave PA illustrated in Fig. 25.3.1. The design decision typically starts from output power requirements that determine the transistor sizes given the supply voltage. For efficient generation of high output power that requires a very high impedance transformation ratio, power combining may be necessary. Optimizing power combining and MN design is done through a series of trial-and-error processes taking losses, size and bandwidth into account. This iterative process is repeated for driver cells, inter-stage matching, and input splitters, and again with extracted layouts and EM simulations. This approach not only limits the design space to a small set of pre-fixed templates, but the design time can also be significant. Here, we propose an approach for an algorithmic design flow for RF/mmWave ICs, that allows: 1) architecture selection, circuit topology and parameter optimization, and inverse EM synthesis in a non-intuitive design space, and 2) drastic reduction of total design time by eliminating unnecessary iterative design processes. We demonstrate this methodology for a broadband mmWave and sub-THz PA, spanning 34-to-70GHz with peak $\mathrm{P}_{\text{sat}}$ of 21. $2\text{dBm}, \text{PAE}_{\max}$ of 26%, and a 100-to-120GHz sub-THz PA with peak $\mathrm{P}_{\text{sat}}$ of 12.6dBm. Compared to prior works on optimizing circuit parameters with simulation based analog low-frequency circuits [1], [2] or passive synthesis with human-designed architecture and circuits [3], this is the first work that demonstrates an end-to-end RFIC AI-enabled synthesis with both active and passive optimization, and from specifications to layout with fabricated and measured results.
This paper explores the efficacy of near-peer computing mentors in fostering computing identity among students in courses across disciplines. The Computing Fellows program ''attaches'' near-peer mentors to a wide range of undergraduate courses to create a cross-disciplinary learning environment for mentors and students. The mentors support the integration of computing into courses through activities including in-class workshops, one-on-one consultations, and drop-in office hours. Prior research has established the positive impact of near-peer mentoring on students' sense of computer science identity in computer science educational pathways. Our paper contributes to this literature by studying the experience of near-peer mentoring for students in courses across disciplines, including in courses in which there is no coding prerequisite. Our research demonstrates a positive relationship between increases in key aspects of computing identity and interaction with a computing fellow. In a multi-year mixed-methods study, we analyze student evaluations from courses with attached fellows and cross-reference the results with student focus group data. Among our findings, students who say they met with a fellow at least once demonstrate a reported increase in computing interest (not seen in our full sample) and computing competency (larger than in our full sample). These increases also hold among the sub-group of students in courses with no coding prerequisites. Our qualitative data further shows the variety of ways students describe the impact of computing fellows on their engagement with computing.
Background and Context. Existing works in computing students' help-seeking and resource selection identified an expanding set of important dimensions that students consider when choosing a help resource. However, most works either assume a predefined list of help resources or focus on one specific help resource, while the landscape of help resources evolve at a faster speed. Objectives. We seek to study how students value each dimension in the help landscape in their resource selection and utilization processes, as well as how their identities relate to their perceptions of the landscape. Method. We surveyed N=1,625 students on their perceptions of 8 dimensions across 12 offerings of 7 courses at 2 institutions. Findings. We found a consistent pattern of four distinct dimension tiers ordered from most to least important: (1) timeliness of help, (2) availability and adaptability of the resource, (3) the resource's time/space anchor and the effort to phrase the help need, (4) formality and socialness of the resource. We also found men and first-years rate all dimensions as less important than their classmates. Implications. Our results reveal what the students collectively value most when selecting help resources and thus can inform practitioners seeking to improve their course help ecosystem. 
Students from historically underrepresented communities in computer science (CS) report being told that their successes are due to special treatment based on their gender and/or racial identity. We refer to this microaggression as the discounting-success microaggression. We analyzed survey responses from 4,327 CS majors across 221 institutions in the U.S. We found that students who identify as women, Black, and/or Asian were more likely than men and white students, respectively, to report the discounting-success microaggression. This discounting-success microaggression significantly and negatively predicts students’ self-efficacy, sense of belonging, and plans to persist in CS. Our results elucidate the negative influence of the discounting-success microaggression on CS student outcomes. Efforts are needed to improve the culture and interactions in CS to eliminate the prevalence of this harmful microaggression. 
We present ASM Visualizer, a tool that is designed to help students learn assembly programming, aiding in their understanding of how assembly instructions are executed and the relationship between assembly and equivalent high-level language code. Our tool allows a user to step both forward and backward through the execution of an assembly program, one instruction at a time, seeing how instruc- tions use and modify values in stack memory and CPU registers. ASM Visualizer presents three user-interface modes, supporting different stages of learning assembly programming. Beginners can step through basic arithmetic instructions, whereas more advanced learners can trace through function call/return sequences, stack frame manipulation, or entire assembly programs. We present our experiences using ASM Visualizer in introductory-level courses at our two institutions, and we discuss other ways in which our tool could be used by educators in both introductory and advanced CS courses. Results from a preliminary assessment of students using our tool show that students gain confidence in their understanding of different aspects of assembly programming. We feel that the visual interface to assembly code execution that ASM Visualizer provides is key to helping students understand assembly. 
For the last 10 years, our university has offered a two-semester bridge into a master's in computer science for people with undergraduate degrees in non-computing disciplines. Since its inception, the program has expanded to eight campuses across North America and has opened admission to students from all disciplines, including non-STEM disciplines. The bridge program has over 2000 currently enrolled students, with more than 50% women every year since 2020, and domestic enrollment has increased relative to direct entry master's students. Our data show that bridge students, including those with non-STEM backgrounds, perform comparably to direct entry students in terms of GPA. We attribute much of the program's success to institutional investment in resources specifically designed to meet the unique needs of bridge students. These resources include dedicated academic and career advising, co-curricular programming, and the hiring of full-time teaching faculty specifically recruited to teach these bridge students. This paper examines data pertaining to the bridge program and MSCS from 2013 to 2023; it includes analyses of the expansion of the bridge program to eight campuses in North America, the admission of students with non-STEM degrees to the bridge, the achievement of enrolling over 50% women and non-binary identifying students, the success of bridge students in the MSCS program and in obtaining job placements, and domestic student enrollment growth as compared to traditional direct entry master's students. 
The "Computer Science for All" initiative advocates for universal access to computer science (CS) instruction. A key strategy toward this end has been to establish CS content standards outlining what all students should have the opportunity to learn. Standards can support curriculum quality and access to quality CS instruction, but only if they are used to inform curriculum design and instructional practice. Professional learning offered to teachers of CS has typically focused on learning to implement a specific curriculum, rather than deepening understanding of CS concepts. We set out to develop a set of educative resources, formative assessment tools and teacher professional development (PD) sessions to support middle school CS teachers' knowledge of CS standards and standards-aligned formative assessment literacy. Our PD and associated resources focus on five CS standards in the Algorithm and Programming strand and are meant to support teachers using any CS curriculum or programming language. In this experience report, we share what we learned from implementing our standards-based PD with four middle school CS teachers. Teachers initially perceived standards as irrelevant to their teaching but they came to appreciate how a deeper understanding of CS concepts could enhance their instructional practice. Analysis of PD observations and exit surveys, teacher interviews, and teacher responses to a survey assessing CS pedagogical content knowledge demonstrated the complexity of using content standards as a driver of high-quality CS instruction at the middle school level, and reinforced our position that more standards-focused PD is needed. 
FDI (False Data Injection) attacks are critical to address as they can compromise the integrity and reliability of data in cyber-physical systems, leading to potentially severe consequences in sectors such as power systems. The feasibility of FDI attacks has been extensively studied from various perspectives, including access to measurements and sensors, knowledge of the system, and design considerations using residual-based detection methods. Most research has focused on DC-based FDI attacks; however, designing AC FDI attacks involves solving a nonlinear optimization problem, presenting additional challenges in assessing their feasibility. Specifically, it is often unclear whether the infeasibility of some designed AC FDI attacks is due to the nonconvexity and nonlinearity inherent to AC power flows or if it stems from inherent infeasibility in specific cases, with local solvers returning infeasibility. This paper addresses this issue by leveraging the principle that if a convexified AC FDI attack design problem is infeasible, the attack design itself is infeasible, irrespective of nonlinear solution challenges. We propose an AC FDI attack design based on convexified power flow equations and assess the feasibility of the proposed attack by examining the extent of the attackable region. This approach utilizes a Quadratic Convex (QC) relaxation technique to convexify AC power flows. To evaluate the proposed method, we implement it on the IEEE 118-bus test system and assess the feasibility of an AC FDI attack across various attack zones.
The balance of supply and demand is pivotal in ensuring efficient and reliable power grid utilization. With the growth of demand, the integration of renewable energy resources (RERs) into the power grid is increasing phenomenally. Because of the random nature of RERs, the power grid is vulnerable to frequency and voltage stability issues. Enhancing the performance of generation forecasts is one of the efficient ways to deal with these issues. An accurate generation forecast will facilitate better planning to reduce the impacts caused by the unpredictable nature of RERS. Implementing deep learning (DL) methods in forecasting is becoming more popular as it has been proven to generate forecasts with high accuracy. This paper implements a DL method for solar irradiance forecasting using hyperparameter optimization (HPO). Solcast dataset is utilized to validate the efficacy of the proposed method. The model performance with activation functions, rectified linear unit, Leaky $\mathbf{ReLu}$, and exponential linear unit with and without HPO is compared. Additionally, Bayesian and random search optimization methods are implemented, and their accuracy is also compared. The result shows the significant enhancement of the model performance after HPO, proving its importance in deep learning applications. Finally, the activation function and the optimization method based on accuracy and time of execution are suggested.