The problems of changing the walking speed and stride length of impact-free gaits for point-foot planar bipeds are addressed. The impact-free gaits are designed using an approach developed in prior work. It is shown that the impulse controlled Poincar´e map (ICPM) approach can be modified to transition between orbits defining gaits with different walking speeds, and the continuous controller can be changed during the swing phase to transition between gaits that have distinct stride lengths. The effectiveness of the approaches is demonstrated using simulations carried out on a five-link biped.
Alternating-time temporal logic (ATL) extends branching time logic by enabling quantification over paths that result from the strategic choices made by multiple agents in various coalitions within the system. While classical temporal logics express properties of “closed” systems, ATL can express properties of “open” systems resulting from interactions among several agents. Reinforcement learning (RL) is a sampling-based approach to decision-making where learning agents, guided by a scalar reward function, discover optimal policies through repeated interactions with the environment. The challenge of translating high-level objectives into scalar rewards for RL has garnered increased interest, particularly following the success of model-free RL algorithms. This paper presents an approach for deploying model-free RL to verify multi-agent systems against ATL specifications. The key contribution of this paper is a verification procedure for model-free RL of quantitative and non-nested classic ATL properties, based on Q-learning, demonstrated on a natural subclass of non-nested ATL formulas.
Inverse rendering pipelines are gaining prominence in realizing photo-realistic reconstruction of real-world objects for emulating them in virtual reality scenes. Apart from material reflectances, spectral rendering and in-scene illuminants' spectral power distributions (SPDs) play important roles in producing photo-realistic images. We present a simple, low-cost technique to capture and reconstruct the SPD of uniform illuminants. Instead of requiring a costly spectrometer for such measurements, our method uses a diffractive compact disk (CD-ROM) and a machine learning approach for accurate estimation. We show our method to work well with spotlights under simulations and few real-world examples. Presented results clearly demonstrate the reliability of our approach through quantitative and qualitative evaluations, especially in spectral rendering of iridescent materials.
Precise seizure identification plays a vital role in understanding cortical connectivity and informing treatment decisions. Yet, the manual diagnostic methods for epileptic seizures are both labor-intensive and highly specialized. In this study, we propose a Hyperdimensional Computing (HDC) classifier for accurate and efficient multi-type seizure classification. Despite previous seizure analysis efforts using HDC being limited to binary detection (seizure or no seizure), our work breaks new ground by utilizing HDC to classify seizures into multiple distinct types. HDC offers significant advantages, such as lower memory requirements, a reduced hardware footprint for wearable devices, and decreased computational complexity. Due to these attributes, HDC can be an alternative to traditional machine learning methods, making it a practical and efficient solution, particularly in resource-limited scenarios or applications involving wearable devices. We evaluated the proposed technique on the latest version of TUH EEG Seizure Corpus (TUSZ) dataset and the evaluation result demonstrate noteworthy performance, achieving a weighted F1 score of 94.6%. This outcome is in line with, or even exceeds, the performance achieved by the state-ofthe-art traditional machine learning methods.
Electric vehicles (EVs) have the potential to serve as energy storage solutions through bidirectional charging technology, which allows them to both draw power from and feed power back into the grid, homes, or other vehicles. This capability enables EVs to reduce emissions, optimize costs, and support the grid by storing energy during periods of high production and supplying it when demand is high. In this vision paper, we focus on unlocking the potential of EVs as energy storage solutions while ensuring they remain readily available for transportation, their primary purpose. A significant research gap exists in that most current studies prioritize energy management, often using simplistic approaches that inadequately address the travel needs of EV owners. We believe the database community can be instrumental in maximizing the dual role of EVs as transportation and energy storage. We present a non-exhaustive list of research directions for various EV stakeholders, including individual EV owners, groups of independent yet cooperative EVs, commercial EV fleets, and autonomous EVs, and hope to inspire the database community for further exploration.
Text clustering methods traditionally rely on a shared vocabulary and script, which poses a challenge for cross-lingual text clustering problems that arise in a variety of domains including social media, news, finance, and more. Recent approaches to cross-lingual clustering have found success by leveraging latent embedding space representations of neural models and more recently by directly using Large Language Models (LLMs) to do text clustering in zero-shot or few-shot settings. However, much of the recent work focuses on short text, like social media posts. In this paper, we use cross-lingual clustering in the news domain as a case study to test whether LLMs can effectively cluster long documents by extracting and maintaining keyphrases associated with each cluster of documents. We compare the clustering several LLMs produce in a zero-shot setting to a more traditional online clustering method that uses TF-IDF to cluster documents based on their content and time of publication. We find that LLMs tend to cluster the articles based on the text, in particular based on the language of the text more than the content, and ignore the time and location of publication, indicating further work is needed before LLMs can reliably be used in clustering news articles across multiple languages.
Sun glare during driving poses a significant threat to driver and pedestrian safety. Navigation and route planning typically seeks to minimize the distance or time between the desired origin and destination, accounting for traffic patterns and other heuristics like minimizing the number of stoplights or left turns encountered on a route. However, current navigation methods do not support avoidance of complicated, temporally-dependent safety factors, like adverse road and environmental conditions. We take avoiding incident sun glare to the driver as an example of dynamic safety-aware navigation and lay out potential strategies for addressing this previously unexplored problem. We present a reinforcement learning-based method for computing sun glare-low routes through an elastic function that accounts for the direct angle between the sun and the driving direction. Our preliminary work shows that in some cases it is possible to reduce the sun glare exposure on a route by trading off additional travel distance. We envision future safety-aware navigation approaches that can automatically balance this trade-off and account for additional dynamic spatially and temporally-dependent safety-related environmental factors, like road and weather conditions, to determine the safest and most efficient route between any two given points.
In the field of mobility, the focus in the past few years has been on the proverbial last mile connectivity. However this paper, narrows the scope from "miles" to the "last 100 yards" presenting a unique sets of issues that are not seen at other levels. The last 100 yards encompass routing and connectivity issues within confined spaces such as houses, apartment building, office spaces and many others. Some of the challenges in this context include coordinating between traditional delivery services (e.g., Fedex, DHL or Amazon Prime) and specialized pilots authorized to operate within the human dominated spaces of the last 100 yards. Ensuring timely delivery of perishable items, addressing the risks of delivery theft and recipient accuracy, and managing the storage and redelivery of packages when recipient are not present further complicates the process. Despite the challenges, the last 100 yards also present opportunities for novel solutions based on automation, robotic routing, social modelling, and industrial planning. In these confined spaces fully automated robotic solutions become feasible as navigation speed and routes are limited and the area can be easily geofenced.
Spatial Pattern Matching is an important search problem that involves reasoning about the relative position, distance, and orientation of objects with respect to one another. Spatial relationships between objects contain a lot of information about the world, which makes them useful in applications like Point of Interest (POI) retrieval and location-based services. However, spatial pattern matching is an NP-hard problem in the worst case. This paper presents a theoretical comparison of spatial pattern matching approaches, showing how the prominent methods compare for each type of spatial relation they support. We further highlight the common techniques used to gain performance improvements and provide suggestions towards developing approximate solutions to this form of spatial search.
Given coarser-resolution projections from global climate models or satellite data, the downscaling problem aims to estimate finer-resolution regional climate data, capturing fine-scale spatial patterns and variability. Downscaling is any method to derive high-resolution data from low-resolution variables, often to provide more detailed and local predictions and analyses. This problem is societally crucial for effective adaptation, mitigation, and resilience against significant risks from climate change. The challenge arises from spatial heterogeneity and the need to recover finer-scale features while ensuring model generalization. Most downscaling methods [21] fail to capture the spatial dependencies at finer scales and underperform on real-world climate datasets, such as sea-level rise. We propose a novel Kriging-informed Conditional Diffusion Probabilistic Model (Ki-CDPM) to capture spatial variability while preserving fine-scale features. Experimental results on climate data show that our proposed method is more accurate than state-of-the-art downscaling techniques.
Each year, selective American colleges sort through tens of thousands of applications to identify a first-year class that displays both academic merit and diversity. In the 2023-2024 admissions cycle, these colleges faced unprecedented challenges to doing so. First, the number of applications has been steadily growing year-over-year. Second, test-optional policies that have remained in place since the COVID-19 pandemic limit access to key information that has historically been predictive of academic success. Most recently, longstanding debates over affirmative action culminated in the Supreme Court banning race-conscious admissions. Colleges have explored machine learning (ML) models to address the issues of scale and missing test scores, often via ranking algorithms intended to allow human reviewers to focus attention on ‘top’ applicants. However, the Court’s ruling will force changes to these models, which were previously able to consider race as a factor in ranking. There is currently a poor understanding of how these mandated changes will shape applicant ranking algorithms, and, by extension, admitted classes. We seek to address this by quantifying the impact of different admission policies on the applications prioritized for review. We show that removing race data from a previously developed applicant ranking algorithm reduces the diversity of the top-ranked pool of applicants without meaningfully increasing the academic merit of that pool. We further measure the impact of policy change on individuals by quantifying arbitrariness in applicant rank. We find that any given policy has a high degree of arbitrariness (i.e. at most 9% of applicants are consistently ranked in the top 20%), and that removing race data from the ranking algorithm increases arbitrariness in outcomes for most applicants.
This paper demonstrates Pyneapple-L, an open-source library designed to enhance scalable spatial analysis through learning-based techniques. Through collaboration with social scientists and domain experts, we identify scalability challenges inherent in conventional spatial analysis methods, particularly as the data size increases. Pyneapple-L addresses these challenges by leveraging learning-based models to offer scalable solutions. We demonstrate two modules: scalable learning of spatial hotspots along spatial networks and augmented geographically weighted regression. To showcase Pyneapple-L, we have developed a user-friendly frontend web application to interact with different datasets, algorithms, model configurations, and visualize outcomes on interactive maps that support both broad and analytical views.
Geo-obfuscation is a Location Privacy Protection Mechanism used in location-based services that allows users to report obfuscated locations instead of exact ones. A formal privacy criterion, geoindistinguishability (Geo-Ind), requires real locations to be hard to distinguish from nearby locations (by attackers) based on their obfuscated representations. However, Geo-Ind often fails to consider context, such as road networks and vehicle traffic conditions, making it less effective in protecting the location privacy of vehicles, of which the mobility are heavily influenced by these factors. In this paper, we introduce VehiTrack, a new threat model to demonstrate the vulnerability of Geo-Ind in protecting vehicle location privacy from context-aware inference attacks. Our experiments demonstrate that VehiTrack can accurately determine exact vehicle locations from obfuscated data, reducing average inference errors by 61.20% with Laplacian noise and 47.35% with linear programming (LP) compared to traditional Bayesian attacks. By using contextual data like road networks and traffic flow, VehiTrack effectively eliminates a significant number of seemingly "impossible" locations during its search for the actual location of the vehicles. Based on these insights, we propose TransProtect, a new geo-obfuscation approach that limits obfuscation to realistic vehicle movement patterns, complicating attackers' ability to differentiate obfuscated from actual locations. Our results show that TransProtect increases VehiTrack's inference error by 57.75% with Laplacian noise and 27.21% with LP, significantly enhancing protection against these attacks.
Understanding and predicting Origin-Destination (OD) flows is crucial for urban planning and transportation management. Traditional OD prediction models, while effective within single cities, often face limitations when applied across different cities due to varied traffic conditions, urban layouts, and socio-economic factors. In this paper, by employing Large Language Models (LLMs), we introduce a new method for cross-city OD flow prediction. Our approach leverages the advanced semantic understanding and contextual learning capabilities of LLMs to bridge the gap between cities with different characteristics, providing a robust and adaptable solution for accurate OD flow prediction that can be transferred from one city to another. Our novel framework involves four major components: collecting OD training datasets from a source city, instruction-tuning the LLMs, predicting destination POIs in a target city, and identifying the locations that best match the predicted destination POIs. We introduce a new loss function that integrates POI semantics and trip distance during training. By extracting high-quality semantic features from human mobility and POI data, the model understands spatial and functional relationships within urban spaces and captures interactions between individuals and various POIs. Extensive experimental results demonstrate the superiority of our approach over the state-of-the-art learning-based methods in cross-city OD flow prediction.
This study introduces a novel approach to terrain feature classification by incorporating spatial point pattern statistics into deep learning models. Inspired by the concept of location encoding, which aims to capture location characteristics to enhance GeoAI decision-making capabilities, we improve the GeoAI model by a knowledge driven approach to integrate both first-order and second-order effects of point patterns. This paper investigates how these spatial contexts impact the accuracy of terrain feature predictions. The results show that incorporating spatial point pattern statistics notably enhances model performance by leveraging different representations of spatial relationships.
Traditional statistical analyses do not reveal the spatial locations and the temporal occurrences of clusters of anomalous events that are responsible for a significant loss of sea ice extent. To address this problem, we present a novel method named Convolution Matrix Anomaly Detection (CMAD). The onset and progression of clusters of anomalous melting events over the Antarctic Sea ice are studied as loss in sea ice extent, which are essentially negative values, where the traditional convolutional operation of the Convolutional Neural Network (CNN) approach is ineffective. CMAD is based on an inverse max pooling concept in the convolutional operation of CNN to address this gap. CMAD is developed to offer a solution without using a neural network, and unlike a full CNN, it doesn't require any training or testing processes. Satellite images are utilized to establish the loss in the Antarctic region. Our analysis shows that anomalous melting patterns have significantly affected the Weddell and the Ross Sea regions more than any other regions of the Antarctic, consistent with the largest disappearance in sea ice extent over these two regions. These findings bolster the applicability of the inverse max pooling based CMAD in detecting the spatiotemporal evolution of clusters of anomalous melting events over the Antarctic region. The anomalous melting process was first noticed along the outer boundary of the sea ice extent in early September 2022 and gradually engulfed the entire sea ice region by February 2023 -in tandem with the scientific literature. These findings indicate that there is a necessity to delve deeper into the role of the anomalous melting process on sea ice retreat for a better understanding of the sea ice retreat process. The nature of the problem is to detect clusters of contiguous grids of anomalous melting events rather than detecting discrete grid points. CMAD's ability to perform both data clustering and anomaly detection via the pooling operations allows for a more comprehensive analysis of sea ice melt patterns, facilitating the pinpointing of areas with potentially significant melt events. This method has the potential to apply in other fields of study where anomalous events are detected in clusters. The inverse max pooling concept has successfully detected clusters of anomalous events in sea ice and demonstrated the capability to detect anomalies with 87% accuracy in benchmark data. In contrast to well-established conventional methods such as DBSCAN, HDBSCAN, K-Means, Bisecting K-Means, BIRCH, Agglomerative Clustering, OPTICS, and Gaussian Mixtures, when applied to dynamic multidimensional data, CMADBenchmark (which is a variation of CMAD) exhibits superior capabilities in detecting extreme events. The comparative analysis reveals that CMADBenchmark outperforms these traditional approaches, showcasing its heightened sensitivity and efficacy in capturing significant variations within evolving multidimensional datasets over time. This heightens the detection accuracy positions of CMAD as a valuable tool for discerning extreme events in the context of dynamic and changing multidimensional data.
The Antarctic sea ice cover plays a crucial role in regulating global climate and sea level rise. The recent retreat of the Antarctic Sea Ice Extent and the accelerated melting of ice sheets (which causes sea level rise) raise concerns about the impact of climate change. Understanding the spatial patterns of anomalous melting events in sea ice is crucial for improving climate models and predicting future sea level rise, as sea ice serves as a protective barrier for ice sheets. This paper proposes a two-module framework based on Deep Learning that utilizes satellite imagery to identify and predict non-anomalous and anomalous melting regions in Antarctic sea ice. The first module focuses on identifying non-anomalous and anomalous melting regions in the current day by analyzing the difference between consecutive satellite images over time. The second module then leverages the current day's information and predicts the next day's non-anomalous and anomalous melting regions. This approach aims to improve our ability to monitor and predict critical changes in the Antarctic sea ice cover.
This paper presents a novel approach for trajectory anomaly detection using an autoregressive causal-attention model, termed LM-TAD. This method leverages the similarities between language statements and trajectories, both of which consist of ordered elements requiring coherence through external rules and contextual variations. By treating trajectories as sequences of tokens, our model learns the probability distributions over trajectories, enabling the identification of anomalous locations with high precision. We incorporate user-specific tokens to account for individual behavior patterns, enhancing anomaly detection tailored to user context. Our experiments demonstrate the effectiveness of LM-TAD on both synthetic and real-world datasets. In particular, the model outperforms existing methods on the Pattern of Life (PoL) dataset by detecting user-contextual anomalies and achieves competitive results on the Porto taxi dataset, highlighting its adaptability and robustness. Additionally, we introduce the use of perplexity and surprisal rate metrics for detecting outliers and pinpointing specific anomalous locations within trajectories. The LM-TAD framework supports various trajectory representations, including GPS coordinates, staypoints, and activity types, proving its versatility in handling diverse trajectory data. Moreover, our approach is well-suited for online trajectory anomaly detection, significantly reducing computational latency by caching key-value states of the attention mechanism, thereby avoiding repeated computations. The code to reproduce experiments in this paper can be found at the following link: https://github.com/jonathankabala/LMTAD.
In urban science, understanding mobility patterns and analyzing how people move around cities helps improve the overall quality of life and supports the development of more livable, efficient, and sustainable urban areas. A challenging aspect of this work is the collection of mobility data through user tracking or travel surveys, given the associated privacy concerns, noncompliance, and high cost. This work proposes an innovative AI-based approach for synthesizing travel surveys by prompting large language models (LLMs), aiming to leverage their vast amount of relevant background knowledge and text generation capabilities. Our study evaluates the effectiveness of this approach across various U.S. metropolitan areas by comparing the results against existing survey data at different granularity levels. These levels include (i) pattern level, which compares aggregated metrics such as the average number of locations traveled and travel time, (ii) trip level, which focuses on comparing trips as whole units using transition probabilities, and (iii) activity chain level, which examines the sequence of locations visited by individuals. Our work covers several proprietary and open-source LLMs, revealing that open-source base models like Llama-2, when fine-tuned on even a limited amount of actual data, can generate synthetic data that closely mimics the actual travel survey data and, as such, provides an argument for using such data in mobility studies.
Ranking is a ubiquitous method for focusing the attention of human evaluators on a manageable subset of options. Its use as part of human decision-making processes ranges from surfacing potentially relevant products on an e-commerce site to prioritizing college applications for human review. While ranking can make human evaluation more effective by focusing attention on the most promising options, we argue that it can introduce unfairness if the uncertainty of the underlying relevance model differs between groups of options. Unfortunately, such disparity in uncertainty appears widespread, often to the detriment of minority groups for which relevance estimates can have higher uncertainty due to a lack of data or appropriate features. To address this fairness issue, we propose Equal-Opportunity Ranking (EOR) as a new fairness criterion for ranking and show that it corresponds to a group-wise fair lottery among the relevant options even in the presence of disparate uncertainty. EOR optimizes for an even cost burden on all groups, unlike the conventional Probability Ranking Principle, and is fundamentally different from existing notions of fairness in rankings, such as demographic parity and proportional Rooney rule constraints that are motivated by proportional representation relative to group size. To make EOR ranking practical, we present an efficient algorithm for computing it in time O(nlog (n)) and prove its close approximation guarantee to the globally optimal solution. In a comprehensive empirical evaluation on synthetic data, a US Census dataset, and a real-world audit of Amazon search queries, we find that the algorithm reliably guarantees EOR fairness while providing effective rankings.
Aerosol Optical Depth (AOD) is a crucial parameter for monitoring air quality, but satellite-based measurements often suffer from significant gaps due to cloud cover and other obstructions. These missing data, usually categorized as Missing Not At Random (MNAR), pose challenges for accurate air quality assessments. This study applies a Generative Adversarial Imputation Network (GAIN) to impute missing AOD data from the MODIS MAIAC dataset across the Northeast United States, addressing the MNAR challenge by leveraging relevant meteorological covariates, such as cloud cover, relative humidity, and temperature. The GAIN model was trained using data from 2021 to 2022, with hyperparameter tuning conducted to optimize performance. The tuning process revealed that a low learning rate and minimal weight decay yielded the most stable and accurate results. The model was validated against AERONET data, achieving a correlation coefficient (R) of 0.89, demonstrating strong alignment between imputed and observed AOD values. The GAIN model also demonstrated strong predictive accuracy, achieving an average R2 of 0.94, MSE of 0.0046, and RMSE of 0.0676. Cross-validation confirmed the robustness and generalizability of the model across various datasets. The model's performance was compared with traditional imputation methods like MICE and MissForest. GAIN outperformed both models, superiorly handling MNAR data and minimizing error across all metrics. This comparative analysis emphasizes the GAIN model's ability to capture complex spatial and temporal dependencies in the dataset effectively. In addition to filling data gaps, the GAIN model preserved the spatial distribution of AOD, showing higher concentrations in urban areas and regions with elevated pollution. During the 2023 Canadian wildfire event, the model successfully imputed AOD levels, capturing the sharp rise in aerosol concentrations. This study demonstrates the effectiveness of GAIN in handling complex MNAR scenarios, offering a reliable solution for improving AOD data coverage and enhancing the accuracy of air quality assessments.
Wastewater infrastructures are vital in urban cities, but aging sanitary sewer systems face issues like cracked pipes and damaged manholes, leading to infiltration and inflow problems. Climate change contributes to more frequent heavy precipitation, potentially causing significant sewer overflows. These overflows can endanger public health by carrying disease-causing microorganisms, pathogens, chemicals, and pollutants. To improve the maintenance of wastewater systems, which often face challenges of sparse sensor coverage in the field, we develop a graph dataset and a spatio-temporal hydraulic model - HydroNet - tailored to urban wastewater systems. The trained HydroNet is able to predict temporal water depth, which can help identify potential infiltration, leading to an automated urban wastewater system with minimal time and labor costs.
Abstract—The recent increase in attacks against publicly networked industrial control systems (ICS) has demonstrated a need for network-based anomaly detection systems, offering realtime flagging of potentially malicious activity by internal and external threat actors. Fuzzy hashing, also known as similarity hashing, has gained popularity in malware analysis and digital forensics circles as it provides analysts functionality to determine the similarity of two pieces of data by providing a similarity score. This work proposes a scheme that utilizes the similarity score to find variations from a self-establishing baseline in an ICS network to identify anomalous network traffic sections that could signify malicious activity.
The task of “relative placement” is to predict the placement of one object in relation to another, e.g. placing a mug onto a mug rack. Through explicit object-centric geometric reasoning, recent methods for relative placement have made tremendous progress towards data-efficient learning for robot manipulation while generalizing to unseen task variations. However, they have yet to represent deformable transformations, despite the ubiquity of non-rigid bodies in real world settings. As a first step towards bridging this gap, we propose “cross-displacement” - an extension of the principles of relative placement to geometric relationships between deformable objects - and present a novel vision-based method to learn cross-displacement through dense diffusion. To this end, we demonstrate our method’s ability to generalize to unseen object instances, out- of-distribution scene configurations, and multimodal goals on multiple highly deformable tasks (both in simulation and in the real world) beyond the scope of prior works. 
Visualizations such as bar charts, scatter plots, and objects on geographical maps often convey critical information, including exact and relative numeric values, using shapes. The choice of shape and method of encoding information is often selected arbitrarily, or decided based on convention or common practice. However, past studies have shown that the human eye can be fooled by visual representations. The Ebbinghaus illusion demonstrates that the perceived relative sizes of shapes depends on their configuration, which in turn can affect judgements, especially in visualizations like proportional symbol maps. In this study we evaluate the effects of varying the type of shapes and metrics for encoding geospatial data in visual representations on a spatio-temporal map interface. We find that some combinations of shape and metric are more conducive to accurate human judgements than others, and we provide recommendations for applying these findings in future spatial visualization designs.
As training datasets become increasingly drawn from unstructured, uncontrolled environments such as the web, researchers and industry practitioners have increasingly relied upon data filtering techniques to “filter out the noise” of web-scraped data. While datasets have been widely shown to reflect the biases and values of their creators, in this paper we contribute to an emerging body of research that assesses the filters used to create these datasets. We show that image-text data filtering also has biases and is value-laden, encoding specific notions of what is counted as “high-quality” data. In our work, we audit a standard approach of image-text CLIP-filtering on the academic benchmark DataComp’s CommonPool by analyzing discrepancies of filtering through various annotation techniques across multiple modalities of image, text, and website source. We find that data relating to several imputed demographic groups — such as LGBTQ+ people, older women, and younger men — are associated with higher rates of exclusion. We also find prevalence of Western bias, where the CLIP filter is more likely to include data related to Western countries compared to that of non-Western countries. Moreover, we demonstrate cases of exclusion amplification: not only are certain marginalized groups already underrepresented in the unfiltered data, but CLIP-filtering excludes data from these groups at higher rates. The data-filtering step in the machine learning pipeline can therefore exacerbate representation disparities already present in the data-gathering step, especially when existing filters are designed to optimize a specifically-chosen downstream performance metric like zero-shot image classification accuracy. Finally, we show that the NSFW filter fails to remove sexually-explicit content from CommonPool, and that CLIP-filtering includes several categories of copyrighted content at high rates. Our conclusions point to a need for fundamental changes in dataset creation and filtering practices. Content warning: This paper discusses societal stereotypes and sexually-explicit material that may be disturbing, distressing, and/or offensive to the reader.
Spatiotemporal data, which captures how variables evolve across space and time, is ubiquitous in fields such as environmental science, epidemiology, and urban planning. However, identifying causal relationships in these datasets is challenging due to the presence of spatial dependencies, temporal autocorrelation, and confounding factors. This tutorial provides a comprehensive introduction to spatiotemporal causal inference, offering both theoretical foundations and practical guidance for researchers and practitioners. We explore key concepts such as causal inference frameworks, the impact of confounding in spatiotemporal settings, and the challenges posed by spatial and temporal dependencies. The paper covers synthetic spatiotemporal benchmark data generation, widely used spatiotemporal causal inference techniques, including regression-based, propensity score-based, and deep learning-based methods, and demonstrates their application using synthetic datasets. Through step-by-step examples, readers will gain a clear understanding of how to address common challenges and apply causal inference techniques to spatiotemporal data. This tutorial serves as a valuable resource for those looking to improve the rigor and reliability of their causal analyses in spatiotemporal contexts.
Spatial reasoning is a particularly challenging task that requires inferring implicit information about objects based on their relative positions in space. In an effort to develop general purpose geo-foundation models that can perform a variety of spatial reasoning tasks, preliminary work has explored what kinds of world knowledge and spatial reasoning capabilities Large Language Models (LLMs) naturally inherit from their training data. Recent work suggests that LLMs contain geospatial knowledge in the form of understanding geo-coordinates and associating spatial meaning to the key terms "near" and "far." In this paper, we show that LLMs lack the ability to adapt the meaning of the words "near" and "far" to the appropriate scale when provided contextual reference points. By uncovering biases in how LLMs answer distance-related spatial questions, we set the groundwork for developing new techniques that may enable LLMs to perform accurate spatial reasoning.
The increased availability of datasets during the COVID-19 pandemic enabled machine-learning approaches for modeling and forecasting infectious diseases. However, such approaches are known to amplify the bias in the data they are trained on. Bias in such input data like clinical case data for COVID-19 is difficult to measure due to disparities in testing availability, reporting standards, and healthcare access among different populations and regions. Furthermore, the way such biases may propagate through the modeling pipeline to decision-making is relatively unknown. Therefore, we present a system that leverages a highly detailed agent-based model (ABM) of infectious disease spread in a city to simulate the collection of biased clinical case data where the bias is known. Our system allows users to load either a pre-selected region or select their own (using OpenStreetMap data for the environment and census data for the population), specify population and infectious disease parameters, and the degree(s) to which different populations will be overrepresented or underrepresented in the case data. In addition to the system, we provide a large number of benchmark datasets that produce case data at different levels of bias for different regions. Wehope that infectious disease modelers will use these datasets to investigate how well their models are robust to data bias or whether their model is overfit to biased data. 
Human mobility anomaly detection based on location is essential in areas such as public health, safety, welfare, and urban planning. Developing models and approaches for location-based anomaly detection requires a comprehensive dataset. However, privacy concerns and the absence of ground truth hinder the availability of publicly available datasets. With this paper, we provide extensive simulated human mobility datasets featuring various anomaly types created using an existing Urban Patterns of Life Simulation. To create these datasets, we inject changes in the logic of individual agents to change their behavior. Specifically, we create four of anomalous agent behavior by (1) changing the agents’ appetite (causing agents to have meals more frequently), (2) changing their group of interest (causing agents to interact with different agents from another group). (3) changing their social place selection (causing agents to visit different recreational places) and (4) changing their work schedule (causing agents to skip work), For each type of anomaly, we use three degrees of behavioral change to tune the difficulty of detecting the anomalous agents. To select agents to inject anomalous behavior into, we employ three methods: (1) Random selection using a centralized manipulation mechanism, (2) Spread based selection using an infectious disease model, and (3) through exposure of agents to a specific location. All datasets are split into normal and anomalous phases. The normal phase, which can be used for training models of normalcy, exhibits no anomalous behavior. The anomalous phase, which can be used for testing for anomalous detection algorithm, includes ground truth labels that indicate, for each five-minute simulation step, which agents are anomalous at that time. Datasets are generated using the maps (roads and buildings) for Atlanta and Berlin having 1k agents in each simulation. All datasets are openly available at https://osf.io/dg6t3/. Additionally, we provide instructions to regenerate the data for other locations and numbers of agents.
Identifying anomalous human spatial trajectory patterns can indicate dynamic changes in mobility behavior with applications in domains like infectious disease monitoring and elderly care. Recent advancements in large language models (LLMs) have demonstrated their ability to reason in a manner akin to humans. This presents significant potential for analyzing temporal patterns in human mobility. In this paper, we conduct empirical studies to assess the capabilities of leading LLMs like GPT-4 and Claude-2 in detecting anomalous behaviors from mobility data, by comparing to specialized methods. Our key findings demonstrate that LLMs can attain reasonable anomaly detection performance even without any specific cues. In addition, providing contextual clues about potential irregularities could further enhances their prediction efficacy. Moreover, LLMs can provide reasonable explanations for their judgments, thereby improving transparency. Our work provides insights on the strengths and limitations of LLMs for human spatial trajectory analysis. 
Semantic trajectories, which enrich spatial-temporal data with textual information such as trip purposes or location activities, are key for identifying outlier behaviors critical to healthcare, social security, and urban planning. Traditional outlier detection relies on heuristic rules, which requires domain knowledge and limits its ability to identify unseen outliers. Besides, there lacks a comprehensive approach that can jointly consider multi-modal data across spatial, temporal, and textual dimensions. Addressing the need for a domain-agnostic model, we propose the Transferable Outlier Detection for Human Semantic Trajectories (TOD4Traj) framework. TOD4Traj first introduces a modality feature unification module to align diverse data feature representations, enabling the integration of multi-modal information and enhancing transferability across different datasets. A contrastive learning module is further proposed for identifying regular mobility patterns both temporally and across populations, allowing for a joint detection of outliers based on individual consistency and group majority patterns. Our experimental results have shown TOD4Traj's superior performance over existing models, demonstrating its effectiveness and adaptability in detecting human trajectory outliers across various datasets.
Analyzing individual human trajectory data helps our understanding of human mobility and finds many commercial and academic applications. There are two main approaches to accessing trajectory data for research: one involves using real-world datasets like GeoLife, while the other employs simulations to synthesize data. Real-world data provides insights from real human activities, but such data is generally sparse due to voluntary participation. Conversely, simulated data can be more comprehensive but may capture unrealistic human behavior. In this Data and Resource paper, we combine the benefit of both by leveraging the statistical features of real-world data and the comprehensiveness of simulated data. Specifically, we extract features from the real-world GeoLife dataset such as the average number of individual daily trips, average radius of gyration, and maximum and minimum trip distances. We calibrate the Pattern of Life Simulation, a realistic simulation of human mobility, to reproduce these features. Therefore, we use a genetic algorithm to calibrate the parameters of the simulation to mimic the GeoLife features. For this calibration, we simulated numerous random simulation settings, measured the similarity of generated trajectories to GeoLife, and iteratively (over many generations) combined parameter settings of trajectory datasets most similar to GeoLife. Using the calibrated simulation, we simulate large trajectory datasets that we call GeoLife+, where + denotes the Kleene Plus, indicating unlimited replication with at least one occurrence. We provide simulated GeoLife+ data with 182, 1k, and 5k over 5 years, 10k, and 50k over a year and 100k users over 6 months of simulation lifetime. 
Transformer-based models are popular for time series forecasting and spatiotemporal prediction due to their ability to infer semantic correlations in long sequences. However, for human mobility prediction, temporal correlations, such as location patterns at the same time on previous days or weeks, are essential. While positional encodings help retain order, the self-attention mechanism causes a loss of temporal detail. To validate this claim, we used a simple approach in the 2nd ACM SIGSPATIAL Human Mobility Prediction Challenge, predicting locations based on past patterns weighted by reliability scores for missing data. Our simple approach was among the top 10 competitors and significantly outperformed the Transformer-based model that won the 2023 challenge. 
In this paper, we use the Fisher information matrix (FIM) to analyze the interaction between low-earth orbit (LEO) satellites and 5G base stations in providing 9D receiver localization and correcting LEO ephemeris. First, we give a channel model that captures all the information in the LEO-receiver, LEO-BS, and BS-receiver links. Subsequently, we use FIM to capture the amount of information about the channel parameters in these links. Then, we transform these FIM for channel parameters to the FIM for the 9D (3D position, 3D orientation, and 3D velocity estimation) receiver localization parameters and the LEO position and velocity offset. Closed-form expressions for the entries in the FIM for these location parameters are presented. Our results on identifiability utilizing the FIM for the location parameters indicate: i) with one LEO, we need three BSs and three time slots to both estimate the 9D location parameters and correct the LEO position and velocity, ii) with two LEO, we need three BSs and three time slots to both estimate the 9D location parameters and correct the LEO position and velocity, and iii) with three LEO, we need three BSs and four-time slots to both estimate the 9D location parameters and correct the LEO position and velocity. We notice from the Cramer Rao lower bound that the operating frequency and number of receive antennas have negligible impact on the estimation accuracy of the orientation of the receiver and the LEO velocity, respectively.
This paper explores the benefits of sensor-based line-of-sight/non-line-of-sight (LOS/NLOS) information on the detection performance of an on-off keying (OOK) communication link. Bayes risk and composite likelihood ratio test (LRT) methods are used to derive the optimal decision rule for minimizing probability of error in a dynamic LOS/NLOS channel. By exploring three varying degrees of knowledge, it is shown that one can achieve improved constant-false-alarm-rate (CFAR) detection performance in an ensemble of trials over the uniformly-most-powerful (UMP) test when labeled information about LOS is provided at the receiver. It is also shown that, in the presence of prior knowledge of signal presence, one can benefit from LOS statistics when minimizing probability of error.
Many modern navigation scenarios involve autonomous agents navigating in an environment with a priori information of landmarks and obstacles. In such applications, agents make important navigation related decisions for avoiding obstacles. The agents often self-localize using Time-Of-Flight (TOF) measurements from known anchors by continuous sensor-polling. This polling uses system resources such as wireless spectrum and energy and a more opportunistic polling strategy would conserve these resources. For a previously obtained measurement, as the time elapses, the associated position information with this measurement ages. This is because the agent is continuously moving. Clearly, the nature of change of position information depends on the nature of the agent’s motion. In this work, we investigate the time dependence of position information as previously obtained measurements age. We discuss two general motion models - linear and circular, associated with the agent’s motion. Using the Cramer-Rao lower bound (CRLB), we analyze the effect of motion models and their influence on information content of aged sensor measurements. In particular for map based navigation systems, viewing an agent’s trajectory as a combination of linear and circular motion between waypoints, we analyze the variation of position error bound (PEB) with age. Finally, we present our insights about opportunistic sensor polling to a mab based navigation scenario.
Since the terahertz frequency band (0.1–1 THz) has attracted considerable attention for the upcoming sixth-generation (6G) wireless communication systems, accurate models for multipath propagation in this frequency range need to be established. Such models advantageously use the fact that multi-path components (MPCs) occur typically in clusters, i.e., groups of MPCs that have similar delays and angles. In this paper, we first analyze the limitations of a widely used clustering algorithm, Kernel-Power-Density (KPD), in evaluating an extensive THz outdoor measurement campaign at 145–146 GHz, particularly its inability to detect small clusters. We introduce a modified version, which we term multi-level KPD (ML-KPD), iteratively applying KPD to detect whether a cluster determined in the previous round is made up of multiple clusters. We first apply the method to synthetic channels to demonstrate its efficacy and select suitable values for the adaptive hyperparameters. Then, multi-level KPD is applied to our channel measurements in line-of-sight (LOS) and non-line-of-sight (NLOS) environments to determine statistics for the number of clusters and the cluster spreads. 
As the complexity and connectivity of networks increase, the need for novel malware detection approaches becomes imperative. Traditional security defenses are becoming less effective against the advanced tactics of today’s cyberattacks. Deep Packet Inspection (DPI) has emerged as a key technology in strengthening network security, offering detailed analysis of network traffic that goes beyond simple metadata analysis. DPI examines not only the packet headers but also the payload content within, offering a thorough insight into the data traversing the network. This study proposes a novel approach that leverages a large language model (LLM) and few-shot learning to accurately recognizes novel, unseen malware types with few labels samples. Our proposed approach uses a pretrained LLM on known malware types to extract the embeddings from packets. The embeddings are then used alongside few labeled samples of an unseen malware type. This technique is designed to acclimate the model to different malware representations, further enabling it to generate robust embeddings for each trained and unseen classes. Following the extraction of embeddings from the LLM, few-shot learning is utilized to enhance performance with minimal labeled data. Our evaluation, which utilized two renowned datasets, focused on identifying malware types within network traffic and Internet of Things (IoT) environments. Our approach shows promising results with an average accuracy of 86.35% and F1-Score of 86.40% on different malware types across the two datasets.
Federated learning is a promising privacy-preserving learning paradigm in which multiple clients can collaboratively learn a model with their image data kept local. For protecting data ownership, personalized watermarks are usually added to the image data by each client. However, the introduced watermarks can lead to a shortcut learning problem, where the learned model performs predictions over-rely on the simple watermark-related features and represents a low accuracy on real-world data. Existing works assume the central server can directly access the predefined shortcut features during the training process. However, these may fail in the federated learning setting as the shortcut features of the heterogeneous watermarked data are difficult to obtain. In this paper, we propose a federated Morozov regularization technique, where the regularization parameter can be adaptively determined based on the watermark knowledge of all the clients in a privacy-preserving way, to eliminate the shortcut learning problem caused by the watermarked data. Specifically, federated Morozov regularization firstly performs lightweight local watermark mask estimation in each client to obtain the locations and intensities knowledge of local watermarks. Then, it aggregates the estimated local watermark masks to generate the global watermark knowledge with a weighted averaging. Finally, federated Morozov regularization determines the regularization parameter for each client by combining the local and global watermark knowledge. With the regularization parameter determined, the model is trained as normal federated learning. We implement and evaluate federated Morozov regularization based on a real-world deployment of federated learning on 40 Jetson devices with real-world datasets. The results show that federated Morozov regularization improves model accuracy by 11.22% compared to existing baselines.
Deep Neural Networks (DNNs) are an attractive solution to address several problems in Radio Frequency Machine Learning Systems (RFMLS). The key blocker that inhibits the widespread deployment of DNNs in real-world tactical scenarios is the performance degradation experienced under dynamic channel conditions. Test Time Adaptation (TTA) presents a promising solution to mitigate this issue by dynamically updating the DNN to adapt to the current channel conditions in an unsupervised manner. Although it offers superior performance and practical benefits, TTA introduces new security concerns and vulnerabilities, potentially exposing sensitive deployments to Adversarial Machine Learning (AML) activity. In this work, we introduce a novel attack strategy named Adversarial Dynamic Adaptation (ADA) that leverages the inherent vulnerabilities in TTA to compromise RFMLS tasks. We demonstrate that even under realistic assumptions and while perturbing only 20% of the samples in a test data batch, ADA degrades the performance of the unperturbed data by up to 20.3% compared to similar attacks designed for computer vision tasks. By assessing the robustness against the latest TTA methods, ADA serves as a valuable tool to identify and understand the security risks associated with adapting DNNs at the test time in mission-critical and sensitive deployments.
With continued advancements in the cellular domain, the number of network devices is expanding rapidly. Furthermore, as devices become smarter and more advanced with the deployment of dynamic spectrum awareness techniques that facilitate increased spectral efficiency, developing robust methodologies to analyze limited spectrum resources will be crucial to the success of FutureG wireless communications. Central to the idea of dynamic spectrum awareness is the need for robust techniques and metrics that provide insight into the spectral utilization and can be harnessed by dynamic control loops to maximize spectral usage and eliminate wasteful inefficiencies. While a plethora of dynamic spectrum awareness tactics have been proposed, existing strategies do not analyze the power spectral density probability distribution across time, which we demonstrate is a relevant indicator of spectral usage and signal quality. Specifically, we harness the commercial-grade, standalone, 5G network architecture located at the Air Force Research Laboratory (AFRL) to perform an experimental measurement study and show the effectiveness of representing the localized spectral utilization as a distribution of power correlations w.r.t. the bandwidth’s frequency bins. We analyze the distributional structure of power spectral density correlations for a variety of wireless channel environments (i.e., LOS, NLOS, and Edge scenarios) and locations, and our key finding is that the shape of the frequency-bin correlation distribution changes significantly depending upon the particular wireless channel situation. As such, our approach - which is not only blind (i.e., it doesn’t require knowledge about the signal prior to sensing the spectrum), but also highly practical from an implementation standpoint - provides fresh insight into the spectral utilization, signal structure, and channel quality conditions.
The importance of secure wireless communication is increasing as adversaries’ eavesdropping capabilities become more advanced. In this paper, we propose a novel steganography method that utilizes error pattern embedding to minimize the likelihood of detection by eavesdroppers. Unlike existing error pattern embedding steganography, we introduce a secret codebook generation algorithm designed to maximize the secret codebook size. Our algorithm is applicable to any coding scheme that possesses a predefined maximum number of correctable errors. In addition, we propose a novel steganalysis scheme for error pattern embedding steganography. Our method is based on comparing two distinct empirical relative entropies: one derived from the empirical probability mass function (pmf) of observed transmitted signals and the other from the empirical pmf of randomly generated signals following a Bernoulli $\left( {\frac{1}{2}} \right)$ distribution. Simulation results indicate that our algorithm enhances security by effectively reducing the detection probability by the eavesdropper while simultaneously increasing the capacity for secret information.
The advent of 5G technology introduces significant advancements in speed, latency, and device connectivity, but also poses complex security challenges. Among typical denial-of-service (DoS) attacks, jamming attack can severely degrade network performance by interfering critical communication channels. To address this issue, we propose a novel security solution utilizing multipath communication, which distributes message segments across multiple paths to ensure message recovery even when some paths are compromised. This strategy enhances network resilience and aligns with zero-trust architecture principles. Moreover, the proposed scheme has been implemented in our test bed to validate the concept and assess the network performance under jamming attacks. Our findings demonstrate that this method eliminates the negative impacts caused by DoS attacks, maintaining the integrity and availability of critical network services. The results highlight the robustness of multipath communication in securing 5G networks against sophisticated attacks, thereby safeguarding essential communications in dynamic and potentially hostile environments.
The shift to cloud-native architectures in the 5G control plane introduces significant challenges in securing complex network functions deployed as microservices. Traditional security mechanisms, such as service mesh architectures and kernel-level solutions, often suffer from protocol limitations, high overhead, or incomplete inter-service encryption. Therefore, we present Zero Trust X Security Enforcement Microservice (ZTX-SEM), a protocol-agnostic zero-trust security solution designed specifically for cloud-native deployments. We propose a lightweight packet interception mechanism that operates across all protocols, a proactive authentication strategy that reduces latency and ensures continuous readiness, and an optimized secret key lookup that accelerates encrypted communication processing. Our experiments on a Kubernetes-deployed 5G control plane (core) demonstrate that ZTX-SEM outperforms existing solutions such as Istio, achieving 75% and 28% reductions in resource utilization and session setup times, respectively.
We propose an interactive and intelligent hybrid teleconferencing system compatible with Virtual Reality devices. Our system understands meeting contexts and leverages user interactions to enhance better system configuration. Employing interactive scene graphs [11], the system extracts and transmits essential meeting context to users while relaying user interactions back to the streaming systems for user-involved adaptive streaming and foveated rendering. We demonstrate the system's real-time performance and compatibility with commercial VR devices such as the Meta Quest 3.
The rapidly advancing cyber-physical domain of synthetic biology modifies the functionality of micro-organisms which can act as living computational devices. Applications include intelligent drug delivery, customized cancer therapies, and pollution detection and mitigation. While many synthetic biology applications have been proposed, prototyped, and even deployed, these systems often lack standard approaches to verify their safety and security. One approach, the assurance case, provides evidence demonstrating proper implementation in the target application, and is often used in other safety-critical domains. However, synthetic biologists lack guidance in developing assurance arguments and tracing safety and security requirements to evidence as required for building assurance cases. Although there has been some research combining safety and security artifacts, such techniques often require extensive expertise from different domains and may not be accessible to synthetic biologists. In this paper we propose SynBioTrace, an assistive process to help propagate information from risk-based analyses of such systems to preliminary assurance cases. SynBioTrace preserves traceability among its steps so that the assurance case can be further refined. We apply and evaluate it through a case study based on a real-world synthetic biology application. Our case study suggests this approach could aid synthetic biologists in identifying, documenting, and structuring safety and security artifacts, as well as linking evidence to support traceability for a complete, integrated assurance case.
Radio Frequency Machine Learning Systems (RFMLS) have attracted increasing interest over the past few years. However, it has been demonstrated that RFMLS are vulnerable to Adversarial Machine Learning (AML). While AML has been extensively investigated in traditional domains, current state of the art often compromises the performance on benign data or introduces excessive computational overhead. As such, it cannot meet the strict requirements of tactical RFMLS. In this paper, we propose a novel defense approach based on dynamic adaptation of Deep Neural Network (DNN). Specifically, we leverage a hypernetwork to dynamically generate diverse parameters for a target DNN during inference. In addition, an ensemble learning and multi-stage training framework is proposed to train such a hypernetwork. Experimental results show that the proposed defense can increase the accuracy on adversarial examples by 48% and 16% in comparison to naturally trained DNN and defensive training strategies, respectively.
We study algorithms for drawing planar graphs and 1-planar graphs using cubic Bézier curves with bounded curvature. We show that any n-vertex 1-planar graph has a 1-planar RAC drawing using a single cubic Bézier curve per edge, and this drawing can be computed in O(n) time given a combinatorial 1-planar drawing. We also show that any n-vertex planar graph G can be drawn in O(n) time with a single cubic Bézier curve per edge, in an O(n)× O(n) bounding box, such that the edges have Θ(1/degree(v)) angular resolution, for each v ∈ G, and O(√n) curvature. 
Swarical, a Swarm-based hierarchical localization technique, enables miniature drones, Flying Light Specks (FLSs), to accurately and efficiently localize and illuminate complex 2D and 3D shapes. Its accuracy depends on the physical hardware (sensors) of FLSs used to track neighboring FLSs to localize themselves. It uses the specification of the sensors to convert mesh files into point clouds that enable a swarm of FLSs to localize at the highest accuracy afforded by their sensors. Swarical considers a heterogeneous mix of FLSs with different orientations for their tracking sensors, ensuring a line of sight between a localizing FLS and its anchor FLS. We present an implementation using Raspberry cameras and ArUco markers. A comparison of Swarical with a state of the art decentralized localization technique shows that it is as accurate and more than 2x faster. 