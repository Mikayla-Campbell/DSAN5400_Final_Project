Cold start delays are a main pain point for today’s FaaS (Function-as-a-Service) platforms. A widely used mitigation strategy is keeping recently invoked function containers alive in memory to enable warm starts with minimal overhead. This paper identifies new challenges that state-of-the-art FaaS keep-alive policies neglect. These challenges are caused by concurrent function invocations, a common FaaS workload behavior. First, concurrent requests present a trade-off between reusing busy containers (delayed warm starts) versus cold-starting containers. Second, concurrent requests cause imbalanced evictions of containers that will be reused shortly thereafter. To tackle the challenges, we propose a novel serverless function container orchestration algorithm called CIDRE. CIDRE makes informed decisions to speculatively choose between a delayed warm start and a cold start under concurrency-driven function scaling. CIDRE uses both fine-grained container-level and coarse-grained concurrency information to make balanced eviction decisions. We evaluate CIDRE extensively using two production FaaS workloads. Results show that CIDRE reduces the cold start ratio and the average invocation overhead by up to 75.1% and 39.3% compared to state-of-the-art function keep-alive policies.
3D Gaussian Splatting (3DGS) is an emerging approach for training and representing real-world 3D scenes. Due to its photorealistic novel view synthesis and fast rendering speed (e.g., over 100 FPS), it has the potential to transform how scenes that can be explored in 6 degrees-of-freedom (6-DoF) are represented. However, a limiting factor of 3DGS is its large size, which requires high network bandwidth for streaming reconstructed real-world 3D scenes. In this paper, we propose SGSS for optimizing the streaming transmission of 3DGS scenes during 6-DoF navigation. Since not all Gaussians in the full scene are needed for rendering a user's view, SGSS uses view-adaptive streaming, enabled by optimized spatial partitioning of the scene, for achieving network transmission savings. For each spatial partition, SGSS uses an importance-based Gaussians pre-sorting scheme to enhance the initial view quality and reduce the user-perceived scene loading time. We further design a client-side view-adaptive streaming algorithm that features lightweight visibility checking, prioritized streaming, incremental processing, and stream pausing/resuming schemes. We implement SGSS with JavaScript and WebGL2. Evaluation results show that the quality of views rendered with SGSS streaming is consistently higher than or on par with state-of-the-art approaches. Furthermore, the view-adaptive streaming of SGSS can result in high savings in network transmission without impacting the view quality.
Single-Sparse-Matrix Kernels (SSMKs) such as SpMM, SDDMM, SpMV, and SpTS form the backbone of applications such as data analytics, graph processing, finite-element analysis, machine learning (including GNNs and LLMs), etc. This paper introduces Residue-based Acceleration of Single Sparse Matrix Computation via Adaptive Tiling (RASSM), an input-dependent, adaptive 2-dimensional tiling technique for SSMKs. The adaptation leverages the concept of a residue matrix: a data structure that compactly captures the pattern of non-zeros in the sparse matrix. With residues, we show it is possible to make intelligent decisions on adaptive tile sizes, resulting in increased cache occupancy. Residues allow for optimizations across both spatial and temporal locality. RASSM improves data movement and overall performance as compared to prior techniques. For example, using spatial analysis for SpMM on commodity server CPUs, RASSM has 1.30X speedup over MKL, 1.32X over J-Stream, 1.20X over ASpT, 1.11X over CSF-4 uniform-shape, and 1.10X over CSF-4 uniform-occupancy. RASSM with temporalanalysis improves this to 1.36X (vs. MKL), 1.38X (vs. J-Stream), 1.26X (vs. ASpT), 1.17X (vs. CSF-4 uniform-shape), and 1.16X (vs. CSF-4 uniform-occupancy).
We study the problem of agnostic PAC reinforcement learning (RL): given a policy class Pi, how many rounds of interaction with an unknown MDP (with a potentially large state and action space) are required to learn an epsilon-suboptimal policy with respect to Pi? Towards that end, we introduce a new complexity measure, called the spanning capacity, that depends solely on the set Pi and is independent of the MDP dynamics. With a generative model, we show that the spanning capacity characterizes PAC learnability for every policy class Pi. However, for online RL, the situation is more subtle. We show there exists a policy class Pi with a bounded spanning capacity that requires a superpolynomial number of samples to learn. This reveals a surprising separation for agnostic learnability between generative access and online access models (as well as between deterministic/stochastic MDPs under online access). On the positive side, we identify an additional sunflower structure which in conjunction with bounded spanning capacity enables statistically efficient online RL via a new algorithm called POPLER, which takes inspiration from classical importance sampling methods as well as recent developments for reachable-state identification and policy evaluation in reward-free exploration.
Optimizing quantum circuits is critical: the number of quantum operations needs to be minimized for a successful evaluation of a circuit on a quantum processor. In this paper we unify two disparate ideas for optimizing quantum circuits, rewrite rules, which are fast standard optimizer passes, and unitary synthesis, which is slow, requiring a search through the space of circuits. We present a clean, unifying framework for thinking of rewriting and resynthesis as abstract circuit transformations. We then present a radically simple algorithm, guoq, for optimizing quantum circuits that exploits the synergies of rewriting and resynthesis. Our extensive evaluation demonstrates the ability of guoq to strongly outperform existing optimizers on a wide range of benchmarks.
New low-precision accelerators, vector instruction sets, and library functions make maximizing accuracy and performance of numerical code increasingly challenging. Two lines of work---traditional compilers and numerical compilers---attack this problem from opposite directions. Traditional compiler backends optimize for specific target environments but are limited in their ability to balance performance and accuracy. Numerical compilers trade off accuracy and performance, or even improve both, but ignore the target environment. We join aspects of both to produce Chassis, a target-aware numerical compiler. Chassis compiles mathematical expressions to operators from a target description, which lists the real expressions each operator approximates and estimates its cost and accuracy. Chassis then uses an iterative improvement loop to optimize for speed and accuracy. Specifically, a new instruction selection modulo equivalence algorithm efficiently searches for faster target-specific programs, while a new cost-opportunity heuristic supports iterative improvement. We demonstrate Chassis capabilities on 9 different targets, including hardware ISAs, math libraries, and programming languages. Chassis finds better accuracy and performance trade-offs than both Clang (by 3.5×) or Herbie (by up to 2.0×) by leveraging low-precision accelerators, accuracy-optimized numerical helper functions, and library subcomponents.
The protein scaffold filling problem remains a significant challenge in computational proteomics, which is critical for accurate protein function prediction and drug design. Despite recent advancements, current sequencing methods often yield incomplete protein sequences, referred to as scaffolds, which require precise filling for further analysis. This paper presents a web-based application, implemented using the Django framework, adopting our previously developed machine learning and deep learning techniques for protein scaffold filling. The platform allows users to try our pre-trained models or train models on their datasets for new scaffolds. This system provides a versatile tool for researchers in computational proteomics, enhancing the efficiency of protein sequence prediction. The developed web application can be accessed through https://psf.ncat.edu/.
Inverter-based resources (IBRs) exhibit distinct short-circuit characteristics that challenge traditional protective relays designed for systems dominated by synchronous generators. While research often focuses on IBRs’ positive-sequence currents during faults, their zero- and negative-sequence responses under unsymmetrical faults remain underexplored. Factors such as transformer configurations and grounding methods further complicate the design of protection schemes relying on these sequence components. This paper enhances the understanding of IBR short-circuit behavior during both symmetrical and unsymmetrical faults and investigates the impact of various transformer configurations on these behaviors. We highlight the limitations of traditional protective relays in safeguarding IBRs due to their constrained fault current levels, minimal negative-sequence components, and, in many cases, the absence of zero-sequence currents. To address these challenges, a novel incremental focused directional protection scheme is introduced. This approach offers enhanced fault detection capabilities under the complex conditions posed by high renewable energy penetration and diverse transformer configurations. The proposed method provides a robust solution for ensuring reliable protection in modern power systems with high integration of IBRs, contributing to improved grid stability and resilience. 
Federated learning (FL) enables multiple parties to collaboratively train machine learning models while preserving data privacy. However, securing communication within FL frameworks remains a significant challenge due to potential vulnerabilities to data breaches and integrity attacks. This paper proposes a novel approach using Dilithium, a robust digital signature framework, to enhance data security in FL. By integrating Dilithium into FL protocols, this study demonstrates robust communication security, preventing data tampering and unauthorized access, thereby promoting safer and more efficient collaborative model training across distributed networks. Furthermore, our approach incorporates an optimized client selection algorithm and a parallelized GPU-based training process that reduces latency and ensures seamless synchronization among participants. Experimental results demonstrate that our system achieves a total processing time of 6.891 seconds, significantly outperforming the 10.24 seconds of normal FL and 12.32 seconds of FL-Dilithium systems on the same computing platforms. Additionally, the proposed model achieves an accuracy of 94%, surpassing the 93% of the normal FL. 
Economic constraints on recruiting experts hinder efforts to build qualified datasets for utilizing AI in professional domains (e.g., medical diagnosis), which could provide societal benefits. To solve this issue, previous studies introduced crowdsourcing and AI to enable non-experts to perform expert-level data labeling. Yet, they encountered three challenges: 1) the limited applicability of crowdsourcing in less specialized domains (e.g., identifying animal species); 2) the chicken-and-egg problem, a paradox where high-performance AI is required to build a dataset to train such AI; and 3) over-reliance on AI, where non-experts, lacking expertise, may incorrectly label data when guided by sub-optimal AI. To address this, we introduce DANNY (Data ANnotation for Non-experts made easY), an AI-based tool designed to help non-experts label an arthritis dataset, aiming to increase labeling accuracy and mitigate over-reliance on AI. By externalizing a cognitive forcing intervention to foster critical thinking, DANNY provides two visualizations: 1) the Criteria phase, where non-experts define criteria across four arthritis features, and 2) the Correction phase, where they refine these criteria by comparing them to AI suggestions. In a study with 28 participants, DANNY users achieved higher accuracy and a more appropriate reliance on AI dependency than control groups. A follow-up study with 12 participants demonstrates how DANNY can be used to improve AI with an ensemble method. Our findings contribute new insights into using AI to support non-experts in labeling domain-specific data when expert resources are limited.
The rise of end-user applications powered by large language models (LLMs), including both conversational interfaces and add-ons to existing graphical user interfaces (GUIs), introduces new privacy challenges. However, many users remain unaware of the risks. This paper explores methods to increase user awareness of privacy risks associated with LLMs in end-user applications. We conducted five co-design workshops to uncover user privacy concerns and their demand for contextual privacy information within LLMs. Based on these insights, we developed CLEAR (Contextual LLM-Empowered Privacy Policy Analysis and Risk Generation), a just-in-time contextual assistant designed to help users identify sensitive information, summarize relevant privacy policies, and highlight potential risks when sharing information with LLMs. We evaluated the usability and usefulness of CLEAR across two example domains: ChatGPT and the Gemini plugin in Gmail. Our findings demonstrated that CLEAR is easy to use and improves users’ understanding of data practices and privacy risks. We also discussed LLM’s duality in posing and mitigating privacy risks, offering design and policy implications.
Recently, several nanostructures with subwavelength scale dimensions were demonstrated to efficiently enhance light chirality. However, the intrinsic lossy nature of metals and inherent narrowband response of dielectric nanostructures pose severe limitations toward the practical realization of chiroptical systems. In our talk, we demonstrate a new way to tackle these problems by designing and experimentally realizing all-dielectric silicon-based L-shaped optical metamaterials based on tilted nanopillars that exhibit broadband and enhanced chirality in transmission operation. We use an emerging bottom-up fabrication approach, named glancing angle deposition, to assemble these new dielectric metamaterials on a wafer scale. We experimentally measure the strong and tunable chiral responses and theoretically explain our findings. The reported strong chirality can be tailored in terms of both amplitude and operating frequency. It may also be totally switched by simply varying the shape and dimensions of the nanopillars.
Needle-based interventions, which number over 30 million annually in the US, suffer from a high complication rate of up to 33% due to inadequate visual guidance. The development of a novel imaging platform integrating Optical Coherent Tomography (OCT) within a minimally invasive probe aims to enhance safety and efficiency. This system, utilizing advanced OCT and machine-learning algorithms, provides real-time, high-resolution imaging to assist in procedures like biopsies and epidural placements, demonstrating potential to reduce complications and procedure times significantly.
Liver transplantation for severe hepatic diseases faces a critical shortage of donor livers globally. Utilizing marginal donor livers could alleviate this issue, yet current biopsy-based assessments are limited in evaluating their viability comprehensively. We propose employing polarization-sensitive optical coherence tomography (PS-OCT) to noninvasively scan multiple regions of donor livers, providing detailed microstructural and tissue property evaluations. Our approach integrates texture feature extraction and machine learning to correlate PS-OCT findings with pathological assessments, demonstrating its potential to enhance pre-transplantation viability evaluations. PS-OCT offers a promising tool for transplant clinics, offering precise, noninvasive insights into liver tissue quality across entire donor organs.
GaN has recently been shown to host bright, photostable, defect single-photon emitters in the 600–700 nm wavelength range that are promising for quantum applications. Our studies have revealed the optical dipole structure, mechanisms associated with optical dipole dephasing, and the spin structure of these emitters. We have also discovered optically detected magnetic resonance (ODMR) in two distinct species of defects. In one group, we found negative optically detected magnetic resonance of a few percent associated with a metastable electronic state, whereas in the other, we found positive optically detected magnetic resonance of up to 30% associated with the ground and optically excited electronic states. We also established coherent control over a single defect’s ground-state spin. In this talk, we will present our results on the basic physics of these defects and also discuss the spin physics associated with the observed ODMR.
Pre-silicon tools for hardening hardware against side-channel and fault injection attacks have become popular recently. However, the security of the system is still threatened by sophisticated physical attacks, which exploit the physical layer characteristics of the computing system beyond the integrated circuits (ICs) and, therefore, bypass the conventional countermeasures. Further, environmental conditions for the hardware can also impact side-channel leakage and fault vulnerability in unexpected ways that are challenging to model in pre-silicon. Thus, attacks cannot be addressed solely by conventional countermeasures at higher layers of the compute stack due to the lack of awareness about the events occurring at the physical layer during runtime. In this paper, we first discuss why the current pre-silicon security and verification tools might fail to achieve security against physical threats in the post-silicon phase. Afterward, we provide insights from the fields of power/signal integrity (PI/SI), and failure analysis (FA) to understand the fundamental issue with the failed current practices. We argue that hardware-based moving target defenses (MTDs) to randomize the physical fabric’s characteristics of the system can mitigate such unaccounted post-silicon threats. We show the effectiveness of such an approach by presenting the results of two case studies in which we perform powerful attacks, such as impedance analysis and laser voltage probing. Finally, we review the overhead of our proposed approach and show that the imposed overhead by MTD solutions can be addressed by making them active only when a threat is detected.
In contrast to uniform distribution in power wires, actual currents tend to exhibit complicated crowding phenomena at the connections between Through-silicon-via (TSV) and power wires. The current crowding effect degrades power integrity and increases the difficulty of 3D IC power delivery network (PDN) analysis. Therefore, a detailed analysis of current distribution and IR drops in power TSVs within 3D IC PDN is important. This paper will explore the complicated current behavior within TSVs and PDNs of the promising face-to-face 3D IC architecture. Since existing simulation methods are computationally intensive and time-consuming, we propose a graph attention network-based (GAT-based) framework, with novel aggregation methods in the GAT models and informative fine-grained graph generation methods, to achieve efficient analysis of current crowding and IR drops in face-to-face 3D IC TSVs. For current density and voltage predictions, the proposed framework attains R2 scores of 0.9776 and 0.9952 compared to ground truth results, respectively. Our framework also demonstrates over 837× speedup than ANSYS Q3D Extractor. Furthermore, the proposed framework outperforms other machine learning-based (ML-based) methods, including the state-of-the-art method.
In this paper, we consider a setting in which geographically constrained “local” wireless services operate in a shared spectrum band and compete in the same market for customers who fall within their local coverage areas. When their desired coverage areas overlap, there are multiple ways that spectrum usage could be coordinated. We discuss ways in which this coordination could arise. We then characterize the market impacts of different forms of coordination via a framework of Cournot competition with congestion. Our analysis illustrates the economic trade-offs of different coordination mechanisms for local services.
We present a new geometric interpretation of Markov Decision Processes (MDPs) with a natural normalization procedure that allows us to adjust the value function at each state without altering the advantage of any action with respect to any policy. This advantage-preserving transformation of the MDP motivates a class of algorithms which we call Reward Balancing, which solve MDPs by iterating through these transformations, until an approximately optimal policy can be trivially found. We provide a convergence analysis of several algorithms in this class, in particular showing that for MDPs for unknown transition probabilities we can improve upon state-of-the-art sample complexity results.
Computing distances and finding shortest paths in massive real-world networks is a fundamental algorithmic task in network analysis. There are two main approaches to solving this task. On one end are traversal-based algorithms like bidirectional breadth-first search (BiBFS), which have no preprocessing step but are slow on individual distance inquiries. On the other end are indexing-based approaches, which create and maintain a large index. This allows for answering individual inquiries very fast; however, index creation is prohibitively expensive. We seek to bridge these two extremes: quickly answer distance inquiries without the need for costly preprocessing. We propose a new algorithm and data structure, WormHole, for approximate shortest path computations. WormHole leverages structural properties of social networks to build a sublinearly sized index, drawing upon the core-periphery decomposition of Ben-Eliezer et al. [10]. Empirically, WormHole's preprocessing time improves upon index-based solutions by orders of magnitude: indexing billion edges graphs takes only a few minutes. Real time performance is consistently much faster than in BiBFS. The acceleration comes at the cost of a minor accuracy trade-off. We complement these empirical results with provable theoretical guarantees.
Wildlife management is increasingly reliant on data-driven insights to address the impacts of climate change on species and ecosystems. However, the complexity of accessing and querying large, multimodal datasets often limits the ability of non-technical users, such as wildlife managers and conservationists, to make informed decisions. To address this challenge, we present WildlifeLookup, a public accessible, intelligent chatbot designed to facilitate natural language interaction with a novel knowledge graph (KN-Wildlife) that houses critical wildlife and environmental data. WildlifeLookup simplifies access to species distributions, habitat interactions, and climate-related events by converting user queries into precise graph queries, reducing the technical barriers for end users. The chatbot WildlifeLookup is available at https://oknbot.ngrok.dev/
360° video streaming requires considerable bandwidth, and many techniques have been proposed to address this problem. One such technique is super-resolution, where the video is compressed at the server, and the client runs a deep learning model to enhance the video quality. However, most of today’s off-the-shelf mobile devices cannot support super-resolution for all tiles in real time. As a result, some tiles cannot be reconstructed to high resolution, significantly reducing users’ Quality of Experience (QoE). To address this problem, we utilize linear interpolation, which requires much less computational overhead. Through experiments, we observe that interpolation can achieve comparable quality, and even outperform super-resolution for some tiles with low spatial complexity. Building on this, we develop a 360° video streaming system that adaptively selects the most suitable downloading strategy, whether interpolation, super-resolution, or ABR at the appropriate bitrate, for each tile to maximize user QoE while considering network bandwidth limitations and the computational constraints of mobile devices. We formalize the 360° video streaming problem as an optimization problem and propose an efficient algorithm to solve it. Extensive evaluations using real user viewing data and 5G network traces demonstrate that our solution significantly outperforms existing techniques in terms of QoE under various scenarios.
This paper describes XRXL, an extended-reality system for increasing student engagement in large lectures. Students wear XR headsets to see 3D visualizations controlled by the instructor. The instructor can virtually retract the roof and walls of the classroom to allow for large-scale visualizations that extend beyond the physical boundaries of the classroom, or to turn the classroom into a 360° theater. The instructor can also partition the classroom into small groups of students and to assist individual groups as needed. XRXL was tested in an IRB-approved user study with 82 students in the context of a mock-lecture on neural networks. To the best of our knowledge, the study is the largest deployment of a co-located collaborative XR application to date. The study shows that students had a favorable opinion of XRXL, that XRXL had a low task load, an acceptable usability level, and that it did not cause cybersickness.
Mistakes, failures, and transgressions committed by a robot are inevitable as robots become more involved in our society. When a wrong behavior occurs, it is important to understand what factors might affect how the robot is perceived by people. In this paper, we investigated how the type of transgressor (human or robot) and type of backstory depicting the transgressor's mental capabilities (default, physio-emotional, socio-emotional, or cognitive) shaped participants' perceptions of the transgressor's morality. We performed an online, between-subjects study in which participants (N=720) were first introduced to the transgressor and its backstory, and then viewed a video of a real-life robot or human pushing down a human. Although participants attributed similarly high intent to both the robot and the human, the human was generally perceived to have higher morality than the robot. However, the backstory that was told about the transgressors' capabilities affected their perceived morality. We found that robots with emotional backstories (i.e., physio-emotional or socio-emotional) had higher perceived moral knowledge, emotional knowledge, and desire than other robots. We also found that humans with cognitive backstories were perceived with less emotional and moral knowledge than other humans. Our findings have consequences for robot ethics and robot design for HRI. 
Offline safe reinforcement learning (OSRL) aims to learn policies with high rewards while satisfying safety constraints solely from data collected offline. However, the learned policies often struggle to handle states and actions that are not present or out-of-distribution (OOD) from the offline dataset, which can result in violation of the safety constraints or overly conservative behaviors during their online deployment. Moreover, many existing methods are unable to learn policies that can adapt to varying constraint thresholds. To address these challenges, we propose constraint-conditioned actor-critic (CCAC), a novel OSRL method that models the relationship between state-action distributions and safety constraints, and leverages this relationship to regularize critics and policy learning. CCAC learns policies that can effectively handle OOD data and adapt to varying constraint thresholds. Empirical evaluations on the benchmarks show that CCAC significantly outperforms existing methods for learning adaptive, safe, and high-reward policies. 
Language model approaches have recently been integrated into binary analysis tasks, such as function similarity detection and function signature recovery. These models typically employ a two-stage training process: pre-training via Masked Language Modeling (MLM) on machine code and fine-tuning for specific tasks. While MLM helps to understand binary code struc- tures, it ignores essential code characteristics, including control and data flow, which negatively affect model generalization. Recent work leverages domain-specific features (e.g., control flow graphs and dynamic execution traces) in transformer-based approaches to improve binary code semantic understanding. However, this approach involves complex feature engineering, a cumbersome and time-consuming process that can introduce predictive uncertainty when dealing with stripped or obfuscated code, leading to a performance drop. In this paper, we introduce PROTST, a novel transformer-based methodology for binary code embedding. PROTST employs a hierarchical training process based on a unique tree-like structure, where knowledge progressively flows from fundamental tasks at the root to more specialized tasks at the leaves. This progressive teacher-student paradigm allows the model to build upon previously learned knowledge, resulting in high-quality embeddings that can be effectively leveraged for diverse downstream binary analysis tasks. The effectiveness of PROTST is evaluated in seven binary analysis tasks, and the results show that PROTST yields an average validation score (F1, MRR, and Recall@1) improvement of 14.8% compared to traditional two-stage training and an average validation score of 10.7% compared to multimodal two-stage frameworks. 
Social robots need to be able to interact effectively with small groups. While there is a significant interest in human-robot interaction in groups, little focus has been placed on developing autonomous social robot decision-making methods that operate smoothly with small groups of any size (e.g. 2, 3, or 4 interactants). In this work, we propose a Template- and Graph-based Modeling approach for robots interacting in small groups (TGM), enabling them to interact with groups in a way that is group-size agnostic. Critically, we separate the decision about the target of their communication, or ''whom to address?'' from the decision of ''what to communicate?'', which allows us to use template-based actions. We further use Graph Neural Networks (GNNs) to efficiently decide on ''whom'' and ''what''. We evaluated TGM using imitation learning and compared the structured reasoning achieved through GNNs to unstructured approaches for this two-part decision-making problem. On two different datasets, we show that TGM outperforms the baselines encouraging future work to invest in collecting larger datasets. 
Open-ended coding tasks, which ask students to construct programs according to certain specifications, are common in computer science education. Student modeling can be challenging since their open-ended nature means that student code can be diverse. Traditional knowledge tracing (KT) models that only analyze response correctness may not fully capture nuances in student knowledge from student code. In this paper, we introduce Test case-Informed Knowledge Tracing for Open-ended Coding (TIKTOC), a framework to simultaneously analyze and predict both open-ended student code and whether the code passes each test case. We augment the existing CodeWorkout dataset with the test cases used for a subset of the open-ended coding questions, and propose a multi-task learning KT method to simultaneously analyze and predict 1) whether a student’s code submission passes each test case and 2) the student’s open-ended code, using a large language model as the backbone. We quantitatively show that these methods outperform existing KT methods for coding that only use the overall score a code submission receives. We also qualitatively demonstrate how test case information, combined with open-ended code, helps us gain fine-grained insights into student knowledge.
Recent advances in large language models (LLMs) have led to the development of artificial intelligence (AI)-powered tutoring chatbots, showing promise in providing broad access to high-quality personalized education. Existing works have studied how to make LLMs follow tutoring principles, but have not studied broader uses of LLMs for supporting tutoring. Up until now, tracing student knowledge and analyzing misconceptions has been difficult and time-consuming to implement for open-ended dialogue tutoring. In this work, we investigate whether LLMs can be supportive of this task: we first use LLM prompting methods to identify the knowledge components/skills involved in each dialogue turn, i.e., a tutor utterance posing a task or a student utterance that responds to it. We also evaluate whether the student responds correctly to the tutor and verify the LLM’s accuracy using human expert annotations. We then apply a range of knowledge tracing (KT) methods on the resulting labeled data to track student knowledge levels over an entire dialogue. We conduct experiments on two tutoring dialogue datasets, and show that a novel yet simple LLM-based method, LLMKT, significantly outperforms existing KT methods in predicting student response correctness in dialogues. We perform extensive qualitative analyses to highlight the challenges in dialogueKT and outline multiple avenues for future work.
Theater-based design methods are seeing increased use in social robotics, as embodied roleplay is an ideal method for designing embodied interactions. Yet theater-based design methods are often cast as simply one possible tool; there has been little consideration of the importance of specific improvisational skills for theater-based design; and there has been little consideration of how to train students in theater-based design methods. We argue that improvisation is not just one possible tool of social robot design, but is instead central to social robotics. Leveraging recent theoretical work on Applied Improvisation, we show how improvisational skills represent (1) a set of key capabilities needed for any socially interactive robot, (2) a set of learning objectives for training engineers in social robot design, and (3) a set of methodologies for training those engineers to engage in theater-based design methods. Accordingly, we argue for a reconceptualization of Social Robotics as an Applied Improvisation project; we present, as a speculative pedagogical artifact, a sample syllabus for an envisioned Applied Improvisation driven Social Robotics course that might give students the technical and improvisational skills necessary to be effective robot designers; and we present a case study in which Applied Improvisation methods were simultaneously used (a) by instructors, to rapidly scaffold engineering students’ improvisational skills and (b) by those students, to engage in more effective human-robot interaction design. 
Skills in collaborative problem solving (CPS) are essential for the 21st century, enabling students to solve complex problems effectively. As the demand for these skills rises, understanding their development and manifestation becomes increasingly important. To address this need, we present a data-driven framework that identifies behavioral patterns associated with CPS practices and can assess students’ learning outcomes. It provides explainable insights into the relationship between students’ behaviors and learning performance. We employ embedding and clustering techniques to categorize similar trace logs and apply Latent Dirichlet allocation to generate meaningful descriptors. To capture the temporal evolution of student behaviors, we introduce a graph-based representation of transitions between behavior patterns extracted using constraint-based pattern mining. We map behavioral patterns to a CPS ontology by analyzing how action sequences correspond to specific CPS practices. Analysis of semi-structured trace log data from 61 middle school students engaged in collaborative game-based learning reveals that the extracted behavioral patterns significantly predict student learning gains using generalized additive models. Our analysis identifies patterns that provide insights into the relationship between student use of CPS practices and learning outcomes.
Qwerty is a high-level quantum programming language built on bases and functions rather than circuits. This new paradigm introduces new challenges in compilation, namely synthesizing circuits from basis translations and automatically specializing adjoint or predicated forms of functions. This paper presents ASDF, an open-source compiler for Qwerty that answers these challenges in compiling basis-oriented languages. Enabled with a novel high-level quantum IR implemented in the MLIR framework, our compiler produces OpenQASM 3 or QIR for either simulation or execution on hardware. Our compiler is evaluated by comparing the fault-tolerant resource requirements of generated circuits with other compilers, finding that ASDF produces circuits with comparable cost to prior circuit-oriented compilers.
Deploying complex machine learning models on resource-constrained devices is challenging due to limited computational power, memory, and model retrainability. To address these limitations, a hybrid system can be established by augmenting the local model with a server-side model, where samples are selectively deferred by a rejector and then sent to the server for processing. The hybrid system enables efficient use of computational resources while minimizing the overhead associated with server usage. The recently proposed Learning to Help (L2H) model proposed training a server model given a fixed local (client) model. This differs from the Learning to Defer (L2D) framework which trains the client for a fixed (expert) server. In both L2D and L2H, the training includes learning a rejector at the client to determine when to query the server. In this work, we extend the L2H model from binary to multi-class classification problems and demonstrate its applicability in a number of different scenarios of practical interest in which access to the server may be limited by cost, availability, or policy. We derive a stage-switching surrogate loss function that is differentiable, convex, and consistent with the Bayes rule corresponding to the 0-1 loss for the L2H model. Experiments show that our proposed methods offer an efficient and practical solution for multi-class classification in resource-constrained environments. 
Recent advancements in quantum technologies, particularly in quantum sensing and simulation, have facilitated the generation and analysis of inherently quantum data. This progress underscores the necessity for developing efficient and scalable quantum data management strategies. This goal faces immense challenges due to the exponential dimensionality of quantum data and its unique quantum properties such as no-cloning and measurement stochasticity. Specifically, classical storage and manipulation of an arbitrary n-qubit quantum state requires exponential space and time. Hence, there is a critical need to revisit foundational data management concepts and algorithms for quantum data. In this paper, we propose succinct quantum data sketches to support basic database operations such as search and selection. We view our work as an initial step towards the development of quantum data management model, opening up many possibilities for future research in this direction. 
Recent advancements in neural rendering technologies and their supporting devices have paved the way for immersive 3D experiences, significantly transforming human interaction with intelligent devices across diverse applications. However, achieving the desired real-time rendering speeds for immersive interactions is still hindered by (1) the lack of a universal algorithmic solution for different application scenarios and (2) the dedication of existing devices or accelerators to merely specific rendering pipelines. To overcome this challenge, we have developed a unified neural rendering accelerator that caters to a wide array of typical neural rendering pipelines, enabling real-time and on-device rendering across different applications while maintaining both efficiency and compatibility. Our accelerator design is based on the insight that, although neural rendering pipelines vary and their algorithm designs are continually evolving, they typically share common operators, predominantly executing similar workloads. Building on this insight, we propose a reconfigurable hardware architecture that can dynamically adjust dataflow to align with specific rendering metric requirements for diverse applications, effectively supporting both typical and the latest hybrid rendering pipelines. Benchmarking experiments and ablation studies on both synthetic and real-world scenes demonstrate the effectiveness of the proposed accelerator. It achieves real-time rendering speeds (> 30 FPS) and up to 119× speedups over state-of-the-art neural rendering hardware across varied rendering pipelines, while adhering to power consumption constraints of around 5 W, typical for edge devices. Consequently, the proposed unified accelerator stands out as the first solution capable of achieving real-time neural rendering across varied representative pipelines on edge devices, potentially paving the way for the next generation of neural graphics applications.